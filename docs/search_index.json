[
["index.html", "STA 444/5 - Introductory Data Science using R Preface Other Resources Acknowledgements", " STA 444/5 - Introductory Data Science using R Derek L. Sonderegger September 11, 2019 Preface This book is intended to provide students with a resource for learning R while using it during an introductory statistics course. The Introduction section covers common issues that students in a typical statistics course will encounter and provides a simple examples and does not attempt to be exhaustive. The Deeper Details section addresses issues that commonly arise in many data wrangling situations and is intended to give students a deep enough understanding of R that they will be able to use it as their primary computing resource to manipulate, graph and model data. Other Resources There are a great number of very good online and physical resources for learning R. Hadley Wickham and Garrett Grolemund’s free online book R for Data Science. This is a wonderful introduction to the tidyverse and is free. If there is any book I’d recommend buying, this would be it. Michael Freeman’s book Programming Skills for Data Science. This book covers much of what we’ll do in this class and is quite readable. Hadley Wickham and Jenny Bryan have a whole book on R packages to effectively manage large projects. Acknowledgements These online books are used a huge amount of work and I appreciative the support of my wife Aubrey and the love our our two children Elise and Casey. "],
["1-familiarization.html", "Chapter 1 Familiarization 1.1 R file Types 1.2 R as a simple calculator 1.3 Assignment 1.4 Vectors 1.5 Packages 1.6 Finding Help 1.7 Exercises", " Chapter 1 Familiarization R is a open-source program that is commonly used in statistics and machine learning. It runs on almost every platform and is completely free and is available at www.r-project.org. Most cutting-edge statistical research is first available on R. The basic editor that comes with R works fairly well, but you should consider running R through the program RStudio which is located at rstudio.com. This is a completely free Integrated Developement Environment that works on Macs, Windows and a couple of flavors of Linux. It simplifies a bunch of more annoying aspects of the standard R GUI and supports things like tab completion. R is a script based language, and there isn’t a point-and-click interface for data wrangling and statistical modeling. While the initial learning curve will be steeper, understanding how to write scripts will be valuable because scripts leave a clear description of what steps were performed. This is a critical aspect of what is known as reproducable research and a good practice. While it may seem tempting to type commands into the console directly, but because the goal is to create a script that contains all of the necessary commands to perform an analysis, users should get into the habit of always writing their commands into their R script (or Rmarkdown file) and executing the command from there. 1.1 R file Types One of the worst things about a pocket calculator is there is no good way to go several steps and easily see what you did or fix a mistake (there is nothing more annoying than re-typing something because of a typo. To avoid these issues I always work with RMarkdown (or script) files instead of typing directly into the console. You will quickly learn that it is impossible to write R code correctly the first time and you’ll save yourself a huge amount of work by just embracing this from the beginning. Furthermore, having an R file fully documents how you did your analysis, which can help when writing the methods section of a paper. Finally, having a file makes it easy to re-run an analysis after a change in the data (additional data values, transformed data, or removal of outliers). There are three common ways to store R commands in some file: scripts, notebooks, and Rmarkdown files. The distinction between the R scripts and the other two is substantial as R scripts just store R commands, but don’t make any attampt to save the results in any distinct format. Both notebooks and Rmarkdown files save the results of an analysis and present the results in a nice readable fashion. I encourage people to use Rmarkdown files over notebooks because the Rmarkdown knitting enforces a reproducable workflow whereas notebooks can be run out of order. Rmarkdown files are written in a way to combine the R commands, commentary, and the command outputs all together into one coherent document. For most people that use R to advance their research, using Rmarkdown is the most useful. 1.1.1 R Scripts (.R files) The first type of file that we’ll discuss is a traditional script file. To create a new .R script in RStudio go to File -&gt; New File -&gt; R Script. This opens a new window in RStudio where you can type commands and functions as a common text editor. Type whatever you like in the script window and then you can execute the code line by line (using the run button or its keyboard shortcut to run the highlighted region or whatever line the curser is on) or the entire script (using the source button). Other options for what piece of code to run are available under the Code dropdown box. It often makes your R files more readable if you break a single command up into multiple lines. R scripts will disregard all whitespace (including line breaks) so you can safely spread your command over as multiple lines. Finally, it is useful to leave comments in the script for things such as explaining a tricky step, who wrote the code and when, or why you chose a particular name for a variable. The # sign will denote that the rest of the line is a comment and R will ignore it. An R script for a homework assignment might look something like this: # Problem 1 # Calculate the log of a couple of values and make a plot # of the log function from 0 to 3 log(0) log(1) log(2) x &lt;- seq(.1,3, length=1000) plot(x, log(x)) # Problem 2 # Calculate the exponential function of a couple of values # and make a plot of the function from -2 to 2 exp(-2) exp(0) exp(2) x &lt;- seq(-2, 2, length=1000) plot(x, exp(x)) This looks perfectly acceptable as a way of documenting what you did, but this script file doesn’t contain the actual results of commands I ran, nor does it show you the plots. Also anytime I want to comment on some output, it needs to be offset with the commenting character #. It would be nice to have both the commands and the results merged into one document. This is what the R Markdown file does for us. 1.1.2 R Markdown (.Rmd files) When I was a graduate student, I had to tediously copy and past tables of output from the R console and figures I had made into my Microsoft Word document. Far too often I would realize I had made a small mistake in part (b) of a problem and would have to go back, correct my mistake, and then redo all the laborious copying. I often wished that I could write both the code for my statistical analysis and the long discussion about the interpretation all in the same document so that I could just re-run the analysis with a click of a button and all the tables and figures would be updated by magic. Fortunately that magic now exists. To create a new R Markdown document, we use the File -&gt; New File -&gt; R Markdown... dropdown option and a menu will appear asking you for the document title, author, and preferred output type. In order to create a PDF, you’ll need to have LaTeX installed, but the HTML output nearly always works and I’ve had good luck with the MS Word output as well. The R Markdown is an implementation of the Markdown syntax that makes it extremely easy to write webpages and give instructions for how to do typesetting sorts of things. This syntax was extended to allow use to embed R commands directly into the document. Perhaps the easiest way to understand the syntax is to look at an at the RMarkdown website. The R code in my document is nicely separated from my regular text using the three backticks and an instruction that it is R code that needs to be evaluated. The output of this document looks good as a HTML, PDF, or MS Word document. I have actually created this entire book using RMarkdown. To see what the the Rmarkdown file looks like for any chapter, just click on the pencil icon at the top of the online notes. While writing an Rmarkdown file, each of the code chunks can be executed in a couple of different ways. 1. Press the green arrow at the top of the code chunk to run the entire chunk. 2. The run button has several options has several options. 3. There are keyboard shortcuts, on the Mac it is Cmd-Return. To insert a new code chunk, a user can type it in directly, use the green Insert button, or the keyboard shortcut. To produce a final output document that you’ll present to your boss/collegues/client where you want to combine the code, output, and commentary you’ll “knit” the document which causes all of the R code to be run in a new R session, and then weave together the output into your document. This can be done using the knit button at the top of the Editor Window. 1.1.3 R Notebooks (.Rmd files) Notebooks are just very specialized types of Rmarkdown file. Here, the result of each code chunk that is run manually is saved, but when previewing the output, all of the R code is NOT re-run. Therefore it is possible to run the code, then modify the code, and then produce a document where the written code and output do not match up. As a result of this “feature” I strongly discourage the use of notebooks in favor of the standard Rmarkdown files. 1.2 R as a simple calculator Assuming that you have started R on whatever platform you like, you can use R as a simple calculator. In either your Rmarkdown file code chunk (or just run this in the console), run the following # Some simple addition 2+3 ## [1] 5 In this fashion you can use R as a very capable calculator. 6*8 ## [1] 48 4^3 ## [1] 64 exp(1) # exp() is the exponential function ## [1] 2.718282 R has most constants and common mathematical functions you could ever want. sin(), cos(), and other trigonometry functions are available, as are the exponential and log functions exp(), log(). The absolute value is given by abs(), and round() will round a value to the nearest integer. pi # the constant 3.14159265... ## [1] 3.141593 sin(0) ## [1] 0 log(5) # unless you specify the base, R will assume base e ## [1] 1.609438 log(5, base=10) # base 10 ## [1] 0.69897 Whenever I call a function, there will be some arguments that are mandatory, and some that are optional and the arguments are separated by a comma. In the above statements the function log() requires at least one argument, and that is the number(s) to take the log of. However, the base argument is optional. If you do not specify what base to use, R will use a default value. You can see that R will default to using base \\(e\\) by looking at the help page (by typing help(log) or ?log at the command prompt). Arguments can be specified via the order in which they are passed or by naming the arguments. So for the log() function which has arguments log(x, base=exp(1)). If I specify which arguments are which using the named values, then order doesn’t matter. # Demonstrating order does not matter if you specify # which argument is which log(x=5, base=10) ## [1] 0.69897 log(base=10, x=5) ## [1] 0.69897 But if we don’t specify which argument is which, R will decide that x is the first argument, and base is the second. # If not specified, R will assume the second value is the base... log(5, 10) ## [1] 0.69897 log(10, 5) ## [1] 1.430677 When I specify the arguments, I have been using the name=value notation and a student might be tempted to use the &lt;- notation here. Don’t do that as the name=value notation is making an association mapping and not a permanent assignment. 1.3 Assignment We need to be able to assign a value to a variable to be able to use it later. R does this by using an arrow &lt;- or an equal sign =. While R supports either, for readability, I suggest people pick one assignment operator and stick with it. I personally prefer to use the arrow. Variable names cannot start with a number, may not include spaces, and are case sensitive. tau &lt;- 2*pi # create two variables my.test.var = 5 # notice they show up in &#39;Environment&#39; tab in RStudio! tau ## [1] 6.283185 my.test.var ## [1] 5 tau * my.test.var ## [1] 31.41593 As your analysis gets more complicated, you’ll want to save the results to a variable so that you can access the results later. If you don’t assign the result to a variable, you have no way of accessing the result. 1.4 Vectors While single values are useful, it is very important that we are able to make groups of values. The most fundamental aggregation of values is called a vector. In R, we will require vectors to always be of the same type (e.g. all integers or all character strings). To create a vector, we just need to use the collection function c(). x &lt;- c(&#39;A&#39;,&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) x ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; y &lt;- c( 4, 3, 8, 10 ) y ## [1] 4 3 8 10 It is very common to have to make sequences of integers, and R has a shortcut to do this. The notation A:B will produce a vector starting with A and incrementing by one until we get to B. 2:6 ## [1] 2 3 4 5 6 1.5 Packages One of the greatest strengths about R is that so many people have devloped add-on packages to do some additional function. For example, plant community ecologists have a large number of multivariate methods that are useful but were not part of R. So Jari Oksanen got together with some other folks and put together a package of functions that they found useful. The result is the package vegan. To download and install the package from the Comprehensive R Archive Network (CRAN), you just need to ask RStudio it to install it via the menu Tools -&gt; Install Packages.... Once there, you just need to give the name of the package and RStudio will download and install the package on your computer. Many major analysis types are available via downloaded packages as well as problem sets from various books (e.g. Sleuth3 or faraway) and can be easily downloaded and installed from CRAN via the menu. Once a package is downloaded and installed on your computer, it is available, but it is not loaded into your current R session by default. The reason it isn’t loaded is that there are thousands of packages, some of which are quite large and only used occasionally. So to improve overall performance only a few packages are loaded by default and the you must explicitly load packages whenever you want to use them. You only need to load them once per session/script. library(vegan) # load the vegan library For a similar performance reason, many packages do not automatically load their datasets unless explicitly asked. Therefore when loading datasets from a package, you might need to do a two-step process of loading the package and then loading the dataset. library(faraway) # load the package into memory ## ## Attaching package: &#39;faraway&#39; ## The following object is masked from &#39;package:lattice&#39;: ## ## melanoma data(&quot;butterfat&quot;) # load the dataset into memory If you don’t need to load any functions from a package and you just want the datasets, you can do it in one step. data(&#39;butterfat&#39;, package=&#39;faraway&#39;) # just load the dataset, not anything else butterfat[1:6, ] # print out the first 6 rows of the data ## Butterfat Breed Age ## 1 3.74 Ayrshire Mature ## 2 4.01 Ayrshire 2year ## 3 3.77 Ayrshire Mature ## 4 3.78 Ayrshire 2year ## 5 4.10 Ayrshire Mature ## 6 4.06 Ayrshire 2year Similarly, if I am not using many functions from a package, I might choose call the functions using the notation package::function(). This is particularly important when two packages both have functions with the same name and it gets confusing which function you want to use. For example the packages mosaic and dplyr both have a function tally. So if I’ve already loaded the dplyr package but want to use the mosaic::tally() function I would use the following: mosaic::tally( c(0,0,0,1,1,1,1,2) ) ## X ## 0 1 2 ## 3 4 1 Finally, many researchers and programmers host their packages on GitHub (or equivalent site) and those packages can easily downloaded using tools from the devtools pacakge, which can be downloaded from CRAN. devtools::install_github(&#39;dereksonderegger/SiZer&#39;) ## Skipping install of &#39;SiZer&#39; from a github remote, the SHA1 (8745f2e4) has not changed since last install. ## Use `force = TRUE` to force installation 1.6 Finding Help There are many complicated details about R and nobody knows everything about how each individual package works. As a result, a robust collection of resources has been developed and you are undoubtably not the first person to wonder how to do something. 1.6.1 How does this function work? If you know the function you need, but just don’t know how to use it, the built-in documentation is really quite good. Suppose I am interested in how the rep function works. We could access the rep help page by searching in the help window or from the console via help(rep). The document that is displayed shows what arguments the function expects and what it will return. At the bottom of the help page is often a set of examples demonstrating different ways to use the function. As you get more proficient in R, these help files become quite handy, but initially they feel quite overwhelming. 1.6.2 How does this package work? If a package author really wants their package to be used by a wide audience, they will provide a “vignette”. These are a set of notes that explain enough of how a package works to get a user able to utilize the package effectively. This documentation is targetted towards people the know some R, but deep technical knowledge is not expected. Whenever I encounter a new package that might be applicable to me, the first thing I do is see if it has a vignette, and if so, I start reading it. If a package doesn’t have a vignette, I’ll google “R package XXXX” and that will lead to documentation on CRAN that gives a list of functions in the package. 1.6.3 How do I do XXX? Often I find myself asking how to do something but I don’t know the function or package to use. In those cases, I will use the coding question and answer site stackoverflow. This is particularly effective and I encourage students to spend some time to understand the solutions presented instead of just copying working code. By digging into why a particular code chunk works, you’ll learn all sorts of neat tricks and you’ll find yourself utilizing the site less frequently. 1.7 Exercises Create an RMarkdown file that solves the following exercises. Calculate \\(\\log\\left(6.2\\right)\\) first using base \\(e\\) and second using base \\(10\\). To figure out how to do different bases, it might be helpful to look at the help page for the log function. Calculate the square root of 2 and save the result as the variable named sqrt2. Have R display the decimal value of sqrt2. Hint: use Google to find the square root function. Perhaps search on the keywords “R square root function”. This exercise walks you through installing a package with all the datasets used in the textbook The Statistical Sleuth. Install the package Sleuth3 on your computer using RStudio. Load the package using the library() command. Print out the dataset case0101 "],
["2-data-frames.html", "Chapter 2 Data Frames 2.1 Introduction to Importing Data 2.2 Data Types 2.3 Basic Manipulation 2.4 Exercises", " Chapter 2 Data Frames # Load my favorite packages: dplyr, ggplot2, forcats, readr, and stringr library(tidyverse, quietly = TRUE) Data frames are the fundamental unit of data storage that casual users of R need to work with. Conceptually they are just like a single tab in a spreadsheet (e.g. Excel) file. There are multiple rows and columns and each column is of the same type of information (e.g. numerical values, dates, or character strings) and each row represents a single observation. Because the columns have meaning and we generally give them column names, it is desirable to want to access an element by the name of the column as opposed to the column number. While writing formulas in large Excel spreadsheets I often get annoyed trying to remember which column something was in and muttering “Was total biomass in column P or Q?” A system where I could just name the column Total_Biomass and then always refer to it that way, is much nicer to work with and I make fewer dumb mistakes. In this chapter we will briefly cover the minimal set of tools for working with data frames. First we discuss how to import data sets, both packages from packages and from appropriately formated Excel and .csv files. Finally we’ll see how to create a data frame “by hand” and to access columns and do simple manipulations. In this chapter, we will focus on standard R data frame manipulations so that readers gain basic familiarity with non-tidyverse accessor methods. 2.1 Introduction to Importing Data 2.1.1 From a Package For many students, they will be assigned homework that utilizes data sets that are stored in some package. To access those, we would need to first install the package if we haven’t already. Recall to do that, we can use the Rstudio menu bar “Tools -&gt; Install Packages…” mouse action. Because we might have thousands of packages installed on a computer, and those packages might all have data sets associated with them, they aren’t loaded into memory by default. Instead we have to go through a two-step process of making sure that the package is installed on the computer, and then load the desired data set into the running session of R. Once the package is intalled, we can load the data into our session via the following command: data(&#39;alfalfa&#39;, package=&#39;faraway&#39;) # load the data set &#39;alfalfa&#39; from the package &#39;faraway&#39; Because R tries to avoid loading datasets until it is sure that you need them, the object alfalfa isn’t initially loaded as a data.frame but rather as a “promise” that it eventually will be loaded whenever you first use it. So lets first access it by viewing it. View(alfalfa) There are two ways to enter the view command. Either executing the View() function from the console, or clicking on either the white table or the object name in the Environment tab. # Show the image of the environment tab with the white table highlighted 2.1.2 Import from .csv or .xls files Often times data is stored in a “Comma Separated Values” file (with the file suffix of .csv) where the rows in the file represent the data frame rows, and the columns are just separated by commas. The first row of the file is usually the column titles. Alternatively, the data might be stored in an Excel file and we just need to tell R where the file is and which worksheet tab to import. The hardest part for people that are new to programming is giving the path to the data file. In this case, I recommend students use the data import wizard that RStudio includes which is accessed via ‘File -&gt; Import Dataset’. This will then give you a choice of file types to read from (.csv files are in the “Text” options). Once you have selected the file type to import, the user is presented with a file browser window where the desired file should be located. Once the file is chosen, we can import of the file. Critically, we should notice that the import wizard generates R code that does the actual import. We MUST copy that code into our Rmarkdown file or else the import won’t happen when we try to knit the Rmarkdown into an output document because knitting always occurs in a completely fresh R session. So only use the import wizard to generate the import code! The code generated by the import wizard ends with a View() command and I typically remove that as it can interfer with the knitting process. The code that I’ll paste into my RMarkdown file typically looks like this: library(readxl) Melioid_IgG &lt;- read_excel(&quot;~/Dropbox/NAU/MAGPIX serology/Data/Melioid_IgG.xlsx&quot;) # View(Melioid_IgG) 2.2 Data Types Data frames are required that each column have the same type. That is to say, if a column is numeric, you can just change one value to a character string. Below are the most common data types that are commonly used within R. Integers - These are the integer numbers \\(\\left(\\dots,-2,-1,0,1,2,\\dots\\right)\\). To convert a numeric value to an integer you may use the function as.integer(). Numeric - These could be any number (whole number or decimal). To convert another type to numeric you may use the function as.numeric(). Strings - These are a collection of characters (example: Storing a student’s last name). To convert another type to a string, use as.character(). Factors - These are strings that can only values from a finite set. For example we might wish to store a variable that records home department of a student. Since the department can only come from a finite set of possibilities, I would use a factor. Factors are categorical variables, but R calls them factors instead of categorical variable. A vector of values of another type can always be converted to a factor using the as.factor() command. For converting numeric values to factors, I will often use the function cut(). Logicals - This is a special case of a factor that can only take on the values TRUE and FALSE. (Be careful to always capitalize TRUE and FALSE. Because R is case-sensitive, TRUE is not the same as true.) Using the function as.logical() you can convert numeric values to TRUE and FALSE where 0 is FALSE and anything else is TRUE. Depending on the command, R will coerce your data from one type to another if necessary, but it is a good habit to do the coercion yourself. If a variable is a number, R will automatically assume that it is continuous numerical variable. If it is a character string, then R will assume it is a factor when doing any statistical analysis. Most of these types are familiar to beginning R users except for factors. Factors are how R keeps track of categorical variables. R does this in a two step pattern. First it figures out how many categories there are and remembers which category an observation belongs two and second, it keeps a vector character strings that correspond to the names of each of the categories. # A character vector y &lt;- c(&#39;B&#39;,&#39;B&#39;,&#39;A&#39;,&#39;A&#39;,&#39;C&#39;) y ## [1] &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;C&quot; # convert the vector of characters into a vector of factors z &lt;- factor(y) str(z) ## Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 2 2 1 1 3 Notice that the vector z is actually the combination of group assignment vector 2,2,1,1,3 and the group names vector “A”,”B”,”C”. So we could convert z to a vector of numerics or to a vector of character strings. as.numeric(z) ## [1] 2 2 1 1 3 as.character(z) ## [1] &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;C&quot; Often we need to know what possible groups there are, and this is done using the levels() command. levels(z) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; Notice that the order of the group names was done alphabetically, which we did not chose. This ordering of the levels has implications when we do an analysis or make a plot and R will always display information about the factor levels using this order. It would be nice to be able to change the order. Also it would be really nice to give more descriptive names to the groups rather than just the group code in my raw data. Useful functions for controling the order and labels of the factor can be found in the forcats package which we use in a later chapter. 2.3 Basic Manipulation Occasionally I’ll need to create a small data frame “by hand” to facilitate creating graphs in ggplot2. In this final section, we’ll cover creating a data frame and doing simple manipulations using the base R commands and syntax. To create a data frame, we have to squish together a bunch of columns vectors. The command data.frame() does exactly that. In the example below, I list the names, ages and heights (in inches) of my family. family &lt;- data.frame( Names = c(&#39;Derek&#39;, &#39;Aubrey&#39;, &#39;Elise&#39;, &#39;Casey&#39;), Age = c(42, 39, 6, 3), Height.in = c(64, 66, 43, 39) ) family ## Names Age Height.in ## 1 Derek 42 64 ## 2 Aubrey 39 66 ## 3 Elise 6 43 ## 4 Casey 3 39 To access a particular column, we could use the $ operator. We could then do something like calculate the mean or standard deviation. family$Age ## [1] 42 39 6 3 mean( family$Age ) ## [1] 22.5 sd( family$Age ) ## [1] 20.85665 As an alternative to the “$” operator, we could use the [row, column] notation. To select a particular row or column, we can select them by either name or location. family[ , &#39;Age&#39;] # all the rows, Age column ## [1] 42 39 6 3 family[ 2, &#39;Age&#39;] # age of person in row 2 ## [1] 39 Next we could calculate everybodies height in centimeters by multiplying the heights by 2.54 and saving the result in column appropriately named. family$Height.cm &lt;- family$Height.in * 2.54 # calculate the heights and save them! family # view our result! ## Names Age Height.in Height.cm ## 1 Derek 42 64 162.56 ## 2 Aubrey 39 66 167.64 ## 3 Elise 6 43 109.22 ## 4 Casey 3 39 99.06 2.4 Exercises Create a data frame “by hand” with the names, ages, and heights of your own family. If this feels odd, feel free to make up people or include pets. Calculate the mean age amongst your family. I have a spreadsheet file hosted on GitHub at https://raw.githubusercontent.com/dereksonderegger/570L/master/data-raw/Example_1.csv. Because the readr package doesn’t care whether a file is on your local computer or on the Internet, we’ll use this file. Start the import wizard using: “File -&gt; Import Dataset -&gt; From Text (readr) …” and input the above web URL. Click the update button near the top to cause the wizard to preview the result. Save the generated code to your Rmarkdown file and show the first few rows using the head() command. "],
["3-graphing.html", "Chapter 3 Graphing 3.1 Basic Graphs 3.2 Title &amp; Axis Labels 3.3 Annotation 3.4 Faceting 3.5 Exercises", " Chapter 3 Graphing library(tidyverse, quietly = TRUE) # loading ggplot2 and dplyr There are three major “systems” of making graphs in R. The basic plotting commands in R are quite effective but the commands do not have a way of being combined in easy ways. Lattice graphics (which the mosaic package uses) makes it possible to create some quite complicated graphs but it is very difficult to do make non-standard graphs. The last package, ggplot2 tries to not anticipate what the user wants to do, but rather provide the mechanisms for pulling together different graphical concepts and the user gets to decide which elements to combine. To make the most of ggplot2 it is important to wrap your mind around “The Grammar of Graphics”. Briefly, the act of building a graph can be broken down into three steps. Define what data set we are using. What is the major relationship we wish to examine? In what way should we present that relationship? These relationships can be presented in multiple ways, and the process of creating a good graph relies on building layers upon layers of information. For example, we might start with printing the raw data and then overlay a regression line over the top. Next, it should be noted that ggplot2 is designed to act on data frames. It is actually hard to just draw three data points and for simple graphs it might be easier to use the base graphing system in R. However for any real data analysis project, the data will already be in a data frame and this is not an annoyance. These notes are sufficient for creating simple graphs using ggplot2, but are not intended to be exhaustive. There are many places online to get help with ggplot2. One very nice resource is the website, http://www.cookbook-r.com/Graphs/, which gives much of the information available in the book R Graphics Cookbook which I highly recommend. Second is just googling your problems and see what you can find on websites such as StackExchange. One way that ggplot2 makes it easy to form very complicated graphs is that it provides a large number of basic building blocks that, when stacked upon each other, can produce extremely complicated graphs. A full list is available at http://docs.ggplot2.org/current/ but the following list gives some idea of different building blocks. These different geometries are different ways to display the relationship between variables and can be combined in many interesting ways. Geom Description Required Aesthetics geom_histogram A histogram x geom_bar A barplot x geom_density A density plot of data. (smoothed histogram) x geom_boxplot Boxplots x, y geom_line Draw a line (after sorting x-values) x, y geom_path Draw a line (without sorting x-values) x, y geom_point Draw points (for a scatterplot) x, y geom_smooth Add a ribbon that summarizes a scatterplot x, y geom_ribbon Enclose a region, and color the interior ymin, ymax geom_errorbar Error bars ymin, ymax geom_text Add text to a graph x, y, label geom_label Add text to a graph x, y, label geom_tile Create Heat map x, y, fill A graph can be built up layer by layer, where: Each layer corresponds to a geom, each of which requires a dataset and a mapping between an aesthetic and a column of the data set. If you don’t specify either, then the layer inherits everything defined in the ggplot() command. You can have different datasets for each layer! Layers can be added with a +, or you can define two plots and add them together (second one over-writes anything that conflicts). 3.1 Basic Graphs 3.1.1 Scatterplots To start with, we’ll make a very simple scatterplot using the iris dataset. The iris dataset contains observations on three species of iris plants where we’ve measured the length and width of both the petals and sepals. We will make a scatterplot of Sepal.Length versus Petal.Length, which are two columns in the dataset. data(iris) # load the iris dataset that comes with R str(iris) # what columns do we have to play with... ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ggplot( data=iris, aes(x=Sepal.Length, y=Petal.Length) ) + geom_point( ) The data set we wish to use is specified using data=iris. The relationship we want to explore is x=Sepal.Length and y=Petal.Length. This means the x-axis will be the Sepal Length and the y-axis will be the Petal Length. The way we want to display this relationship is through graphing 1 point for every observation. We can define other attributes that might reflect other aspects of the data. For example, we might want for the color of the data point to change dynamically based on the species of iris. ggplot( data=iris, aes(x=Sepal.Length, y=Petal.Length) ) + geom_point( aes(color=Species) ) The aes() command inside the previous section of code is quite mysterious. The way to think about the aes() is that it gives you a way to define relationships that are data dependent. In the previous graph, the x-value and y-value for each point was defined dynamically by the data, as was the color. If we just wanted all the data points to be colored blue and larger, then the following code would do that ggplot( data=iris, aes(x=Sepal.Length, y=Petal.Length) ) + geom_point( color=&#39;blue&#39;, size=4 ) The important part isn’t that color and size were defined in the geom_point() but that they were defined outside of an aes() function! Anything set inside an aes() command will be of the form attribute=Column_Name and will change based on the data. Anything set outside an aes() command will be in the form attribute=value and will be fixed. 3.1.2 Box Plots Boxplots are a common way to show a categorical variable on the x-axis and continuous on the y-axis. ggplot(iris, aes(x=Species, y=Petal.Length)) + geom_boxplot() The boxes show the \\(25^{th}\\), \\(50^{th}\\), and \\(75^{th}\\) percentile and the lines coming off the box extend to the smallest and largest non-outlier observation. 3.2 Title &amp; Axis Labels To make a graph more understandable, it is necessary to tweak the axis labels and add a main title and such. Here we’ll adjust labels in a graph, including the legend labels. # Save the graph before I add more to it. P &lt;- ggplot( data=iris, aes(x=Sepal.Length, y=Petal.Length) ) + geom_point( aes(color=Species) ) + labs( title=&#39;Sepal Length vs Petal Length&#39;) + labs( x=&quot;Sepal Length (cm)&quot;, y=&quot;Petal Length (cm)&quot; ) + labs( color=&quot;Species Name&quot;) + labs( caption = &quot;iris data from Edgar Anderson (1935)&quot; ) # Print out the plot P You could either call the labs() command repeatedly with each label, or you could provide multiple arguements to just one labs() call. 3.3 Annotation One way to improve the clarity of a graph is to remove the legend and label the points directly on the graph. For example, we could instead have the species names near the cloud of data points for the species. Usually our annotations aren’t stored in the data.frame that contains our data of interest. So we need to either create a new (usually small) data.frame that contains all the information needed to create the annotation or we need to set the necessary information in-place. Either way, we need to specify the x and y coordinates, the label to be printed as well as any other attribute that is set in the global aes() command. That means if color has been set globally, the annotation layer also needs to address the color attribute. 3.3.1 Using a data.frame To do this in ggplot, we need to make a data frame that has the columns Sepal.Length and Petal.Length so that we can specify where each label should go, as well as the label that we want to print. Also, because color is matched to the Species column, this small dataset should also have a the Species column. This step always requires a bit of fussing with the graph because the text size and location should be chosen based on the size of the output graphic and if I rescale the image it often looks awkward. Typically I leave this step until the figure is being prepared for final publication. # create another data frame that has the text labels I want to add to the graph. annotation.data &lt;- data.frame( Sepal.Length = c(4.5, 6.5, 7.0), # Figured out the label location by eye. Petal.Length = c(2.25, 3.75, 6.5), # If I rescale the graph, I would redo this step. Species = c(&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;), Text = c(&#39;SETOSA&#39;, &#39;VERSICOLOR&#39;, &#39;VIRGINICA&#39;) ) # Use the previous plot I created, along with the # aes() options already defined. P + geom_text( data=annotation.data, aes(label=Text), size=2.5) + # write the labels theme( legend.position = &#39;none&#39; ) # remove the legend 3.3.2 Setting attributes in-line Instead of creating a new data frame, we could just add a new layer and just set all of the graph attributes manually. To do this, we have to have one layer for each text we want to add to the graph. P + geom_text( x=4.5, y=2.25, size=2, label=&#39;SETOSA&#39; ) + geom_text( x=6.5, y=3.75, size=2, label=&#39;VERSICOLOR&#39; ) + geom_text( x=7.0, y=6.50, size=2, label=&#39;VIRGINICA&#39; ) Finally there is a geom_label layer that draws a nice box around what you want to print. P + geom_label( x=4.5, y=2.25, size=2, label=&#39;SETOSA&#39; ) + geom_label( x=6.5, y=3.75, size=2, label=&#39;VERSICOLOR&#39; ) + geom_label( x=7.0, y=6.50, size=2, label=&#39;VIRGINICA&#39; ) My recommendation is to just set the x, y, and label attributes manually if you have one or two annotations to print on the graph. If you have many annotations to print, the create a data frame that contains all of them and use data= argument in the geom to use that created annotation data set. 3.4 Faceting The goal with faceting is to make many panels of graphics where each panel represents the same relationship between variables, but something changes between each panel. For example using the iris dataset we could look at the relationship between Sepal.Length and Petal.Length either with all the data in one graph, or one panel per species. library(ggplot2) ggplot(iris, aes(x=Sepal.Length, y=Petal.Length)) + geom_point() + facet_grid( . ~ Species ) The line facet_grid( formula ) tells ggplot2 to make panels, and the formula tells how to orient the panels. In R formulas are always interpretated in the order y ~ x. Because I want the species to change as we go across the page, but don’t have anything I want to change vertically we use . ~ Species to represent that. If we had wanted three graphs stacked then we could use Species ~ .. For a second example, we look at a dataset that examines the amount a waiter was tipped by 244 parties. Covariates that were measured include the day of the week, size of the party, total amount of the bill, amount tipped, whether there were smokers in the group and the gender of the person paying the bill data(tips, package=&#39;reshape&#39;) head(tips) ## total_bill tip sex smoker day time size ## 1 16.99 1.01 Female No Sun Dinner 2 ## 2 10.34 1.66 Male No Sun Dinner 3 ## 3 21.01 3.50 Male No Sun Dinner 3 ## 4 23.68 3.31 Male No Sun Dinner 2 ## 5 24.59 3.61 Female No Sun Dinner 4 ## 6 25.29 4.71 Male No Sun Dinner 4 It is easy to look at the relationship between the size of the bill and the percent tipped. ggplot(tips, aes(x = total_bill, y = tip / total_bill )) + geom_point() Next we ask if there is a difference in tipping percent based on gender or day of the week by plotting this relationship for each combination of gender and day. ggplot(tips, aes(x = total_bill, y = tip / total_bill )) + geom_point() + facet_grid( sex ~ day ) Sometimes we want multiple rows and columns of facets, but there is only one categorical variable with many levels. In that case we use facet_wrap which takes a one-sided formula. ggplot(tips, aes(x = total_bill, y = tip / total_bill )) + geom_point() + facet_wrap( ~ day ) Finally we can allow the x and y scales to vary between the panels by setting “free”, “free_x”, or “free_y”. In the following code, the y-axis scale changes between the gender groups. ggplot(tips, aes(x = total_bill, y = tip / total_bill )) + geom_point() + facet_grid( sex ~ day, scales=&quot;free_y&quot; ) 3.5 Exercises For the dataset trees, which should already be pre-loaded. Look at the help file using ?trees for more information about this data set. We wish to build a scatterplot that compares the height and girth of these cherry trees to the volume of lumber that was produced. Create a graph using ggplot2 with Height on the x-axis, Volume on the y-axis, and Girth as the either the size of the data point or the color of the data point. Which do you think is a more intuitive representation? Add appropriate labels for the main title and the x and y axes. The R-squared value for a regression through these points is 0.36 and the p-value for the statistical significance of height is 0.00038. Add text labels “R-squared = 0.36” and “p-value = 0.0004” somewhere on the graph. Consider the following small dataset that represents the number of times per day my wife played “Ring around the Rosy” with my daughter relative to the number of days since she has learned this game. The column yhat represents the best fitting line through the data, and lwr and upr represent a 95% confidence interval for the predicted value on that day. Rosy &lt;- data.frame( times = c(15, 11, 9, 12, 5, 2, 3), day = 1:7, yhat = c(14.36, 12.29, 10.21, 8.14, 6.07, 4.00, 1.93), lwr = c( 9.54, 8.5, 7.22, 5.47, 3.08, 0.22, -2.89), upr = c(19.18, 16.07, 13.2, 10.82, 9.06, 7.78, 6.75)) Using ggplot() and geom_point(), create a scatterplot with day along the x-axis and times along the y-axis. Add a line to the graph where the x-values are the day values but now the y-values are the predicted values which we’ve called yhat. Notice that you have to set the aesthetic y=times for the points and y=yhat for the line. Because each geom_ will accept an aes() command, you can specify the y attribute to be different for different layers of the graph. Add a ribbon that represents the confidence region of the regression line. The geom_ribbon() function requires an x, ymin, and ymax columns to be defined. For examples of using geom_ribbon() see the online documentation: http://docs.ggplot2.org/current/geom_ribbon.html. ggplot(Rosy, aes(x=day)) + geom_point(aes(y=times)) + geom_line( aes(y=yhat)) + geom_ribbon( aes(ymin=lwr, ymax=upr), fill=&#39;salmon&#39;) What happened when you added the ribbon? Did some points get hidden? If so, why? Reorder the statements that created the graph so that the ribbon is on the bottom and the data points are on top and the regression line is visible. The color of the ribbon fill is ugly. Use Google to find a list of named colors available to ggplot2. For example, I googled “ggplot2 named colors” and found the following link: http://sape.inf.usi.ch/quick-reference/ggplot2/colour. Choose a color for the fill that is pleasing to you. Add labels for the x-axis and y-axis that are appropriate along with a main title. We’ll next make some density plots that relate several factors towards the birthweight of a child. The MASS package contains a dataset called birthwt which contains information about 189 babies and their mothers. In particular there are columns for the mother’s race and smoking status during the pregnancy. Load the birthwt by either using the data() command or loading the MASS library. Read the help file for the dataset using MASS::birthwt. The covariates race and smoke are not stored in a user friendly manner. For example, smoking status is labeled using a 0 or a 1. Because it is not obvious which should represent that the mother smoked, we’ll add better labels to the race and smoke variables. For more information about dealing with factors and their levels, see the Factors chapter in these notes. library(tidyverse) data(&#39;birthwt&#39;, package=&#39;MASS&#39;) birthwt &lt;- birthwt %&gt;% mutate( race = factor(race, labels=c(&#39;White&#39;,&#39;Black&#39;,&#39;Other&#39;)), smoke = factor(smoke, labels=c(&#39;No Smoke&#39;, &#39;Smoke&#39;))) Graph a histogram of the birthweights bwt using ggplot(birthwt, aes(x=bwt)) + geom_histogram(). Make separate graphs that denote whether a mother smoked during pregnancy by appending + facet_grid() command to your original graphing command. Perhaps race matters in relation to smoking. Make our grid of graphs vary with smoking status changing vertically, and race changing horizontally (that is the formula in facet_grid() should have smoking be the y variable and race as the x). Remove race from the facet grid, (so go back to the graph you had in part d). I’d like to next add an estimated density line to the graphs, but to do that, I need to first change the y-axis to be density (instead of counts), which we do by using aes(y=..density..) in the ggplot() aesthetics command. Next we can add the estimated smooth density using the geom_density() command. To really make this look nice, lets change the fill color of the histograms to be something less dark, lets use fill='cornsilk' and color='grey60'. To play with different colors that have names, check out the following: [http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf]. Change the order in which the histogram and the density line are added to the plot. Does it matter and which do you prefer? Finally consider if you should have the histograms side-by-side or one ontop of the other (i.e. . ~ smoke or smoke ~ .). Which do you think better displayes the decrease in mean birthweight and why? Load the dataset ChickWeight which comes preloaded in R and get the background on the dataset by reading the manual page ?ChickWeight. Produce a separate scatter plot of weight vs age for each chick. Use color to distinguish the four different Diet treatments. We could examine this data by producing a scatterplot for each diet. Most of the code below is readable, but if we don’t add the group aesthetic the lines would not connect the dots for each Chick but would instead connect the dots across different chicks. data(ChickWeight) ggplot(ChickWeight, aes(x=Time, y=weight, group=Chick )) + geom_point() + geom_line() + facet_grid( ~ Diet) "],
["4-data-wrangling.html", "Chapter 4 Data Wrangling 4.1 Verbs 4.2 Split, apply, combine 4.3 Exercises", " Chapter 4 Data Wrangling library(tidyverse, quietly = TRUE) # loading ggplot2 and dplyr Many of the tools to manipulate data frames in R were written without a consistent syntax and are difficult use together. To remedy this, Hadley Wickham (the writer of ggplot2) introduced a package called plyr which was quite useful. As with many projects, his first version was good but not great and he introduced an improved version that works exclusively with data.frames called dplyr which we will investigate. The package dplyr strives to provide a convenient and consistent set of functions to handle the most common data frame manipulations and a mechanism for chaining these operations together to perform complex tasks. The Dr Wickham has put together a very nice introduction to the package that explains in more detail how the various pieces work and I encourage you to read it at some point. [http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html]. One of the aspects about the data.frame object is that R does some simplification for you, but it does not do it in a consistent manner. Somewhat obnoxiously character strings are always converted to factors and subsetting might return a data.frame or a vector or a scalar. This is fine at the command line, but can be problematic when programming. Furthermore, many operations are pretty slow using data.frame. To get around this, Dr Wickham introduced a modified version of the data.frame called a tibble. A tibble is a data.frame but with a few extra bits. For now we can ignore the differences. The pipe command %&gt;% allows for very readable code. The idea is that the %&gt;% operator works by translating the command a %&gt;% f(b) to the expression f(a,b). This operator works on any function and was introduced in the magrittr package. The beauty of this comes when you have a suite of functions that takes input arguments of the same type as their output. For example, if we wanted to start with x, and first apply function f(), then g(), and then h(), the usual R command would be h(g(f(x))) which is hard to read because you have to start reading at the innermost set of parentheses. Using the pipe command %&gt;%, this sequence of operations becomes x %&gt;% f() %&gt;% g() %&gt;% h(). Written Meaning a %&gt;% f(b) f(a,b) b %&gt;% f(a, .) f(a, b) x %&gt;% f() %&gt;% g() g( f(x) ) In dplyr, all the functions below take a data set as its first argument and outputs an appropriately modified data set. This will allow me to chain together commands in a readable fashion. The pipe command works with any function, not just the dplyr functions and I often find myself using it all over the place. 4.1 Verbs The foundational operations to perform on a data set are: Subsetting - Returns a with only particular columns or rows – select - Selecting a subset of columns by name or column number. – filter - Selecting a subset of rows from a data frame based on logical expressions. – slice - Selecting a subset of rows by row number. arrange - Re-ordering the rows of a data frame. mutate - Add a new column that is some function of other columns. summarise - calculate some summary statistic of a column of data. This collapses a set of rows into a single row. Each of these operations is a function in the package dplyr. These functions all have a similar calling syntax, the first argument is a data set, subsequent arguments describe what to do with the input data frame and you can refer to the columns without using the df$column notation. All of these functions will return a data set. 4.1.1 Subsetting These function allows you select certain columns and rows of a data frame. 4.1.1.1 select() Often you only want to work with a small number of columns of a data frame and want to be able to select a subset of columns or perhaps remove a subset. The function to do that is dplyr::select() # Create a tiny data frame that is easy to see what is happening grades &lt;- data.frame( l.name = c(&#39;Cox&#39;, &#39;Dorian&#39;, &#39;Kelso&#39;, &#39;Turk&#39;), Exam1 = c(93, 89, 80, 70), Exam2 = c(98, 70, 82, 85), Final = c(96, 85, 81, 92) ) grades ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 ## 2 Dorian 89 70 85 ## 3 Kelso 80 82 81 ## 4 Turk 70 85 92 I could select the columns Exam columns by hand, or by using an extension of the : operator # select( grades, Exam1, Exam2 ) # from `grades`, select columns Exam1, Exam2 grades %&gt;% select( Exam1, Exam2 ) # Exam1 and Exam2 ## Exam1 Exam2 ## 1 93 98 ## 2 89 70 ## 3 80 82 ## 4 70 85 grades %&gt;% select( Exam1:Final ) # Columns Exam1 through Final ## Exam1 Exam2 Final ## 1 93 98 96 ## 2 89 70 85 ## 3 80 82 81 ## 4 70 85 92 grades %&gt;% select( -Exam1 ) # Negative indexing by name drops a column ## l.name Exam2 Final ## 1 Cox 98 96 ## 2 Dorian 70 85 ## 3 Kelso 82 81 ## 4 Turk 85 92 grades %&gt;% select( 1:2 ) # Can select column by column position ## l.name Exam1 ## 1 Cox 93 ## 2 Dorian 89 ## 3 Kelso 80 ## 4 Turk 70 The select() command has a few other tricks. There are functional calls that describe the columns you wish to select that take advantage of pattern matching. I generally can get by with starts_with(), ends_with(), and contains(), but there is a final operator matches() that takes a regular expression. grades %&gt;% select( starts_with(&#39;Exam&#39;) ) # Exam1 and Exam2 ## Exam1 Exam2 ## 1 93 98 ## 2 89 70 ## 3 80 82 ## 4 70 85 grades %&gt;% select( starts_with(&#39;Exam&#39;), starts_with(&#39;F&#39;) ) ## Exam1 Exam2 Final ## 1 93 98 96 ## 2 89 70 85 ## 3 80 82 81 ## 4 70 85 92 The dplyr::select function is quite handy, but there are several other packages out there that have a select function and we can get into trouble with loading other packages with the same function names. If I encounter the select function behaving in a weird manner or complaining about an input argument, my first remedy is to be explicit about it is the dplyr::select() function by appending the package name at the start. 4.1.1.2 filter() It is common to want to select particular rows where we have some logical expression to pick the rows. # select students with Final grades greater than 90 grades %&gt;% filter(Final &gt; 90) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 ## 2 Turk 70 85 92 You can have multiple logical expressions to select rows and they will be logically combined so that only rows that satisfy all of the conditions are selected. The logicals are joined together using &amp; (and) operator or the | (or) operator and you may explicitly use other logicals. For example a factor column type might be used to select rows where type is either one or two via the following: type==1 | type==2. # select students with Final grades above 90 and # average score also above 90 grades %&gt;% filter(Exam2 &gt; 90, Final &gt; 90) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 # we could also use an &quot;and&quot; condition grades %&gt;% filter(Exam2 &gt; 90 &amp; Final &gt; 90) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 4.1.1.3 slice() When you want to filter rows based on row number, this is called slicing. # grab the first 2 rows grades %&gt;% slice(1:2) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 ## 2 Dorian 89 70 85 4.1.2 arrange() We often need to re-order the rows of a data frame. For example, we might wish to take our grade book and sort the rows by the average score, or perhaps alphabetically. The arrange() function does exactly that. The first argument is the data frame to re-order, and the subsequent arguments are the columns to sort on. The order of the sorting column determines the precedent… the first sorting column is first used and the second sorting column is only used to break ties. grades %&gt;% arrange(l.name) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 ## 2 Dorian 89 70 85 ## 3 Kelso 80 82 81 ## 4 Turk 70 85 92 The default sorting is in ascending order, so to sort the grades with the highest scoring person in the first row, we must tell arrange to do it in descending order using desc(column.name). grades %&gt;% arrange(desc(Final)) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 ## 2 Turk 70 85 92 ## 3 Dorian 89 70 85 ## 4 Kelso 80 82 81 In a more complicated example, consider the following data and we want to order it first by Treatment Level and secondarily by the y-value. I want the Treatment level in the default ascending order (Low, Medium, High), but the y variable in descending order. # make some data dd &lt;- data.frame( Trt = factor(c(&quot;High&quot;, &quot;Med&quot;, &quot;High&quot;, &quot;Low&quot;), levels = c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;)), y = c(8, 3, 9, 9), z = c(1, 1, 1, 2)) dd ## Trt y z ## 1 High 8 1 ## 2 Med 3 1 ## 3 High 9 1 ## 4 Low 9 2 # arrange the rows first by treatment, and then by y (y in descending order) dd %&gt;% arrange(Trt, desc(y)) ## Trt y z ## 1 Low 9 2 ## 2 Med 3 1 ## 3 High 9 1 ## 4 High 8 1 4.1.3 mutate() I often need to create a new column that is some function of the old columns. In the dplyr package, this is a mutate command. To do ths, we give a mutate( NewColumn = Function of Old Columns ) command. # Modify the grades data frame and replace the old version with the new # that contains the newly created &quot;average&quot; column grades &lt;- grades %&gt;% mutate( average = (Exam1 + Exam2 + Final)/3 ) You can do multiple calculations within the same mutate() command, and you can even refer to columns that were created in the same mutate() command. grades %&gt;% mutate( average = (Exam1 + Exam2 + Final)/3, grade = cut(average, c(0, 60, 70, 80, 90, 100), # cut takes numeric variable c( &#39;F&#39;,&#39;D&#39;,&#39;C&#39;,&#39;B&#39;,&#39;A&#39;)) ) # and makes a factor ## l.name Exam1 Exam2 Final average grade ## 1 Cox 93 98 96 95.66667 A ## 2 Dorian 89 70 85 81.33333 B ## 3 Kelso 80 82 81 81.00000 B ## 4 Turk 70 85 92 82.33333 B 4.1.4 summarise() By itself, this function is quite boring, but will become useful later on. Its purpose is to calculate summary statistics using any or all of the data columns. Notice that we get to chose the name of the new column. The way to think about this is that we are collapsing information stored in multiple rows into a single row of values. # calculate the mean of exam 1 grades %&gt;% summarise( mean.E1=mean(Exam1) ) ## mean.E1 ## 1 83 We could calculate multiple summary statistics if we like. # calculate the mean and standard deviation grades %&gt;% summarise( mean.E1=mean(Exam1), stddev.E1=sd(Exam1) ) ## mean.E1 stddev.E1 ## 1 83 10.23067 4.2 Split, apply, combine Aside from unifying the syntax behind the common operations, the major strength of the dplyr package is the ability to split a data frame into a bunch of sub-data frames, apply a sequence of one or more of the operations we just described, and then combine results back together. We’ll consider data from an experiment from spinning wool into yarn. This experiment considered two different types of wool (A or B) and three different levels of tension on the thread. The response variable is the number of breaks in the resulting yarn. For each of the 6 wool:tension combinations, there are 9 replicated observations per wool:tension level. data(warpbreaks) str(warpbreaks) ## &#39;data.frame&#39;: 54 obs. of 3 variables: ## $ breaks : num 26 30 54 25 70 52 51 26 67 18 ... ## $ wool : Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ tension: Factor w/ 3 levels &quot;L&quot;,&quot;M&quot;,&quot;H&quot;: 1 1 1 1 1 1 1 1 1 2 ... The first we must do is to create a data frame with additional information about how to break the data into sub-data frames. In this case, I want to break the data up into the 6 wool-by-tension combinations. Initially we will just figure out how many rows are in each wool-by-tension combination. # group_by: what variable(s) shall we group on. # n() is a function that returns how many rows are in the # currently selected sub-dataframe warpbreaks %&gt;% group_by( wool, tension) %&gt;% # grouping summarise(n = n() ) # how many in each group ## # A tibble: 6 x 3 ## # Groups: wool [2] ## wool tension n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 A L 9 ## 2 A M 9 ## 3 A H 9 ## 4 B L 9 ## 5 B M 9 ## 6 B H 9 The group_by function takes a data.frame and returns the same data.frame, but with some extra information so that any subsequent function acts on each unique combination defined in the group_by. If you wish to remove this behavior, use group_by() to reset the grouping to have no grouping variable. Using the same summarise function, we could calculate the group mean and standard deviation for each wool-by-tension group. warpbreaks %&gt;% group_by(wool, tension) %&gt;% summarise( n = n(), # I added some formatting to show the mean.breaks = mean(breaks), # reader I am calculating several sd.breaks = sd(breaks)) # statistics. ## # A tibble: 6 x 5 ## # Groups: wool [2] ## wool tension n mean.breaks sd.breaks ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A L 9 44.6 18.1 ## 2 A M 9 24 8.66 ## 3 A H 9 24.6 10.3 ## 4 B L 9 28.2 9.86 ## 5 B M 9 28.8 9.43 ## 6 B H 9 18.8 4.89 If instead of summarizing each split, we might want to just do some calculation and the output should have the same number of rows as the input data frame. In this case I’ll tell dplyr that we are mutating the data frame instead of summarizing it. For example, suppose that I want to calculate the residual value \\[e_{ijk}=y_{ijk}-\\bar{y}_{ij\\cdot}\\] where \\(\\bar{y}_{ij\\cdot}\\) is the mean of each wool:tension combination. warpbreaks %&gt;% group_by(wool, tension) %&gt;% # group by wool:tension mutate(resid = breaks - mean(breaks)) %&gt;% # mean(breaks) of the group! head( ) # show the first couple of rows ## # A tibble: 6 x 4 ## # Groups: wool, tension [1] ## breaks wool tension resid ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 26 A L -18.6 ## 2 30 A L -14.6 ## 3 54 A L 9.44 ## 4 25 A L -19.6 ## 5 70 A L 25.4 ## 6 52 A L 7.44 4.3 Exercises The dataset ChickWeight tracks the weights of 48 baby chickens (chicks) feed four different diets. Load the dataset using data(ChickWeight) Look at the help files for the description of the columns. Remove all the observations except for observations from day 10 or day 20. The tough part in this instruction is disguishing between “and” and “or”. Obviously there are no observations that occur from both day 10 AND day 20. Google ‘R logical operators’ to get an introducton to those. Calculate the mean and standard deviation of the chick weights for each diet group on days 10 and 20. The OpenIntro textbook on statistics includes a data set on body dimensions. Load the file using Body &lt;- read.csv(&#39;http://www.openintro.org/stat/data/bdims.csv&#39;) The column sex is coded as a 1 if the individual is male and 0 if female. This is a non-intuitive labeling system. Create a new column sex.MF that uses labels Male and Female. Hint: the ifelse() command will be very convenient here. The ifelse() command in R functions similarly to the same command in Excel. The columns wgt and hgt measure weight and height in kilograms and centimeters (respectively). Use these to calculate the Body Mass Index (BMI) for each individual where \\[BMI=\\frac{Weight\\,(kg)}{\\left[Height\\,(m)\\right]^{2}}\\] Double check that your calculated BMI column is correct by examining the summary statistics of the column (e.g. summary(Body)). BMI values should be between 18 to 40 or so. Did you make an error in your calculation? The function cut takes a vector of continuous numerical data and creates a factor based on your give cut-points. # Define a continuous vector to convert to a factor x &lt;- 1:10 # divide range of x into three groups of equal length cut(x, breaks=3) ## [1] (0.991,4] (0.991,4] (0.991,4] (0.991,4] (4,7] (4,7] (4,7] ## [8] (7,10] (7,10] (7,10] ## Levels: (0.991,4] (4,7] (7,10] # divide x into four groups, where I specify all 5 break points cut(x, breaks = c(0, 2.5, 5.0, 7.5, 10)) ## [1] (0,2.5] (0,2.5] (2.5,5] (2.5,5] (2.5,5] (5,7.5] (5,7.5] ## [8] (7.5,10] (7.5,10] (7.5,10] ## Levels: (0,2.5] (2.5,5] (5,7.5] (7.5,10] # (0,2.5] (2.5,5] means 2.5 is included in first group # right=FALSE changes this to make 2.5 included in the second # divide x into 3 groups, but give them a nicer # set of group names cut(x, breaks=3, labels=c(&#39;Low&#39;,&#39;Medium&#39;,&#39;High&#39;)) ## [1] Low Low Low Low Medium Medium Medium High High High ## Levels: Low Medium High Create a new column of in the data frame that divides the age into decades (10-19, 20-29, 30-39, etc). Notice the oldest person in the study is 67. Body &lt;- Body %&gt;% mutate( Age.Grp = cut(age, breaks=c(10,20,30,40,50,60,70), right=FALSE)) Find the average BMI for each Sex-by-Age combination. "],
["5-statistical-models.html", "Chapter 5 Statistical Models 5.1 Formula Notation 5.2 Basic Models 5.3 Accessor function 5.4 Exercises", " Chapter 5 Statistical Models library(tidyverse, quietly = TRUE) # loading ggplot2 and dplyr options(tibble.width = Inf) # Print all the columns of a tibble (data.frame) While R is a full programming language, it was first developed by statisticians for statisticians. There are several functions to do common statistical tests but because those functions were developed early in R’s history, there is some inconsistency in how those functions work. There have been some attempts to standardize modeling object interfaces, but there were always be a little weirdness. 5.1 Formula Notation Most statistical modeling functions rely on a formula based interface. The primary purpose is to provide a consistent way to designate which columns in a data frame are the response variable and which are the explanatory variables. In particular the notation is \\[\\underbrace{y}_{\\textrm{LHS response}} \\;\\;\\; \\underbrace{\\sim}_{\\textrm{is a function of}} \\;\\;\\; \\underbrace{x}_{\\textrm{RHS explanatory}}\\] Mathematicians often refer to these terms as the Left Hand Side (LHS) and Right Hand Side (RHS). The LHS is always the response and the RHS contains the explanatory variables. In R, the LHS is usually just a single variable in the data. However the RHS can contain multiple variables and in complicated relationships. Right Hand Side Terms Meaning x1 + x2 Both x1 and x2 are additive explanatory variables. In this format, we are adding only the main effects of the x1 and x2 variables. x1:x2 This is the interaction term between x1 and x2 x1 * x2 Because whenever we add an interaction term to a model, we want to also have the main effects. So this is a short cut for adding the main effect of x1 and x2 and also the interaction term x1:x2. (x1 + x2 + x3)^2 This is the main effects of x1, x2, and x3 and also all of the second order interactions. poly(x, degree=2) This fits the degree 2 polynomial. When fit like this, R produces an orthogonal basis for the polynomial, which is more computationally stable, but won’t be appropiate for interpreting the polynomial coefficients. poly(x, degree=2, raw=TRUE) This fits the degree 2 polynomial using \\(\\beta_0 + \\beta_1 x + \\beta_2 x^2\\) and the polynomial polynomial coefficients are suitable for interpretation. I( x^2 ) Ignore the usual rules for interpreting formulas and do the mathematical calculation. This is not necessary for things like sqrt(x) or log(x) but required if there is a conflict between mathematics and the formula interpretation. 5.2 Basic Models The most common statistical models are generally referred to as linear models and the R function for creating a linear model is lm(). This section will introduce how to fit the model to data in a data frame as well as how to fit very specific t-test models. 5.2.1 t-tests There are several varients on T-tests depending on the question of interest, but they all require a continuous response and a categorical explanatory variable with two levels. If there is an obvious pairing between an observation in the first level of the explanatory variable with an observation in the second level, then it is a paired t-test, otherwise it is a two-sample t-test. 5.2.1.1 Two Sample t-tests First we’ll import data from the Lock5Data package that gives SAT scores and gender from 343 students in an introductory statistics class. We’ll also recode the GenderCode column to be more descriptive than 0 or 1. We’ll do a t-test to examine if there is evidence that males and females have a different SAT score at the college these data were take. data(&#39;GPAGender&#39;, package=&#39;Lock5Data&#39;) GPAGender &lt;- GPAGender %&gt;% mutate( Gender = ifelse(GenderCode == 1, &#39;Male&#39;, &#39;Female&#39;)) ggplot(GPAGender, aes(x=Gender, y=SAT)) + geom_boxplot() We’ll use the function t.test() using the formula interface for specifying the response and the explantory variables. The usual practice should be to save the output of the t.test function call (typically as an object named model or model_1 or similar). Once the model has been fit, all of the important quantities have been calculated and saved and we just need to ask for them. Unfortunately, the base functions in R don’t make this particularly easy, but the tidymodels group of packages for building statistical models allows us to wrap all of the important information into a data frame with one row. In this case we will use the broom::tidy() function to extract all the important model results information. model &lt;- t.test(SAT ~ Gender, data=GPAGender) print(model) # print the summary information to the screen ## ## Welch Two Sample t-test ## ## data: SAT by Gender ## t = -1.4135, df = 323.26, p-value = 0.1585 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -44.382840 7.270083 ## sample estimates: ## mean in group Female mean in group Male ## 1195.702 1214.258 broom::tidy(model) # all that information as a data frame ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -18.6 1196. 1214. -1.41 0.158 323. -44.4 ## conf.high method alternative ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 7.27 Welch Two Sample t-test two.sided In the t.test function, the default behavior is to perform a test with a two-sided alternative and to calculate a 95% confidence interval. Those can be adjusted using the alternative and conf.level arguments. See the help documentation for t.test() function to see how to adust those. The t.test function can also be used without using a formula by inputing a vector of response variables for the first group and a vector of response variables for the second. The following results in the same model as the formula based interface. male_SATs &lt;- GPAGender %&gt;% filter( Gender == &#39;Male&#39; ) %&gt;% pull(SAT) female_SATs &lt;- GPAGender %&gt;% filter( Gender == &#39;Female&#39; ) %&gt;% pull(SAT) model &lt;- t.test( male_SATs, female_SATs ) broom::tidy(model) # all that information as a data frame ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18.6 1214. 1196. 1.41 0.158 323. -7.27 ## conf.high method alternative ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 44.4 Welch Two Sample t-test two.sided 5.2.1.2 Paired t-tests In a paired t-test, there is some mechanism for pairing observations in the two categories. For example, perhaps we observe the maximum weight lifted by a strongman competitor while wearing a weight belt vs not wearing the belt. Then we look at the difference between the weights lifted for each athlete. In the example we’ll look at here, we have the ages of 100 randomly selected married heterosexual couples from St. Lawerence County, NY. For any given man in the study, the obvious woman to compare his age to is his wife’s. So a paired test makes sense to perform. data(&#39;MarriageAges&#39;, package=&#39;Lock5Data&#39;) str(MarriageAges) ## &#39;data.frame&#39;: 105 obs. of 2 variables: ## $ Husband: int 53 38 46 30 31 26 29 48 65 29 ... ## $ Wife : int 50 34 44 36 23 31 25 51 46 26 ... ggplot(MarriageAges, aes(x=Husband, y=Wife)) + geom_point() + labs(x=&quot;Husband&#39;s Age&quot;, y=&quot;Wife&#39;s Age&quot;) To do a paired t-test, all we need to do is calculate the difference in age for each couple and pass that into the t.test() function. MarriageAges &lt;- MarriageAges %&gt;% mutate( Age_Diff = Husband - Wife) t.test( MarriageAges$Age_Diff) ## ## One Sample t-test ## ## data: MarriageAges$Age_Diff ## t = 5.8025, df = 104, p-value = 7.121e-08 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 1.861895 3.795248 ## sample estimates: ## mean of x ## 2.828571 Alternatively, we could pass the vector of Husband ages and the vector of Wife ages into the t.test() function and tell it that the data is paired so that the first husband is paired with the first wife. t.test( MarriageAges$Husband, MarriageAges$Wife, paired=TRUE ) ## ## Paired t-test ## ## data: MarriageAges$Husband and MarriageAges$Wife ## t = 5.8025, df = 104, p-value = 7.121e-08 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1.861895 3.795248 ## sample estimates: ## mean of the differences ## 2.828571 Either way that the function is called, the broom::tidy() function could convert the printed output into a nice data frame which can then be used in further analysis. 5.2.2 lm objects The general linear model function lm is more widely used than t.test because lm can be made to perform a t-test and the general linear model allows for fitting more than one explanatory variable and those variables could be either categorical or continuous. The general workflow will be to: Visualize the data Call lm() using a formula to specify the model to fit. Save the results of the lm() call to some object (usually I name it model) Use accessor functions to ask for pertainent quantities that have already been calculated. Store prediction values and model confidence intervals for each data point in the original data frame. Graph the original data along with prediction values and model confidence intervals. To explore this topic we’ll use the iris data set to fit a regression model to predict petal length using sepal length. ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) + geom_point() Now suppose we want to fit a regression model to these data and allow each species to have its own slope. We would fit the interaction model model &lt;- lm( Petal.Length ~ Sepal.Length * Species, data = iris ) 5.3 Accessor function Once a model has been fit, we want to obtain a variety of information from the model object. The way that we get most all of this information using base R commands is to call the summary() function which returns a list and then grab whatever we want out of that. Typically for a report, we could just print out all of the summary information and let the reader pick out the information. summary(model) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length * Species, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.68611 -0.13442 -0.00856 0.15966 0.79607 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8031 0.5310 1.512 0.133 ## Sepal.Length 0.1316 0.1058 1.244 0.216 ## Speciesversicolor -0.6179 0.6837 -0.904 0.368 ## Speciesvirginica -0.1926 0.6578 -0.293 0.770 ## Sepal.Length:Speciesversicolor 0.5548 0.1281 4.330 2.78e-05 *** ## Sepal.Length:Speciesvirginica 0.6184 0.1210 5.111 1.00e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2611 on 144 degrees of freedom ## Multiple R-squared: 0.9789, Adjusted R-squared: 0.9781 ## F-statistic: 1333 on 5 and 144 DF, p-value: &lt; 2.2e-16 But if we want to make a nice graph that includes the model’s \\(R^2\\) value on it, we need to code some way of grabbing particular bits of information from the model fit and wrestling into a format that we can easily manipulate it. Goal Base R command tidymodels version Summary table of Coefficients summary(model)$coef broom::tidy(model) Parameter Confidence Intervals confint(model) broom::tidy(model, conf.int=TRUE) Rsq and Adj-Rsq summary(model)$r.squared broom::glance(model) Model predictions predict(model) broom::augment(model) Model residuals resid(model) broom::augment(model) Model predictions w/ CI predict(model, interval='confidence') Model predictions w/ PI predict(model, interval='prediction') ANOVA table of model fit anova(model) The package broom has three ways to interact with a model. The tidy command gives a nice table of the model coefficents. The glance function gives information about how well the model fits the data overall. The augment function adds the fitted values, residuals, and other diagnostic information to the original data frame used to generate the model. Most of the time, I use the base R commands for accessing information from a model and only resort to the broom commands when I need to access very specific quantities. head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # Remove any previous model prediction values that I&#39;ve added, # and then add the model predictions iris &lt;- iris %&gt;% select( -matches(&#39;fit&#39;), -matches(&#39;lwr&#39;), -matches(&#39;upr&#39;) ) %&gt;% cbind( predict(model, newdata=., interval=&#39;confidence&#39;) ) head(iris, n=3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species fit ## 1 5.1 3.5 1.4 0.2 setosa 1.474373 ## 2 4.9 3.0 1.4 0.2 setosa 1.448047 ## 3 4.7 3.2 1.3 0.2 setosa 1.421721 ## lwr upr ## 1 1.398783 1.549964 ## 2 1.371765 1.524329 ## 3 1.324643 1.518798 Now that the fitted values that define the regression lines and the associated confidence interval band information has been added to my iris data set, we can now plot the raw data and the regression model predictions. ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) + geom_point() + geom_line( aes(y=fit) ) + geom_ribbon( aes( ymin=lwr, ymax=upr), alpha=.3 ) # alpha is the ribbon transparency Now to add the R-squared value to the graph, we need to add a simple text layer. To do that, I’ll make a data frame that has the information, and then add the x and y coordinates for where it should go. Rsq_data &lt;- broom::glance(model) %&gt;% select(r.squared) %&gt;% mutate(r.squared = round(r.squared, digits=3)) %&gt;% # only 3 digits of precision mutate(r.squared = paste(&#39;Rsq =&#39;, r.squared)) %&gt;% # append some text before mutate( Sepal.Length =7, Petal.Length=2.5) # set the location to place it Rsq_data ## # A tibble: 1 x 3 ## r.squared Sepal.Length Petal.Length ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rsq = 0.979 7 2.5 ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) + geom_point( ) + geom_line( aes(y=fit) ) + geom_ribbon( aes( ymin=lwr, ymax=upr), alpha=.3 ) + # alpha is the ribbon transparency geom_label( data=Rsq_data, aes(label=r.squared, color=NULL) ) 5.4 Exercises Using the trees data frame that comes pre-installed in R, fit the regression model that uses the tree Height to explain the Volume of wood harvested from the tree. Graph the data Fit a lm model using the command model &lt;- lm(Volume ~ Height, data=trees). Print out a table with just the regression coefficients, standard error, and upper and lower 95% confidence intervals. Add the model fitted values to the trees data frame along with the regression model confidence intervals. Graph the data and model result. Add the R-squared value as an annotation to the graph. The data set phbirths from the faraway package contains information birth weight, gestational length, and smoking status of mother. We’ll fit a quadratic model to predict infant birthweight using the gestational time. Create scatterplots of gestational length and birthweight for each smoking status. Remove all the observations that are premature (less than 36 weeks). Fit the quadratic model grams ~ poly(gestate,2) * smoke. Add the model fitted values to the phbirths data frame along with the regression model confidence intervals. Graph the data and model result. Create a column for the residuals in the phbirths data set. Create a histogram of the residuals. "],
["6-flow-control.html", "Chapter 6 Flow Control 6.1 Decision statements 6.2 Loops 6.3 Exercises", " Chapter 6 Flow Control library(tidyverse, quietly = TRUE) # loading ggplot2 and dplyr Often it is necessary to write scripts that perform different action depending on the data or to automate a task that must be repeated many times. To address these issues we will introduce the if statement and its closely related cousin if else. To address repeated tasks we will define two types of loops, a while loop and a for loop. 6.1 Decision statements 6.1.1 In dplyr wrangling A very common task within a data wrangling pipeline is to create a new column that recodes information in another column. Consider the following data frame that has name, gender, and political party affiliation of six individuals. In this example, we’ved coded male/female as 1/0 and political party as 1,2,3 for democratic, republican, and independent. people &lt;- data.frame( name = c(&#39;Michelle&#39;, &#39;Barack&#39;, &#39;George&#39;, &#39;Laura&#39;, &#39;Bernie&#39;, &#39;Deborah&#39;), gender = c(0,1,1,0,1,0), party = c(1,1,2,2,3,3) ) people ## name gender party ## 1 Michelle 0 1 ## 2 Barack 1 1 ## 3 George 1 2 ## 4 Laura 0 2 ## 5 Bernie 1 3 ## 6 Deborah 0 3 The command ifelse() works quite well within a dplyr::mutate() command and it responds correctly to vectors. The syntax is ifelse( logical.expression, TrueValue, FalseValue ). people %&gt;% mutate( gender2 = ifelse( gender == 0, &#39;Female&#39;, &#39;Male&#39;) ) ## name gender party gender2 ## 1 Michelle 0 1 Female ## 2 Barack 1 1 Male ## 3 George 1 2 Male ## 4 Laura 0 2 Female ## 5 Bernie 1 3 Male ## 6 Deborah 0 3 Female To do something similar for the case where we have 3 or more categories, we can use the ifelse() command repeatedly to address each category level seperately. people %&gt;% mutate( party2 = ifelse( party == 1, &#39;Democratic&#39;, party), party2 = ifelse( party2 == 2, &#39;Republican&#39;, &#39;Independent&#39;) ) ## name gender party party2 ## 1 Michelle 0 1 Independent ## 2 Barack 1 1 Independent ## 3 George 1 2 Republican ## 4 Laura 0 2 Republican ## 5 Bernie 1 3 Independent ## 6 Deborah 0 3 Independent The same results can be obtained more easily using the forcats::fct_recode() function. See the Factors chapter in this book. 6.1.2 General if else While programming, I often need to perform expressions that are more complicated than what the ifelse() command can do. The general format of an if or and if else is presented here. # Simplest version if( logical.test ){ expression # can be many lines of code } # Including the optional else if( logical.test ){ expression }else{ expression } where the else part is optional. Suppose that I have a piece of code that generates a random variable from the Binomial distribution with one sample (essentially just flipping a coin) but I’d like to label it head or tails instead of one or zero. # Flip the coin, and we get a 0 or 1 result &lt;- rbinom(n=1, size=1, prob=0.5) result ## [1] 1 # convert the 0/1 to Tail/Head if( result == 0 ){ result &lt;- &#39;Tail&#39; }else{ result &lt;- &#39;Head&#39; } result ## [1] &quot;Head&quot; What is happening is that the test expression inside the if() is evaluated and if it is true, then the subsequent statement is executed. If the test expression is false, the next statement is skipped. The way the R language is defined, only the first statement after the if statement is executed (or skipped) depending on the test expression. If we want multiple statements to be executed (or skipped), we will wrap those expressions in curly brackets { }. I find it easier to follow the if else logic when I see the curly brackets so I use them even when there is only one expression to be executed. Also notice that the RStudio editor indents the code that might be skipped to try help give you a hint that it will be conditionally evaluated. # Flip the coin, and we get a 0 or 1 result &lt;- rbinom(n=1, size=1, prob=0.5) result ## [1] 0 # convert the 0/1 to Tail/Head if( result == 0 ){ result &lt;- &#39;Tail&#39; print(&quot; in the if statement, got a Tail! &quot;) }else{ result &lt;- &#39;Head&#39; print(&quot;In the else part!&quot;) } ## [1] &quot; in the if statement, got a Tail! &quot; result ## [1] &quot;Tail&quot; Run this code several times until you get both cases several times. Notice that in the Evironment tab in RStudio, the value of the variable result changes as you execute the code repeatedly. Finally we can nest if else statements together to allow you to write code that has many different execution routes. # randomly grab a number between 0,5 and round it up to 1,2, ..., 5 birth.order &lt;- ceiling( runif(1, 0,5) ) if( birth.order == 1 ){ print(&#39;The first child had more rules to follow&#39;) }else if( birth.order == 2 ){ print(&#39;The second child was ignored&#39;) }else if( birth.order == 3 ){ print(&#39;The third child was spoiled&#39;) }else{ # if birth.order is anything other than 1, 2 or 3 print(&#39;No more unfounded generalizations!&#39;) } ## [1] &quot;No more unfounded generalizations!&quot; To provide a more statistically interesting example of when we might use an if else statement, consider the calculation of a p-value in a 1-sample t-test with a two-sided alternative. Recall the calculate was: If the test statistic t is negative, then p-value = \\(2*P\\left(T_{df} \\le t \\right)\\) If the test statistic t is positive, then p-value = \\(2*P\\left(T_{df} \\ge t \\right)\\). # create some fake data n &lt;- 20 # suppose this had a sample size of 20 x &lt;- rnorm(n, mean=2, sd=1) # testing H0: mu = 0 vs Ha: mu =/= 0 t &lt;- ( mean(x) - 0 ) / ( sd(x)/sqrt(n) ) df &lt;- n-1 if( t &lt; 0 ){ p.value &lt;- 2 * pt(t, df) }else{ p.value &lt;- 2 * (1 - pt(t, df)) } # print the resulting p-value p.value ## [1] 6.914398e-06 This sort of logic is necessary for the calculation of p-values and so something similar is found somewhere inside the t.test() function. 6.2 Loops It is often desirable to write code that does the same thing over and over, relieving you of the burden of repetitive tasks. To do this we’ll need a way to tell the computer to repeat some section of code over and over. However we’ll usually want something small to change each time through the loop and some way to tell the computer how many times to run the loop or when to stop repeating. 6.2.1 while Loops The basic form of a while loop is as follows: # while loop with multiple lines to be repeated while( logical ){ expression1 # multiple lines of R code expression2 } The computer will first evaluate the test expression. If it is true, it will execute the code once. It will then evaluate the test expression again to see if it is still true, and if so it will execute the code section a third time. The computer will continue with this process until the test expression finally evaluates as false. x &lt;- 2 while( x &lt; 100 ){ print( paste(&quot;In loop and x is now:&quot;, x) ) # print out current value of x x &lt;- 2*x } ## [1] &quot;In loop and x is now: 2&quot; ## [1] &quot;In loop and x is now: 4&quot; ## [1] &quot;In loop and x is now: 8&quot; ## [1] &quot;In loop and x is now: 16&quot; ## [1] &quot;In loop and x is now: 32&quot; ## [1] &quot;In loop and x is now: 64&quot; It is very common to forget to update the variable used in the test expression. In that case the test expression will never be false and the computer will never stop. This unfortunate situation is called an infinite loop. # Example of an infinite loop! Do not Run! x &lt;- 1 while( x &lt; 10 ){ print(x) } 6.2.2 for Loops Often we know ahead of time exactly how many times we should go through the loop. We could use a while loop, but there is also a second construct called a for loop that is quite useful. The format of a for loop is as follows: for( index in vector ){ expression1 expression2 } where the index variable will take on each value in vector in succession and then statement will be evaluated. As always, statement can be multiple statements wrapped in curly brackets {}. for( i in 1:5 ){ print( paste(&quot;In the loop and current value is i =&quot;, i) ) } ## [1] &quot;In the loop and current value is i = 1&quot; ## [1] &quot;In the loop and current value is i = 2&quot; ## [1] &quot;In the loop and current value is i = 3&quot; ## [1] &quot;In the loop and current value is i = 4&quot; ## [1] &quot;In the loop and current value is i = 5&quot; What is happening is that i starts out as the first element of the vector c(1,2,3,4,5), in this case, i starts out as 1. After i is assigned, the statements in the curly brackets are then evaluated. Once we get to the end of those statements, i is reassigned to the next element of the vector c(1,2,3,4,5). This process is repeated until i has been assigned to each element of the given vector. It is somewhat traditional to use i and j and the index variables, but they could be anything. We can use this loop to calculate the first \\(10\\) elements of the Fibonacci sequence. Recall that the Fibonacci sequence is defined by \\(F_{n}=F_{n-1}+F_{n-2}\\) where \\(F_{1}=0\\) and \\(F_{2}=1\\). F &lt;- rep(0, 10) # initialize a vector of zeros F[1] &lt;- 0 # F[1] should be zero F[2] &lt;- 1 # F[2] should be 1 print(F) # Show the value of F before the loop ## [1] 0 1 0 0 0 0 0 0 0 0 for( n in 3:10 ){ F[n] &lt;- F[n-1] + F[n-2] # define based on the prior two values print(F) # show F at each step of the loop } ## [1] 0 1 1 0 0 0 0 0 0 0 ## [1] 0 1 1 2 0 0 0 0 0 0 ## [1] 0 1 1 2 3 0 0 0 0 0 ## [1] 0 1 1 2 3 5 0 0 0 0 ## [1] 0 1 1 2 3 5 8 0 0 0 ## [1] 0 1 1 2 3 5 8 13 0 0 ## [1] 0 1 1 2 3 5 8 13 21 0 ## [1] 0 1 1 2 3 5 8 13 21 34 For a more statistical case where we might want to perform a loop, we can consider the creation of the bootstrap estimate of a sampling distribution. library(dplyr) library(ggplot2) SampDist &lt;- data.frame() # Make a data frame to store the means for( i in 1:1000 ){ SampDist &lt;- trees %&gt;% dplyr::sample_frac(replace=TRUE) %&gt;% dplyr::summarise(xbar=mean(Height)) %&gt;% # 1x1 data frame rbind( SampDist ) } ggplot(SampDist, aes(x=xbar)) + geom_histogram( binwidth=0.25) 6.3 Exercises I’ve created a dataset about presidential candidates for the 2020 US election and it is available on the github website for my STA 141 prez &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/dereksonderegger/444/master/data-raw/Prez_Candidate_Birthdays&#39;) prez ## # A tibble: 11 x 5 ## Candidate Gender Birthday Party AgeOnElection ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Pete Buttigieg M 1982-01-19 D 38 ## 2 Andrew Yang M 1975-01-13 D 45 ## 3 Juilan Castro M 1976-09-16 D 44 ## 4 Beto O&#39;Rourke M 1972-09-26 D 48 ## 5 Cory Booker M 1969-04-27 D 51 ## 6 Kamala Harris F 1964-10-20 D 56 ## 7 Amy Klobucher F 1960-05-25 D 60 ## 8 Elizabeth Warren F 1949-06-22 D 71 ## 9 Donald Trump M 1946-06-14 R 74 ## 10 Joe Biden M 1942-11-20 D 77 ## 11 Bernie Sanders M 1941-09-08 D 79 Recode the Gender column to have Male and Female levels. Similarly convert the party variable to be Democratic or Republican. Bernie Sanders was registered as an Independent up until his 2016 presidential run. Change his political party value into ‘Independent’. The \\(Uniform\\left(a,b\\right)\\) distribution is defined on x \\(\\in [a,b]\\) and represents a random variable that takes on any value of between a and b with equal probability. Technically since there are an infinite number of values between a and b, each value has a probability of 0 of being selected and I should say each interval of width \\(d\\) has equal probability. It has the density function \\[f\\left(x\\right)=\\begin{cases} \\frac{1}{b-a} &amp; \\;\\;\\;\\;a\\le x\\le b\\\\ 0 &amp; \\;\\;\\;\\;\\textrm{otherwise} \\end{cases}\\] The R function dunif() evaluates this density function for the above defined values of x, a, and b. Somewhere in that function, there is a chunk of code that evaluates the density for arbitrary values of \\(x\\). Run this code a few times and notice sometimes the result is \\(0\\) and sometimes it is \\(1/(10-4)=0.16666667\\). a &lt;- 4 # The min and max values we will use for this example b &lt;- 10 # Could be anything, but we need to pick something x &lt;- runif(n=1, 0,10) # one random value between 0 and 10 # what is value of f(x) at the randomly selected x value? dunif(x, a, b) ## [1] 0.1666667 We will write a sequence of statements that utilizes an if statements to appropriately calculate the density of x assuming that a, b , and x are given to you, but your code won’t know if x is between a and b. That is, your code needs to figure out if it is and give either 1/(b-a) or 0. We could write a set of if/else statements a &lt;- 4 b &lt;- 10 x &lt;- runif(n=1, 0,10) # one random value between 0 and 10 if( x &lt; a ){ result &lt;- ??? }else if( x &lt;= b ){ result &lt;- ??? }else{ result &lt;- ??? } print(paste(&#39;x=&#39;,round(x,digits=3), &#39; result=&#39;, round(result,digits=3))) Replace the ??? with the appropriate value, either 0 or \\(1/\\left(b-a\\right)\\). Run the code repeatedly until you are certain that it is calculating the correct density value. We could perform the logical comparison all in one comparison. Recall that we can use &amp; to mean “and” and | to mean “or”. In the following two code chunks, replace the ??? with either &amp; or | to make the appropriate result. x &lt;- runif(n=1, 0,10) # one random value between 0 and 10 if( (a&lt;=x) ??? (x&lt;=b) ){ result &lt;- 1/(b-a) }else{ result &lt;- 0 } print(paste(&#39;x=&#39;,round(x,digits=3), &#39; result=&#39;, round(result,digits=3))) x &lt;- runif(n=1, 0,10) # one random value between 0 and 10 if( (x&lt;a) ??? (b&lt;x) ){ result &lt;- 0 }else{ result &lt;- 1/(b-a) } print(paste(&#39;x=&#39;,round(x,digits=3), &#39; result=&#39;, round(result,digits=3))) x &lt;- runif(n=1, 0,10) # one random value between 0 and 10 result &lt;- ifelse( a&lt;x &amp; x&lt;b, ???, ??? ) print(paste(&#39;x=&#39;,round(x,digits=3), &#39; result=&#39;, round(result,digits=3))) I often want to repeat some section of code some number of times. For example, I might want to create a bunch plots that compare the density of a t-distribution with specified degrees of freedom to a standard normal distribution. library(ggplot2) df &lt;- 4 N &lt;- 1000 x &lt;- seq(-4, 4, length=N) data &lt;- data.frame( x = c(x,x), y = c(dnorm(x), dt(x, df)), type = c( rep(&#39;Normal&#39;,N), rep(&#39;T&#39;,N) ) ) # make a nice graph myplot &lt;- ggplot(data, aes(x=x, y=y, color=type, linetype=type)) + geom_line() + labs(title = paste(&#39;Std Normal vs t with&#39;, df, &#39;degrees of freedom&#39;)) # actually print the nice graph we made print(myplot) Use a for loop to create similar graphs for degrees of freedom \\(2,3,4,\\dots,29,30\\). In retrospect, perhaps we didn’t need to produce all of those. Rewrite your loop so that we only produce graphs for \\(\\left\\{ 2,3,4,5,10,15,20,25,30\\right\\}\\) degrees of freedom. Hint: you can just modify the vector in the for statement to include the desired degrees of freedom. The for loop usually is the most natural one to use, but occasionally we have occasions where it is too cumbersome and a different sort of loop is appropriate. One example is taking a random sample from a truncated distribution. For example, I might want to take a sample from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) but for some reason need the answer to be larger than zero. One solution is to just sample from the given normal distribution until I get a value that is bigger than zero. mu &lt;- 0 sigma &lt;- 1 x &lt;- rnorm(1, mean=mu, sd=sigma) print(x) # start the while loop checking if x &lt; 0 # generate a new x value # print the new x value # end the while loop Replace the comments in the above code so that x is a random observation from the truncated normal distribution. "],
["7-factors.html", "Chapter 7 Factors 7.1 Creation and Structure 7.2 Change Labels 7.3 Reorder Levels 7.4 Add or substract Levels", " Chapter 7 Factors In R we can store categorical information as either strings or as factors. To a casual user, it often doesn’t matter how the information is stored because the modeling and graphing programs happily convert strings into factors whenever necessary. However a deeper understanding of how factors are stored and manipulated allows a user much finer control in the modeling and graphing. We will be interested in the following broad classes of manipulations: Edit Factor Labels Goal forcats function Manually change the label(s) fct_recode(f, new_label = &quot;old_label&quot;) Systematically change all labels fct_relabel(f, function) Reorder Levels Goal forcats function Set order manually fct_relevel(f, c('b', 'a', 'c')) Set order based on another vector fct_reorder(f, x) Set order based on which category is most frequent fct_infreq(f) Set order based on when they first appear fct_inorder(f) Reverse factor order fct_rev(f) Rotate order left or right fct_shift(f, steps) Add or Subtract Levels Goal forcats function Manually select categories to collapse into one fct_collapse(f, other = c('a','b')) 7.1 Creation and Structure R stores factors as a combination of a vector of category labels and vector of integers representing which category a data value belongs to. For example, lets create a vector of data relating to what soft drinks my siblings prefer. # A vector of character strings. drinks &lt;- c(&#39;Coke&#39;, &#39;Coke&#39;, &#39;Sprite&#39;, &#39;Pepsi&#39;, &#39;DietCoke&#39;) str(drinks) ## chr [1:5] &quot;Coke&quot; &quot;Coke&quot; &quot;Sprite&quot; &quot;Pepsi&quot; &quot;DietCoke&quot; # convert the vector of character strings into a factor vector drinks &lt;- factor(drinks) str(drinks) # Show the structure of the factor ## Factor w/ 4 levels &quot;Coke&quot;,&quot;DietCoke&quot;,..: 1 1 4 3 2 levels(drinks) # Print out the categories, notice the order matters! ## [1] &quot;Coke&quot; &quot;DietCoke&quot; &quot;Pepsi&quot; &quot;Sprite&quot; as.integer(drinks) # Print the category assigments ## [1] 1 1 4 3 2 Notice that the factor has levels “Coke”, “DietCoke”, “Pepsi”, and “Sprite” and that the order of these levels is very important because each observation is saved as an integer which denotes which category the observation belongs to. Because it takes less memory to store a single integer instead of potentially very long character string, factors are much more space efficient than storing the same data as strings. Because the factor has both a character string representation as well as numeric value, we can convert a factor to either one. as.character(drinks) ## [1] &quot;Coke&quot; &quot;Coke&quot; &quot;Sprite&quot; &quot;Pepsi&quot; &quot;DietCoke&quot; as.integer(drinks) ## [1] 1 1 4 3 2 7.2 Change Labels 7.3 Reorder Levels 7.4 Add or substract Levels "],
["8-importing-data.html", "Chapter 8 Importing Data 8.1 Working directory 8.2 Comma Separated Data 8.3 MS Excel 8.4 Exercises", " Chapter 8 Importing Data Reading data from external sources is necessary. It is most common for data to be in a data-frame like storage, such as a MS Excel workbook, so we will concentrate on reading data into a data.frame. In the typical way data is organized, we think of each column of data representing some trait or variable that we might be interested in. In general, we might wish to investigate the relationship between variables. In contrast, the rows of our data represent a single object on which the column traits are measured. For example, in a grade book for recording students scores throughout the semester, their is one row for every student and columns for each assignment. A greenhouse experiment dataset will have a row for every plant and columns for treatment type and biomass. 8.1 Working directory One concept that will be important is to recognize that every time you start up RStudio, it picks an appropriate working directory. This is the directory where it will first look for script files or data files. By default when you double click on an R script or Rmarkdown file to launch RStudio, it will set the working directory to be the directory that the file was in. Similarly, when you knit an Rmarkdown file, the working directory will be set to the directory where the Rmarkdown file is. For both of these reasons, I always program my scripts assuming that paths to any data files will be relative to where where my Rmarkdown file is. To set the working directory explicitly, you can use the GUI tools Session -&gt; Set Working Directory.... The functions that we will use in this lab all accept a character string that denotes the location of the file. This location could be a web address, it could be an absolute path on your computer, or it could be a path relative to the location of your Rmarkdown file. 'MyFile.csv' Look in the working directory for MyFile.csv. 'MyFolder/Myfile.csv' In the working directory, there is a subdirectory called MyFolder and inside that folder there is a filed called MyFile.csv. 8.2 Comma Separated Data To consider how data might be stored, we first consider the simplest file format… the comma separated values file. In this file time, each of the “cells” of data are separated by a comma. For example, the data file storing scores for three students might be as follows: Able, Dave, 98, 92, 94 Bowles, Jason, 85, 89, 91 Carr, Jasmine, 81, 96, 97 Typically when you open up such a file on a computer with Microsoft Excel installed, Excel will open up the file assuming it is a spreadsheet and put each element in its own cell. However, you can also open the file using a more primitive program (say Notepad in Windows, TextEdit on a Mac) you’ll see the raw form of the data. Having just the raw data without any sort of column header is problematic (which of the three exams was the final??). Ideally we would have column headers that store the name of the column. LastName, FirstName, Exam1, Exam2, FinalExam Able, Dave, 98, 92, 94 Bowles, Jason, 85, 89, 91 Carr, Jasmine, 81, 96, 97 To see another example, open the “Body Fat” dataset from the Lock\\(^{5}\\) introductory text book at the website [http://www.lock5stat.com/datasets/BodyFat.csv]. The first few rows of the file are as follows: Bodyfat,Age,Weight,Height,Neck,Chest,Abdomen,Ankle,Biceps,Wrist 32.3,41,247.25,73.5,42.1,117,115.6,26.3,37.3,19.7 22.5,31,177.25,71.5,36.2,101.1,92.4,24.6,30.1,18.2 22,42,156.25,69,35.5,97.8,86,24,31.2,17.4 12.3,23,154.25,67.75,36.2,93.1,85.2,21.9,32,17.1 20.5,46,177,70,37.2,99.7,95.6,22.5,29.1,17.7 To make R read in the data arranged in this format, we need to tell R three things: Where does the data live? Often this will be the name of a file on your computer, but the file could just as easily live on the internet (provided your computer has internet access). Is the first row data or is it the column names? What character separates the data? Some programs store data using tabs to distinguish between elements, some others use white space. R’s mechanism for reading in data is flexible enough to allow you to specify what the separator is. The primary function that we’ll use to read data from a file and into R is the function read.table(). This function has many optional arguments but the most commonly used ones are outlined in the table below. Argument Default What it does file A character string denoting the file location header FALSE Is the first line column headers? sep &quot; &quot; What character separates columns. &quot; &quot; == any whitespace skip 0 The number of lines to skip before reading data. This is useful when there are lines of text that describe the data or aren’t actual data na.strings ‘NA’ What values represent missing data. Can have multiple. E.g. c('NA', -9999) quote &quot; and ’ For character strings, what characters represent quotes. To read in the “Body Fat” dataset we could run the R command: BodyFat &lt;- read.table( file = &#39;http://www.lock5stat.com/datasets/BodyFat.csv&#39;, # where the data lives header = TRUE, # first line is column names sep = &#39;,&#39; ) # Data is sparated by commas str(BodyFat) ## &#39;data.frame&#39;: 100 obs. of 10 variables: ## $ Bodyfat: num 32.3 22.5 22 12.3 20.5 22.6 28.7 21.3 29.9 21.3 ... ## $ Age : int 41 31 42 23 46 54 43 42 37 41 ... ## $ Weight : num 247 177 156 154 177 ... ## $ Height : num 73.5 71.5 69 67.8 70 ... ## $ Neck : num 42.1 36.2 35.5 36.2 37.2 39.9 37.9 35.3 42.1 39.8 ... ## $ Chest : num 117 101.1 97.8 93.1 99.7 ... ## $ Abdomen: num 115.6 92.4 86 85.2 95.6 ... ## $ Ankle : num 26.3 24.6 24 21.9 22.5 22 23.7 21.9 24.8 25.2 ... ## $ Biceps : num 37.3 30.1 31.2 32 29.1 35.9 32.1 30.7 34.4 37.5 ... ## $ Wrist : num 19.7 18.2 17.4 17.1 17.7 18.9 18.7 17.4 18.4 18.7 ... Looking at the help file for read.table() we see that there are variants such as read.csv() that sets the default arguments to header and sep more intelligently. Also, there are many options to customize how R responds to different input. 8.3 MS Excel Commonly our data is stored as a MS Excel file. There are two approaches you could use to import the data into R. From within Excel, export the worksheet that contains your data as a comma separated values (.csv) file and proceed using the tools in the previous section. Use functions within R that automatically convert the worksheet into a .csv file and read it in. One package that works nicely for this is the readxl package. I generally prefer using option 2 because all of my collaborators can’t live without Excel and I’ve resigned myself to this. However if you have complicated formulas in your Excel file, it is often times safer to export it as a .csv file to guarantee the data imported into R is correct. Furthermore, other spreadsheet applications (such as Google sheets) requires you to export the data as a .csv file so it is good to know both paths. Because R can only import a complete worksheet, the desired data worksheet must be free of notes to yourself about how the data was collected, preliminary graphics, or other stuff that isn’t the data. I find it very helpful to have a worksheet in which I describe the sampling procedure and describe what each column means (and give the units!), then a second worksheet where the actual data is, and finally a third worksheet where my “Excel Only” collaborators have created whatever plots and summary statistics they need. The simplest package for importing Excel files seems to be the package readxl. Another package that does this is the XLConnect which does the Excel -&gt; .csv conversion using Java. Another package the works well is the xlsx package, but it also requires Java to be installed. The nice thing about these two packages is that they also allow you to write Excel files as well. The RODBC package allows R to connect to various databases and it is possible to make it consider an Excel file as an extremely crude database. The readxl package provides a function read_exel() that allows us to specify which sheet within the Excel file to read and what character specifies missing data (it assumes a blank cell is missing data if you don’t specifying anything). One annoying change between read.table() and read_excel() is that the argument for specifying where the file is is different (path= instead of file=). Another difference between the two is that read_excel() does not yet have the capability of handling a path that is a web address. From GitHub, download the files Example_1.xls, Example_2.xls, Example_3.xls and Example_4.xls from the directory [https://github.com/dereksonderegger/570L/tree/master/data-raw]. Place these files in the same directory that you store your course work or make a subdirectory data to store the files in. Make sure that the working directory that RStudio is using is that same directory (Session -&gt; Set Working Directory). # load the library that has the read.xls function. library(readxl) # Where does the data live relative to my current working location? # # In my directory where this Rmarkdown file lives, I have made a subdirectory # named &#39;data-raw&#39; to store all the data files. So the path to my data # file will be &#39;data-raw/Example_1.xls&#39;. # If you stored the files in the same directory as your RMarkdown script, you # don&#39;t have to add any additional information and you can just tell it the # file name &#39;Example_1.xls&#39; # Alternatively I could give the full path to this file starting at the root # directory which, for me, is &#39;~/GitHub/STA570L_Book/data-raw/Example_1.xls&#39; # but for Windows users it might be &#39;Z:/570L/Lab7/Example_1.xls&#39;. This looks # odd because Windows usually uses a backslash to represent the directory # structure, but a backslash has special meaning in R and so it wants # to separate directories via forwardslashes. # read the first worksheet of the Example_1 file data.1 &lt;- read_excel( &#39;data-raw/Example_1.xls&#39; ) # relative to this Rmarkdown file data.1 &lt;- read_excel(&#39;~/GitHub/570L/data-raw/Example_1.xls&#39;) # absolute path # read the second worksheet where the second worksheet is named &#39;data&#39; data.2 &lt;- read_excel(&#39;data-raw/Example_2.xls&#39;, sheet=2 ) data.2 &lt;- read_excel(&#39;data-raw/Example_2.xls&#39;, sheet=&#39;data&#39;) There is one additional problem that shows up while reading in Excel files. Blank columns often show up in Excel files because at some point there was some text in a cell that got deleted but a space remains and Excel still thinks there is data in the column. To fix this, you could find the cell with the space in it, or you can select a bunch of columns at the edge and delete the entire columns. Alternatively, you could remove the column after it is read into R using tools we’ll learn when we get to the Manipulating Data chapter. Open up the file Example_4.xls in Excel and confirm that the data sheet has name columns out to carb. Read in the data frame using the following code: data.4 &lt;- read_excel(&#39;./data-raw/Example_4.xls&#39;, sheet=&#39;data&#39;) # Extra Column Example ## New names: ## * `` -&gt; ...13 ## * `` -&gt; ...14 str(data.4) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 34 obs. of 14 variables: ## $ model: chr &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp : num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat : num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec : num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear : num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb : num 4 4 1 1 2 1 4 2 2 4 ... ## $ ...13: logi NA NA NA NA NA NA ... ## $ ...14: logi NA NA NA NA NA NA ... We notice that after reading in the data, there is an additional column that just has missing data (the NA stands for not available which means that the data is missing) and a row with just a single blank. Go back to the Excel file and go to row 4 column N and notice that the cell isn’t actually blank, there is a space. Delete the space, save the file, and then reload the data into R. You should notice that the extra columns are now gone. 8.4 Exercises Download from GitHub the data file Example_5.xls. Open it in Excel and figure out which sheet of data we should import into R. At the same time figure out how many initial rows need to be skipped. Import the data set into a data frame and show the structure of the imported data using the str() command. Make sure that your data has \\(n=31\\) observations and the three columns are appropriately named. "],
["9-data-manipulation.html", "Chapter 9 Data Manipulation 9.1 Classical functions for summarizing rows and columns 9.2 Package dplyr 9.3 Exercises", " Chapter 9 Data Manipulation # library(tidyverse) # Could load several of Dr Wickham&#39;s commonly used packages all at once. library(dplyr) # or just the one we&#39;ll use today. Most of the time, our data is in the form of a data frame and we are interested in exploring the relationships. This chapter explores how to manipulate data frames and methods. 9.1 Classical functions for summarizing rows and columns 9.1.1 summary() The first method is to calculate some basic summary statistics (minimum, 25th, 50th, 75th percentiles, maximum and mean) of each column. If a column is categorical, the summary function will return the number of observations in each category. # use the iris data set which has both numerical and categorical variables data( iris ) str(iris) # recall what columns we have ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... # display the summary for each column summary( iris ) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 9.1.2 apply() The summary function is convenient, but we want the ability to pick another function to apply to each column and possibly to each row. To demonstrate this, suppose we have data frame that contains students grades over the semester. # make up some data grades &lt;- data.frame( l.name = c(&#39;Cox&#39;, &#39;Dorian&#39;, &#39;Kelso&#39;, &#39;Turk&#39;), Exam1 = c(93, 89, 80, 70), Exam2 = c(98, 70, 82, 85), Final = c(96, 85, 81, 92) ) The apply() function will apply an arbitrary function to each row (or column) of a matrix or a data frame and then aggregate the results into a vector. # Because I can&#39;t take the mean of the last names column, # remove the name column scores &lt;- grades[,-1] scores ## Exam1 Exam2 Final ## 1 93 98 96 ## 2 89 70 85 ## 3 80 82 81 ## 4 70 85 92 # Summarize each column by calculating the mean. apply( scores, # what object do I want to apply the function to MARGIN=2, # rows = 1, columns = 2, (same order as [rows, cols] FUN=mean # what function do we want to apply ) ## Exam1 Exam2 Final ## 83.00 83.75 88.50 To apply a function to the rows, we just change which margin we want. We might want to calculate the average exam score for person. apply( scores, # what object do I want to apply the function to MARGIN=1, # rows = 1, columns = 2, (same order as [rows, cols] FUN=mean # what function do we want to apply ) ## [1] 95.66667 81.33333 81.00000 82.33333 This is useful, but it would be more useful to concatenate this as a new column in my grades data frame. average &lt;- apply( scores, # what object do I want to apply the function to MARGIN=1, # rows = 1, columns = 2, (same order as [rows, cols] FUN=mean # what function do we want to apply ) grades &lt;- cbind( grades, average ) # squish together grades ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 95.66667 ## 2 Dorian 89 70 85 81.33333 ## 3 Kelso 80 82 81 81.00000 ## 4 Turk 70 85 92 82.33333 There are several variants of the apply() function, and the variant I use most often is the function sapply(), which will apply a function to each element of a list or vector and returns a corresponding list or vector of results. 9.2 Package dplyr Many of the tools to manipulate data frames in R were written without a consistent syntax and are difficult use together. To remedy this, Hadley Wickham (the writer of ggplot2) introduced a package called plyr which was quite useful. As with many projects, his first version was good but not great and he introduced an improved version that works exclusively with data.frames called dplyr which we will investigate. The package dplyr strives to provide a convenient and consistent set of functions to handle the most common data frame manipulations and a mechanism for chaining these operations together to perform complex tasks. The Dr Wickham has put together a very nice introduction to the package that explains in more detail how the various pieces work and I encourage you to read it at some point. [http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html]. One of the aspects about the data.frame object is that R does some simplification for you, but it does not do it in a consistent manner. Somewhat obnoxiously character strings are always converted to factors and subsetting might return a data.frame or a vector or a scalar. This is fine at the command line, but can be problematic when programming. Furthermore, many operations are pretty slow using data.frame. To get around this, Dr Wickham introduced a modified version of the data.frame called a tibble. A tibble is a data.frame but with a few extra bits. For now we can ignore the differences. The pipe command %&gt;% allows for very readable code. The idea is that the %&gt;% operator works by translating the command a %&gt;% f(b) to the expression f(a,b). This operator works on any function and was introduced in the magrittr package. The beauty of this comes when you have a suite of functions that takes input arguments of the same type as their output. For example, if we wanted to start with x, and first apply function f(), then g(), and then h(), the usual R command would be h(g(f(x))) which is hard to read because you have to start reading at the innermost set of parentheses. Using the pipe command %&gt;%, this sequence of operations becomes x %&gt;% f() %&gt;% g() %&gt;% h(). Written Meaning a %&gt;% f(b) f(a,b) b %&gt;% f(a, .) f(a, b) x %&gt;% f() %&gt;% g() g( f(x) ) In dplyr, all the functions below take a data set as its first argument and outputs an appropriately modified data set. This will allow me to chain together commands in a readable fashion. The pipe command works with any function, not just the dplyr functions and I often find myself using it all over the place. 9.2.1 Verbs The foundational operations to perform on a data set are: Subsetting - Returns a with only particular columns or rows – select - Selecting a subset of columns by name or column number. – filter - Selecting a subset of rows from a data frame based on logical expressions. – slice - Selecting a subset of rows by row number. arrange - Re-ordering the rows of a data frame. mutate - Add a new column that is some function of other columns. summarise - calculate some summary statistic of a column of data. This collapses a set of rows into a single row. Each of these operations is a function in the package dplyr. These functions all have a similar calling syntax, the first argument is a data set, subsequent arguments describe what to do with the input data frame and you can refer to the columns without using the df$column notation. All of these functions will return a data set. 9.2.1.1 Subsetting with select, filter, and slice These function allows you select certain columns and rows of a data frame. 9.2.1.1.1 select() Often you only want to work with a small number of columns of a data frame. It is relatively easy to do this using the standard [,col.name] notation, but is often pretty tedious. # recall what the grades are grades ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 95.66667 ## 2 Dorian 89 70 85 81.33333 ## 3 Kelso 80 82 81 81.00000 ## 4 Turk 70 85 92 82.33333 I could select the columns Exam columns by hand, or by using an extension of the : operator # select( grades, Exam1, Exam2 ) # select from `grades` columns Exam1, Exam2 grades %&gt;% select( Exam1, Exam2 ) # Exam1 and Exam2 ## Exam1 Exam2 ## 1 93 98 ## 2 89 70 ## 3 80 82 ## 4 70 85 grades %&gt;% select( Exam1:Final ) # Columns Exam1 through Final ## Exam1 Exam2 Final ## 1 93 98 96 ## 2 89 70 85 ## 3 80 82 81 ## 4 70 85 92 grades %&gt;% select( -Exam1 ) # Negative indexing by name works ## l.name Exam2 Final average ## 1 Cox 98 96 95.66667 ## 2 Dorian 70 85 81.33333 ## 3 Kelso 82 81 81.00000 ## 4 Turk 85 92 82.33333 grades %&gt;% select( 1:2 ) # Can select column by column position ## l.name Exam1 ## 1 Cox 93 ## 2 Dorian 89 ## 3 Kelso 80 ## 4 Turk 70 The select() command has a few other tricks. There are functional calls that describe the columns you wish to select that take advantage of pattern matching. I generally can get by with starts_with(), ends_with(), and contains(), but there is a final operator matches() that takes a regular expression. grades %&gt;% select( starts_with(&#39;Exam&#39;) ) # Exam1 and Exam2 ## Exam1 Exam2 ## 1 93 98 ## 2 89 70 ## 3 80 82 ## 4 70 85 The dplyr::select function is quite handy, but there are several other packages out there that have a select function and we can get into trouble with loading other packages with the same function names. If I encounter the select function behaving in a weird manner or complaining about an input argument, my first remedy is to be explicit about it is the dplyr::select() function by appending the package name at the start. 9.2.1.1.2 filter() It is common to want to select particular rows where we have some logically expression to pick the rows. # select students with Final grades greater than 90 grades %&gt;% filter(Final &gt; 90) ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 95.66667 ## 2 Turk 70 85 92 82.33333 You can have multiple logical expressions to select rows and they will be logically combined so that only rows that satisfy all of the conditions are selected. The logicals are joined together using &amp; (and) operator or the | (or) operator and you may explicitly use other logicals. For example a factor column type might be used to select rows where type is either one or two via the following: type==1 | type==2. # select students with Final grades above 90 and # average score also above 90 grades %&gt;% filter(Final &gt; 90, average &gt; 90) ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 95.66667 # we could also use an &quot;and&quot; condition grades %&gt;% filter(Final &gt; 90 &amp; average &gt; 90) ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 95.66667 9.2.1.1.3 slice() When you want to filter rows based on row number, this is called slicing. # grab the first 2 rows grades %&gt;% slice(1:2) ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 95.66667 ## 2 Dorian 89 70 85 81.33333 9.2.1.2 arrange() We often need to re-order the rows of a data frame. For example, we might wish to take our grade book and sort the rows by the average score, or perhaps alphabetically. The arrange() function does exactly that. The first argument is the data frame to re-order, and the subsequent arguments are the columns to sort on. The order of the sorting column determines the precedent… the first sorting column is first used and the second sorting column is only used to break ties. grades %&gt;% arrange(l.name) ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 95.66667 ## 2 Dorian 89 70 85 81.33333 ## 3 Kelso 80 82 81 81.00000 ## 4 Turk 70 85 92 82.33333 The default sorting is in ascending order, so to sort the grades with the highest scoring person in the first row, we must tell arrange to do it in descending order using desc(column.name). grades %&gt;% arrange(desc(Final)) ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 95.66667 ## 2 Turk 70 85 92 82.33333 ## 3 Dorian 89 70 85 81.33333 ## 4 Kelso 80 82 81 81.00000 In a more complicated example, consider the following data and we want to order it first by Treatment Level and secondarily by the y-value. I want the Treatment level in the default ascending order (Low, Medium, High), but the y variable in descending order. # make some data dd &lt;- data.frame( Trt = factor(c(&quot;High&quot;, &quot;Med&quot;, &quot;High&quot;, &quot;Low&quot;), levels = c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;)), y = c(8, 3, 9, 9), z = c(1, 1, 1, 2)) dd ## Trt y z ## 1 High 8 1 ## 2 Med 3 1 ## 3 High 9 1 ## 4 Low 9 2 # arrange the rows first by treatment, and then by y (y in descending order) dd %&gt;% arrange(Trt, desc(y)) ## Trt y z ## 1 Low 9 2 ## 2 Med 3 1 ## 3 High 9 1 ## 4 High 8 1 9.2.1.3 mutate() I often need to create a new column that is some function of the old columns. This was often cumbersome. Consider code to calculate the average grade in my grade book example. grades$average &lt;- (grades$Exam1 + grades$Exam2 + grades$Final) / 3 Instead, we could use the mutate() function and avoid all the grades$ nonsense. grades %&gt;% mutate( average = (Exam1 + Exam2 + Final)/3 ) ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 95.66667 ## 2 Dorian 89 70 85 81.33333 ## 3 Kelso 80 82 81 81.00000 ## 4 Turk 70 85 92 82.33333 You can do multiple calculations within the same mutate() command, and you can even refer to columns that were created in the same mutate() command. grades %&gt;% mutate( average = (Exam1 + Exam2 + Final)/3, grade = cut(average, c(0, 60, 70, 80, 90, 100), # cut takes numeric variable c( &#39;F&#39;,&#39;D&#39;,&#39;C&#39;,&#39;B&#39;,&#39;A&#39;)) ) # and makes a factor ## l.name Exam1 Exam2 Final average grade ## 1 Cox 93 98 96 95.66667 A ## 2 Dorian 89 70 85 81.33333 B ## 3 Kelso 80 82 81 81.00000 B ## 4 Turk 70 85 92 82.33333 B We might look at this data frame and want to do some rounding. For example, I might want to take each numeric column and round it. In this case, the functions mutate_at() and mutate_if() allow us to apply a function to a particular column and save the output. # for each column, if it is numeric, apply the round() function to the column # while using any additional arguments. So round two digits. grades %&gt;% mutate_if( is.numeric, round, digits=2 ) ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 95.67 ## 2 Dorian 89 70 85 81.33 ## 3 Kelso 80 82 81 81.00 ## 4 Turk 70 85 92 82.33 The mutate_at() function works similarly, but we just have to specify with columns. # round columns 2 through 5 grades %&gt;% mutate_at( 2:5, round, digits=2 ) ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 95.67 ## 2 Dorian 89 70 85 81.33 ## 3 Kelso 80 82 81 81.00 ## 4 Turk 70 85 92 82.33 # round columns that start with &quot;ave&quot; grades %&gt;% mutate_at( vars(starts_with(&quot;ave&quot;)), round ) ## l.name Exam1 Exam2 Final average ## 1 Cox 93 98 96 96 ## 2 Dorian 89 70 85 81 ## 3 Kelso 80 82 81 81 ## 4 Turk 70 85 92 82 # These do not work because they doesn&#39;t evaluate to column indices. # I can only hope that at some point, this syntax works # # grades %&gt;% # mutate_at( starts_with(&quot;ave&quot;), round ) # # grades %&gt;% # mutate_at( Exam1:average, round, digits=2 ) Another situation I often run into is the need to select many columns, and calculate a sum or mean across them. Unfortunately the natural tidyverse way of doing this is a bit clumsy and I often resort to the following trick of using the base apply() function inside of a mutate command. Remember the . represents the data frame passed into the mutate function, so in each line we grab the appropriate columns and then stuff the result into apply and assign the output of the apply function to the new column. grades %&gt;% mutate( Exam.Total = select(., Exam1:Final) %&gt;% apply(1, sum) ) %&gt;% mutate( Exam.Avg = select(., Exam1:Final) %&gt;% apply(1, mean)) ## l.name Exam1 Exam2 Final average Exam.Total Exam.Avg ## 1 Cox 93 98 96 95.66667 287 95.66667 ## 2 Dorian 89 70 85 81.33333 244 81.33333 ## 3 Kelso 80 82 81 81.00000 243 81.00000 ## 4 Turk 70 85 92 82.33333 247 82.33333 9.2.1.4 summarise() By itself, this function is quite boring, but will become useful later on. Its purpose is to calculate summary statistics using any or all of the data columns. Notice that we get to chose the name of the new column. The way to think about this is that we are collapsing information stored in multiple rows into a single row of values. # calculate the mean of exam 1 grades %&gt;% summarise( mean.E1=mean(Exam1)) ## mean.E1 ## 1 83 We could calculate multiple summary statistics if we like. # calculate the mean and standard deviation grades %&gt;% summarise( mean.E1=mean(Exam1), stddev.E1=sd(Exam1) ) ## mean.E1 stddev.E1 ## 1 83 10.23067 If we want to apply the same statistic to each column, we use the summarise_all() command. We have to be a little careful here because the function you use has to work on every column (that isn’t part of the grouping structure (see group_by())). There are two variants summarize_at() and summarize_if() that give you a bit more flexibility. # calculate the mean and stddev of each column - Cannot do this to Names! grades %&gt;% select( Exam1:Final ) %&gt;% summarise_all( funs(mean, sd) ) ## Warning: funs() is soft deprecated as of dplyr 0.8.0 ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## This warning is displayed once per session. ## Exam1_mean Exam2_mean Final_mean Exam1_sd Exam2_sd Final_sd ## 1 83 83.75 88.5 10.23067 11.5 6.757712 grades %&gt;% summarise_if(is.numeric, funs(Xbar=mean, SD=sd) ) ## Exam1_Xbar Exam2_Xbar Final_Xbar average_Xbar Exam1_SD Exam2_SD Final_SD ## 1 83 83.75 88.5 85.08333 10.23067 11.5 6.757712 ## average_SD ## 1 7.078266 9.2.1.5 Miscellaneous functions There are some more function that are useful but aren’t as commonly used. For sampling the functions sample_n() and sample_frac() will take a sub-sample of either n rows or of a fraction of the data set. The function n() returns the number of rows in the data set. Finally rename() will rename a selected column. 9.2.2 Split, apply, combine Aside from unifying the syntax behind the common operations, the major strength of the dplyr package is the ability to split a data frame into a bunch of sub-data frames, apply a sequence of one or more of the operations we just described, and then combine results back together. We’ll consider data from an experiment from spinning wool into yarn. This experiment considered two different types of wool (A or B) and three different levels of tension on the thread. The response variable is the number of breaks in the resulting yarn. For each of the 6 wool:tension combinations, there are 9 replicated observations per wool:tension level. data(warpbreaks) str(warpbreaks) ## &#39;data.frame&#39;: 54 obs. of 3 variables: ## $ breaks : num 26 30 54 25 70 52 51 26 67 18 ... ## $ wool : Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ tension: Factor w/ 3 levels &quot;L&quot;,&quot;M&quot;,&quot;H&quot;: 1 1 1 1 1 1 1 1 1 2 ... The first we must do is to create a data frame with additional information about how to break the data into sub-data frames. In this case, I want to break the data up into the 6 wool-by-tension combinations. Initially we will just figure out how many rows are in each wool-by-tension combination. # group_by: what variable(s) shall we group on. # n() is a function that returns how many rows are in the # currently selected sub-dataframe warpbreaks %&gt;% group_by( wool, tension) %&gt;% # grouping summarise(n = n() ) # how many in each group ## # A tibble: 6 x 3 ## # Groups: wool [2] ## wool tension n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 A L 9 ## 2 A M 9 ## 3 A H 9 ## 4 B L 9 ## 5 B M 9 ## 6 B H 9 The group_by function takes a data.frame and returns the same data.frame, but with some extra information so that any subsequent function acts on each unique combination defined in the group_by. If you wish to remove this behavior, use group_by() to reset the grouping to have no grouping variable. Using the same summarise function, we could calculate the group mean and standard deviation for each wool-by-tension group. warpbreaks %&gt;% group_by(wool, tension) %&gt;% summarise( n = n(), # I added some formatting to show the mean.breaks = mean(breaks), # reader I am calculating several sd.breaks = sd(breaks)) # statistics. ## # A tibble: 6 x 5 ## # Groups: wool [2] ## wool tension n mean.breaks sd.breaks ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A L 9 44.6 18.1 ## 2 A M 9 24 8.66 ## 3 A H 9 24.6 10.3 ## 4 B L 9 28.2 9.86 ## 5 B M 9 28.8 9.43 ## 6 B H 9 18.8 4.89 If instead of summarizing each split, we might want to just do some calculation and the output should have the same number of rows as the input data frame. In this case I’ll tell dplyr that we are mutating the data frame instead of summarizing it. For example, suppose that I want to calculate the residual value \\[e_{ijk}=y_{ijk}-\\bar{y}_{ij\\cdot}\\] where \\(\\bar{y}_{ij\\cdot}\\) is the mean of each wool:tension combination. warpbreaks %&gt;% group_by(wool, tension) %&gt;% # group by wool:tension mutate(resid = breaks - mean(breaks)) %&gt;% # mean(breaks) of the group! head( ) # show the first couple of rows ## # A tibble: 6 x 4 ## # Groups: wool, tension [1] ## breaks wool tension resid ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 26 A L -18.6 ## 2 30 A L -14.6 ## 3 54 A L 9.44 ## 4 25 A L -19.6 ## 5 70 A L 25.4 ## 6 52 A L 7.44 9.2.3 Chaining commands together In the previous examples we have used the %&gt;% operator to make the code more readable but to really appreciate this, we should examine the alternative. Suppose we have the results of a small 5K race. The data given to us is in the order that the runners signed up but we want to calculate the results for each gender, calculate the placings, and the sort the data frame by gender and then place. We can think of this process as having three steps: Splitting Ranking Re-arranging. # input the initial data race.results &lt;- data.frame( name=c(&#39;Bob&#39;, &#39;Jeff&#39;, &#39;Rachel&#39;, &#39;Bonnie&#39;, &#39;Derek&#39;, &#39;April&#39;,&#39;Elise&#39;,&#39;David&#39;), time=c(21.23, 19.51, 19.82, 23.45, 20.23, 24.22, 28.83, 15.73), gender=c(&#39;M&#39;,&#39;M&#39;,&#39;F&#39;,&#39;F&#39;,&#39;M&#39;,&#39;F&#39;,&#39;F&#39;,&#39;M&#39;)) We could run all the commands together using the following code: arrange( mutate( group_by( race.results, # using race.results gender), # group by gender place = rank( time )), # mutate to calculate the place column gender, place) # arrange the result by gender and place ## # A tibble: 8 x 4 ## # Groups: gender [2] ## name time gender place ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Rachel 19.8 F 1 ## 2 Bonnie 23.4 F 2 ## 3 April 24.2 F 3 ## 4 Elise 28.8 F 4 ## 5 David 15.7 M 1 ## 6 Jeff 19.5 M 2 ## 7 Derek 20.2 M 3 ## 8 Bob 21.2 M 4 This is very difficult to read because you have to read the code from the inside out. Another (and slightly more readable) way to complete our task is to save each intermediate step of our process and then use that in the next step: temp.df0 &lt;- race.results %&gt;% group_by( gender) temp.df1 &lt;- temp.df0 %&gt;% mutate( place = rank(time) ) temp.df2 &lt;- temp.df1 %&gt;% arrange( gender, place ) It would be nice if I didn’t have to save all these intermediate results because keeping track of temp1 and temp2 gets pretty annoying if I keep changing the order of how things or calculated or add/subtract steps. This is exactly what %&gt;% does for me. race.results %&gt;% group_by( gender ) %&gt;% mutate( place = rank(time)) %&gt;% arrange( gender, place ) ## # A tibble: 8 x 4 ## # Groups: gender [2] ## name time gender place ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Rachel 19.8 F 1 ## 2 Bonnie 23.4 F 2 ## 3 April 24.2 F 3 ## 4 Elise 28.8 F 4 ## 5 David 15.7 M 1 ## 6 Jeff 19.5 M 2 ## 7 Derek 20.2 M 3 ## 8 Bob 21.2 M 4 9.3 Exercises The dataset ChickWeight tracks the weights of 48 baby chickens (chicks) feed four different diets. Load the dataset using data(ChickWeight) Look at the help files for the description of the columns. Remove all the observations except for observations from day 10 or day 20. Calculate the mean and standard deviation of the chick weights for each diet group on days 10 and 20. The OpenIntro textbook on statistics includes a data set on body dimensions. Load the file using Body &lt;- read.csv(&#39;http://www.openintro.org/stat/data/bdims.csv&#39;) The column sex is coded as a 1 if the individual is male and 0 if female. This is a non-intuitive labeling system. Create a new column sex.MF that uses labels Male and Female. Hint: recall either the factor() or cut() command! The columns wgt and hgt measure weight and height in kilograms and centimeters (respectively). Use these to calculate the Body Mass Index (BMI) for each individual where \\[BMI=\\frac{Weight\\,(kg)}{\\left[Height\\,(m)\\right]^{2}}\\] Double check that your calculated BMI column is correct by examining the summary statistics of the column. BMI values should be between 18 to 40 or so. Did you make an error in your calculation? The function cut takes a vector of continuous numerical data and creates a factor based on your give cut-points. # Define a continuous vector to convert to a factor x &lt;- 1:10 # divide range of x into three groups of equal length cut(x, breaks=3) ## [1] (0.991,4] (0.991,4] (0.991,4] (0.991,4] (4,7] (4,7] (4,7] ## [8] (7,10] (7,10] (7,10] ## Levels: (0.991,4] (4,7] (7,10] # divide x into four groups, where I specify all 5 break points cut(x, breaks = c(0, 2.5, 5.0, 7.5, 10)) ## [1] (0,2.5] (0,2.5] (2.5,5] (2.5,5] (2.5,5] (5,7.5] (5,7.5] ## [8] (7.5,10] (7.5,10] (7.5,10] ## Levels: (0,2.5] (2.5,5] (5,7.5] (7.5,10] # (0,2.5] (2.5,5] means 2.5 is included in first group # right=FALSE changes this to make 2.5 included in the second # divide x into 3 groups, but give them a nicer # set of group names cut(x, breaks=3, labels=c(&#39;Low&#39;,&#39;Medium&#39;,&#39;High&#39;)) ## [1] Low Low Low Low Medium Medium Medium High High High ## Levels: Low Medium High Create a new column of in the data frame that divides the age into decades (10-19, 20-29, 30-39, etc). Notice the oldest person in the study is 67. Body &lt;- Body %&gt;% mutate( Age.Grp = cut(age, breaks=c(10,20,30,40,50,60,70), right=FALSE)) Find the average BMI for each Sex-by-Age combination. "],
["10-user-defined-functions.html", "Chapter 10 User Defined Functions 10.1 Basic function definition 10.2 Parameter Defaults 10.3 Ellipses 10.4 Function Overloading 10.5 Scope 10.6 Exercises", " Chapter 10 User Defined Functions It is very important to be able to define a piece of programing logic that is repeated often. For example, I don’t want to have to always program the mathematical code for calculating the sample variance of a vector of data. Instead I just want to call a function that does everything for me and I don’t have to worry about the details. While hiding the computational details is nice, fundamentally writing functions allows us to think about our problems at a higher layer of abstraction. For example, most scientists just want to run a t-test on their data and get the appropriate p-value out; they want to focus on their problem and not how to calculate what the appropriate degrees of freedom are. Functions let us do that. 10.1 Basic function definition In the course of your analysis, it can be useful to define your own functions. The format for defining your own function is function.name &lt;- function(arg1, arg2, arg3){ statement1 statement2 } where arg1 is the first argument passed to the function and arg2 is the second. To illustrate how to define your own function, we will define a variance calculating function. # define my function my.var &lt;- function(x){ n &lt;- length(x) # calculate sample size xbar &lt;- mean(x) # calculate sample mean SSE &lt;- sum( (x-xbar)^2 ) # calculate sum of squared error v &lt;- SSE / ( n - 1 ) # &quot;average&quot; squared error return(v) # result of function is v } # create a vector that I wish to calculate the variance of test.vector &lt;- c(1,2,2,4,5) # calculate the variance using my function calculated.var &lt;- my.var( test.vector ) calculated.var ## [1] 2.7 Notice that even though I defined my function using x as my vector of data, and passed my function something named test.vector, R does the appropriate renaming.If my function doesn’t modify its input arguments, then R just passes a pointer to the inputs to avoid copying large amounts of data when you call a function. If your function modifies its input, then R will take the input data, copy it, and then pass that new copy to the function. This means that a function cannot modify its arguments. In Computer Science parlance, R does not allow for procedural side effects. Think of the variable x as a placeholder, with it being replaced by whatever gets passed into the function. When I call a function, the function might cause something to happen (e.g. draw a plot) or it might do some calculates the result is returned by the function and we might want to save that. Inside a function, if I want the result of some calculation saved, I return the result as the output of the function. The way I specify to do this is via the return statement. (Actually R doesn’t completely require this. But the alternative method is less intuitive and I strongly recommend using the return() statement for readability.) By writing a function, I can use the same chunk of code repeatedly. This means that I can do all my tedious calculations inside the function and just call the function whenever I want and happily ignore the details. Consider the function t.test() which we have used to do all the calculations in a t-test. We could write a similar function using the following code: # define my function one.sample.t.test &lt;- function(input.data, mu0){ n &lt;- length(input.data) xbar &lt;- mean(input.data) s &lt;- sd(input.data) t &lt;- (xbar - mu0)/(s / sqrt(n)) if( t &lt; 0 ){ p.value &lt;- 2 * pt(t, df=n-1) }else{ p.value &lt;- 2 * (1-pt(t, df=n-1)) } # we haven&#39;t addressed how to print things in a organized # fashion, the following is ugly, but works... # Notice that this function returns a character string # with the necessary information in the string. return( paste(&#39;t =&#39;, t, &#39; and p.value =&#39;, p.value) ) } # create a vector that I wish apply a one-sample t-test on. test.data &lt;- c(1,2,2,4,5,4,3,2,3,2,4,5,6) one.sample.t.test( test.data, mu0=2 ) ## [1] &quot;t = 3.15682074900988 and p.value = 0.00826952416706961&quot; Nearly every function we use to do data analysis is written in a similar fashion. Somebody decided it would be convenient to have a function that did an ANOVA analysis and they wrote something similar to the above function, but is a bit grander in scope. Even if you don’t end up writing any of your own functions, knowing how to will help you understand why certain functions you use are designed the way they are. 10.2 Parameter Defaults When I define a function and can let it take as many arguments as I want and I can also give default values to the arguments. For example we can define the normal density function using the following code which gives a default mean of \\(0\\) and default standard deviation of \\(1\\). # a function that defines the shape of a normal distribution. # by including mu=0, we give a default value that the function # user can override dnorm.alternate &lt;- function(x, mu=0, sd=1){ out &lt;- 1 / (sd * sqrt(2*pi)) * exp( -(x-mu)^2 / (2 * sd^2) ) return(out) } # test the function to see if it works dnorm.alternate(1) ## [1] 0.2419707 dnorm.alternate(1, mu=1) ## [1] 0.3989423 # Lets test the function a bit more by drawing the height # of the normal distribution a lots of different points # ... First the standard normal! x &lt;- seq(-3, 3, length=601) plot( x, dnorm.alternate(x) ) # use default mu=0, sd=1 # next a normal with mean 1, and standard deviation 1 plot( x, dnorm.alternate(x, mu=1) ) # override mu, but use sd=1 Many functions that we use have defaults that we don’t normally mess with. For example, the function mean() has an option the specifies what it should do if your vector of data has missing data. The common solution is to remove those observations, but we might have wanted to say that the mean is unknown one component of it was unknown. x &lt;- c(1,2,3,NA) # fourth element is missing mean(x) # default is to return NA if any element is missing ## [1] NA mean(x, na.rm=TRUE) # Only average the non-missing data ## [1] 2 As you look at the help pages for different functions, you’ll see in the function definitions what the default values are. For example, the function mean has another option, trim, which specifies what percent of the data to trim at the extremes. Because we would expect mean to not do any trimming by default, the authors have appropriately defined the default amount of trimming to be zero via the definition trim=0. 10.3 Ellipses When writing functions, I occasionally have a situation where I call function a() and function a() needs to call another function, say b(), and I want to pass an unusual parameter to that function. To do this, I’ll use a set of three periods called an ellipses. What these do is represent a set of parameter values that will be passed along to a subsequent function.For example the following code takes the result of a simple linear regression and plots the data and the regression line and confidence region (basically I’m recreating a function that does the same thing as ggplot2’s geom_smooth() layer). I might not want to specify (and give good defaults) to every single graphical parameter that the plot() function supports. Instead I’ll just use the ‘…’ argument and pass any additional parameters to the plot function. # a function that draws the regression line and confidence interval # notice it doesn&#39;t return anything... all it does is draw a plot show.lm &lt;- function(m, interval.type=&#39;confidence&#39;, fill.col=&#39;light grey&#39;, ...){ x &lt;- m$model[,2] # extract the predictor variable y &lt;- m$model[,1] # extract the response pred &lt;- predict(m, interval=interval.type) plot(x, y, ...) polygon( c(x,rev(x)), # draw the ribbon defined c(pred[,&#39;lwr&#39;], rev(pred[,&#39;upr&#39;])), # by lwr and upr - polygon col=&#39;light grey&#39;) # fills in the region defined by lines(x, pred[, &#39;fit&#39;]) # a set of vertices, need to reverse points(x, y) # the uppers to make a nice figure } This function looks daunting, but we experiment to see what it does. # first define a simple linear model from our cherry tree data m &lt;- lm( Volume ~ Girth, data=trees ) # call the function with no extraneous parameters show.lm( m ) # Pass arguments that will just be passed along to the plot function show.lm( m, xlab=&#39;Girth&#39;, ylab=&#39;Volume&#39;, main=&#39;Relationship between Girth and Volume&#39;) This type of trick is done commonly. Look at the help files for hist() and qqnorm() and you’ll see the ellipses used to pass graphical parameters along to sub-functions. Functions like lm() use the ellipses to pass arguments to the low level regression fitting functions that do the actual calculations. By only including these parameters via the ellipses, must users won’t be tempted to mess with the parameters, but experts who know the nitty-gritty details can still modify those parameters. 10.4 Function Overloading Frequently the user wants to inspect the results of some calculation and display a variable or object to the screen. The print() function does exactly that, but it acts differently for matrices than it does for vectors. It especially acts different for lists that I obtained from a call like lm() or aov(). The reason that the print function can act differently depending on the object type that I pass it is because the function print() is overloaded. What this means is that there is a print.lm() function that is called whenever I call print(obj) when obj is the output of an lm() command. Recall that we initially introduced a few different classes of data, Numerical, Factors, and Logicals. It turns out that I can create more types of classes. x &lt;- seq(1:10) y &lt;- 3+2*x+rnorm(10) h &lt;- hist(y) # h should be of class &quot;Histogram&quot; class(h) ## [1] &quot;histogram&quot; model &lt;- lm( y ~ x ) # model is something of class &quot;lm&quot; class(model) ## [1] &quot;lm&quot; Many common functions such as plot() are overloaded so that when I call the plot function with an object, it will in turn call plot.lm() or plot.histogram() as appropriate. When building statistical models I am often interested in different quantities and would like to get those regardless of the model I am using. Below are a list of functions that work whether I fit a model via aov(), lm(), glm(), or gam(). Quantity Function Name Residuals resid( obj ) Model Coefficients coef( obj ) Summary Table summary( obj ) ANOVA Table anova( obj ) AIC value AIC( obj ) For the residual function, there exists a resid.lm() function, and resid.gam() and it is these functions are called when we run the command resid( obj ). 10.5 Scope Consider the case where we make a function that calculates the trimmed mean. A good implementation of the function is given here. # Define a function for the trimmed mean # x: vector of values to be averaged # k: the number of elements to trim on either side trimmed.mean &lt;- function(x, k=0){ x &lt;- sort(x) # arrange the input according magnitude n &lt;- length(x) # n = how many observations if( k &gt; 0){ x &lt;- x[c(-1*(1:k), -1*((n-k+1):n))] # remove first k, last k } tm &lt;- sum(x) / length(x) # mean of the remaining observations return( tm ) } x &lt;- c(10:1,50) # 10, 9, 8, ..., 1 output &lt;- trimmed.mean(x, k=2) output ## [1] 6 x # x is unchanged ## [1] 10 9 8 7 6 5 4 3 2 1 50 Notice that even though I passed x into the function and then sorted it, x remained unsorted outside the function. When I modified x, R made a copy of x and sorted the copy that belonged to the function so that I didn’t modify a variable that was defined outside of the scope of my function. But what if I didn’t bother with passing x and k. If I don’t pass in the values of x and k, then R will try to find them in my current workspace. # a horribly defined function that has no parameters # but still accesses something called &quot;x&quot; trimmed.mean &lt;- function(){ x &lt;- sort(x) n &lt;- length(x) if( k &gt; 0){ x &lt;- x[c(-1*(1:k), -1*((n-k+1):n))] } tm &lt;- sum(x)/length(x) return( tm ) } x &lt;- c( 1:10, 50 ) # data to trim k &lt;- 2 trimmed.mean() # amazingly this still works ## [1] 6 # but what if k wasn&#39;t defined? rm(k) # remove k trimmed.mean() # now the function can&#39;t find anything named k and throws and error. ## Error in trimmed.mean(): object &#39;k&#39; not found So if I forget to pass some variable into a function, but it happens to be defined outside the function, R will find it. It is not good practice to rely on that because how do I take the trimmed mean of a vector named z? Worse yet, what if the variable x changes between runs of your function? What should be consistently giving the same result keeps changing. This is especially insidious when you have defined most of the arguments the function uses, but missed one. Your function happily goes to the next higher scope and sometimes finds it. When executing a function, R will have access to all the variables defined in the function, all the variables defined in the function that called your function and so on until the base workspace. However, you should never let your function refer to something that is not either created in your function or passed in via a parameter. 10.6 Exercises Write a function that calculates the density function of a Uniform continuous variable on the interval \\(\\left(a,b\\right)\\). The function is defined as \\[f\\left(x\\right)=\\begin{cases} \\frac{1}{b-a} &amp; \\;\\;\\;\\textrm{if }a\\le x\\le b\\\\ 0 &amp; \\;\\;\\;\\textrm{otherwise} \\end{cases}\\] which looks like this We want to write a function duniform(x, a, b) that takes an arbitrary value of x and parameters a and b and return the appropriate height of the density function. For various values of x, a, and b, demonstrate that your function returns the correct density value. Ideally, your function should be able to take a vector of values for x and return a vector of densities. I very often want to provide default values to a parameter that I pass to a function. For example, it is so common for me to use the pnorm() and qnorm() functions on the standard normal, that R will automatically use mean=0 and sd=1 parameters unless you tell R otherwise. To get that behavior, we just set the default parameter values in the definition. When the function is called, the user specified value is used, but if none is specified, the defaults are used. Look at the help page for the functions dunif(), and notice that there are a number of default parameters. For your duniform() function provide default values of 0 and 1 for a and b. Demonstrate that your function is appropriately using the given default values. "],
["11-string-manipulation.html", "Chapter 11 String Manipulation 11.1 Base function 11.2 Package stringr: basic operations 11.3 Package stringr: Pattern Matching 11.4 Regular Expressions 11.5 Exercises", " Chapter 11 String Manipulation Strings make up a very important class of data. Data being read into R often come in the form of character strings where different parts might mean different things. For example a sample ID of “R1_P2_C1_2012_05_28” might represent data from Region 1, Park 2, Camera 1, taken on May 28, 2012. It is important that we have a set of utilities that allow us to split and combine character strings in a easy and consistent fashion. Unfortunately, the utilities included in the base version of R are somewhat inconsistent and were not designed to work nicely together. Hadley Wickham, the developer of ggplot2 and dplyr has this to say: “R provides a solid set of string operations, but because they have grown organically over time, they can be inconsistent and a little hard to learn. Additionally, they lag behind the string operations in other programming languages, so that some things that are easy to do in languages like Ruby or Python are rather hard to do in R.” – Hadley Wickham For this chapter we will introduce the most commonly used functions from the base version of R that you might use or see in other people’s code. Second, we introduce Dr Wickham’s stringr package that provides many useful functions that operate in a consistent manner. 11.1 Base function 1.1.1 paste() The most basic thing we will want to do is to combine two strings or to combine a string with a numerical value. The paste() command will take one or more R objects and converts them to character strings and then pastes them together to form one or more character strings. It has the form: paste( ..., sep = &#39; &#39;, collapse = NULL ) The ... piece means that we can pass any number of objects to be pasted together. The sep argument gives the string that separates the strings to be joined and the collapse argument that specifies if a simplification should be performed after being pasting together. Suppose we want to combine the strings “Peanut butter” and “Jelly” then we could execute: paste( &quot;PeanutButter&quot;, &quot;Jelly&quot; ) ## [1] &quot;PeanutButter Jelly&quot; Notice that without specifying the separator character, R chose to put a space between the two strings. We could specify whatever we wanted: paste( &quot;Hello&quot;, &quot;World&quot;, sep=&#39;_&#39; ) ## [1] &quot;Hello_World&quot; Also we can combine strings with numerical values paste( &quot;Pi is equal to&quot;, pi ) ## [1] &quot;Pi is equal to 3.14159265358979&quot; We can combine vectors of similar or different lengths as well. By default R assumes that you want to produce a vector of character strings as output. paste( &quot;n =&quot;, c(5,25,100) ) ## [1] &quot;n = 5&quot; &quot;n = 25&quot; &quot;n = 100&quot; first.names &lt;- c(&#39;Robb&#39;,&#39;Stannis&#39;,&#39;Daenerys&#39;) last.names &lt;- c(&#39;Stark&#39;,&#39;Baratheon&#39;,&#39;Targaryen&#39;) paste( first.names, last.names) ## [1] &quot;Robb Stark&quot; &quot;Stannis Baratheon&quot; &quot;Daenerys Targaryen&quot; If we want paste() produce just a single string of output, use the collapse= argument to paste together each of the output vectors (separated by the collapse character). paste( &quot;n =&quot;, c(5,25,100) ) # Produces 3 strings ## [1] &quot;n = 5&quot; &quot;n = 25&quot; &quot;n = 100&quot; paste( &quot;n =&quot;, c(5,25,100), collapse=&#39;:&#39; ) # collapses output into one string ## [1] &quot;n = 5:n = 25:n = 100&quot; paste(first.names, last.names, sep=&#39;.&#39;, collapse=&#39; : &#39;) ## [1] &quot;Robb.Stark : Stannis.Baratheon : Daenerys.Targaryen&quot; Notice we could use the paste() command with the collapse option to combine a vector of character strings together. paste(first.names, collapse=&#39;:&#39;) ## [1] &quot;Robb:Stannis:Daenerys&quot; 11.2 Package stringr: basic operations The goal of stringr is to make a consistent user interface to a suite of functions to manipulate strings. “(stringr) is a set of simple wrappers that make R’s string functions more consistent, simpler and easier to use. It does this by ensuring that: function and argument names (and positions) are consistent, all functions deal with NA’s and zero length character appropriately, and the output data structures from each function matches the input data structures of other functions.” - Hadley Wickham We’ll investigate the most commonly used function but there are many we will ignore. Function Description str_c() string concatenation, similar to paste str_length() number of characters in the string str_sub() extract a substring str_trim() remove leading and trailing whitespace str_pad() pad a string with empty space to make it a certain length 11.2.1 Concatenating with str_c() or str_join() The first thing we do is to concatenate two strings or two vectors of strings similarly to the paste() command. The str_c() and str_join() functions are a synonym for the exact same function, but str_join() might be a more natural verb to use and remember. The syntax is: str_c( ..., sep=&#39;&#39;, collapse=NULL) You can think of the inputs building a matrix of strings, with each input creating a column of the matrix. For each row, str_c() first joins all the columns (using the separator character given in sep) into a single column of strings. If the collapse argument is non-NULL, the function takes the vector and joins each element together using collapse as the separator character. # load the stringr library library(stringr) # envisioning the matrix of strings cbind(first.names, last.names) ## first.names last.names ## [1,] &quot;Robb&quot; &quot;Stark&quot; ## [2,] &quot;Stannis&quot; &quot;Baratheon&quot; ## [3,] &quot;Daenerys&quot; &quot;Targaryen&quot; # join the columns together full.names &lt;- str_c( first.names, last.names, sep=&#39;.&#39;) cbind( first.names, last.names, full.names) ## first.names last.names full.names ## [1,] &quot;Robb&quot; &quot;Stark&quot; &quot;Robb.Stark&quot; ## [2,] &quot;Stannis&quot; &quot;Baratheon&quot; &quot;Stannis.Baratheon&quot; ## [3,] &quot;Daenerys&quot; &quot;Targaryen&quot; &quot;Daenerys.Targaryen&quot; # Join each of the rows together separated by collapse str_c( first.names, last.names, sep=&#39;.&#39;, collapse=&#39; : &#39;) ## [1] &quot;Robb.Stark : Stannis.Baratheon : Daenerys.Targaryen&quot; 11.2.2 Calculating string length with str_length() The str_length() function calculates the length of each string in the vector of strings passed to it. text &lt;- c(&#39;WordTesting&#39;, &#39;With a space&#39;, NA, &#39;Night&#39;) str_length( text ) ## [1] 11 12 NA 5 Notice that str_length() correctly interprets the missing data as missing and that the length ought to also be missing. 11.2.3 Extracting substrings with str_sub() If we know we want to extract the \\(3^{rd}\\) through \\(6^{th}\\) letters in a string, this function will grab them. str_sub(text, start=3, end=6) ## [1] &quot;rdTe&quot; &quot;th a&quot; NA &quot;ght&quot; If a given string isn’t long enough to contain all the necessary indices, str_sub() returns only the letters that where there (as in the above case for “Night” 11.2.4 Pad a string with str_pad() Sometimes we to make every string in a vector the same length to facilitate display or in the creation of a uniform system of assigning ID numbers. The str_pad() function will add spaces at either the beginning or end of the of every string appropriately. str_pad(first.names, width=8) ## [1] &quot; Robb&quot; &quot; Stannis&quot; &quot;Daenerys&quot; str_pad(first.names, width=8, side=&#39;right&#39;, pad=&#39;*&#39;) ## [1] &quot;Robb****&quot; &quot;Stannis*&quot; &quot;Daenerys&quot; 11.2.5 Trim a string with str_trim() This removes any leading or trailing whitespace where whitespace is defined as spaces ’ ’, tabs \\t or returns \\n. text &lt;- &#39; Some text. \\n &#39; print(text) ## [1] &quot; Some text. \\n &quot; str_trim(text) ## [1] &quot;Some text.&quot; 11.3 Package stringr: Pattern Matching The previous commands are all quite useful but the most powerful string operation is take a string and match some pattern within it. The following commands are available within stringr. Function Description str_detect() Detect if a pattern occurs in input string str_locate() str_locate_all() Locates the first (or all) positions of a pattern. str_extract() str_extract_all() Extracts the first (or all) substrings corresponding to a pattern str_replace() str_replace_all() Replaces the matched substring(s) with a new pattern str_split() str_split_fixed() Splits the input string based on the inputed pattern We will first examine these functions using a very simple pattern matching algorithm where we are matching a specific pattern. For most people, this is as complex as we need. Suppose that we have a vector of strings that contain a date in the form “2012-May-27” and we want to manipulate them to extract certain information. test.vector &lt;- c(&#39;2008-Feb-10&#39;, &#39;2010-Sept-18&#39;, &#39;2013-Jan-11&#39;, &#39;2016-Jan-2&#39;) 11.3.1 Detecting a pattern using str_detect() Suppose we want to know which dates are in September. We want to detect if the pattern “Sept” occurs in the strings. It is important that I used fixed(“Sept”) in this code to “turn off” the complicated regular expression matching rules and just look for exactly what I specified. str_detect( test.vector, pattern=fixed(&#39;Sept&#39;) ) ## [1] FALSE TRUE FALSE FALSE Here we see that the second string in the test vector included the substring “Sept” but none of the others did. 11.3.2 Locating a pattern using str_locate() To figure out where the “-” characters are, we can use the str_locate() function. str_locate(test.vector, pattern=fixed(&#39;-&#39;) ) ## start end ## [1,] 5 5 ## [2,] 5 5 ## [3,] 5 5 ## [4,] 5 5 which shows that the first dash occurs as the \\(5^{th}\\) character in each string. If we wanted all the dashes in the string the following works. str_locate_all(test.vector, pattern=fixed(&#39;-&#39;) ) ## [[1]] ## start end ## [1,] 5 5 ## [2,] 9 9 ## ## [[2]] ## start end ## [1,] 5 5 ## [2,] 10 10 ## ## [[3]] ## start end ## [1,] 5 5 ## [2,] 9 9 ## ## [[4]] ## start end ## [1,] 5 5 ## [2,] 9 9 The output of str_locate_all() is a list of matrices that gives the start and end of each matrix. Using this information, we could grab the Year/Month/Day information out of each of the dates. We won’t do that here because it will be easier to do this using str_split(). 11.3.3 Replacing substrings using str_replace() Suppose we didn’t like using “-” to separate the Year/Month/Day but preferred a space, or an underscore, or something else. This can be done by replacing all of the “-” with the desired character. The str_replace() function only replaces the first match, but str_replace_all() replaces all matches. str_replace(test.vector, pattern=fixed(&#39;-&#39;), replacement=fixed(&#39;:&#39;) ) ## [1] &quot;2008:Feb-10&quot; &quot;2010:Sept-18&quot; &quot;2013:Jan-11&quot; &quot;2016:Jan-2&quot; str_replace_all(test.vector, pattern=fixed(&#39;-&#39;), replacement=fixed(&#39;:&#39;) ) ## [1] &quot;2008:Feb:10&quot; &quot;2010:Sept:18&quot; &quot;2013:Jan:11&quot; &quot;2016:Jan:2&quot; 11.3.4 Splitting into substrings using str_split() We can split each of the dates into three smaller substrings using the str_split() command, which returns a list where each element of the list is a vector containing pieces of the original string (excluding the pattern we matched on). If we know that all the strings will be split into a known number of substrings (we have to specify how many substrings to match with the n= argument), we can use str_split_fixed() to get a matrix of substrings instead of list of substrings. It is somewhat unfortunate that the _fixed modifier to the function name is the same as what we use to specify to use simple pattern matching. str_split_fixed(test.vector, pattern=fixed(&#39;-&#39;), n=3) ## [,1] [,2] [,3] ## [1,] &quot;2008&quot; &quot;Feb&quot; &quot;10&quot; ## [2,] &quot;2010&quot; &quot;Sept&quot; &quot;18&quot; ## [3,] &quot;2013&quot; &quot;Jan&quot; &quot;11&quot; ## [4,] &quot;2016&quot; &quot;Jan&quot; &quot;2&quot; 11.4 Regular Expressions The next section will introduce using regular expressions. Regular expressions are a way to specify very complicated patterns. Go look at https://xkcd.com/208/ to gain insight into just how geeky regular expressions are. Regular expressions are a way of precisely writing out patterns that are very complicated. The stringr package pattern arguments can be given using standard regular expressions (not perl-style!) instead of using fixed strings. Regular expressions are extremely powerful for sifting through large amounts of text. For example, we might want to extract all of the 4 digit substrings (the years) out of our dates vector, or I might want to find all cases in a paragraph of text of words that begin with a capital letter and are at least 5 letters long. In another, somewhat nefarious example, spammers might have downloaded a bunch of text from webpages and want to be able to look for email addresses. So as a first pass, they want to match a pattern: \\[\\underset{\\textrm{1 or more letters}}{\\underbrace{\\texttt{Username}}}\\texttt{@}\\;\\;\\underset{\\textrm{1 or more letter}}{\\underbrace{\\texttt{OrganizationName}}}\\;\\texttt{.\\;}\\begin{cases} \\texttt{com}\\\\ \\texttt{org}\\\\ \\texttt{edu} \\end{cases}\\] where the Username and OrganizationName can be pretty much anything, but a valid email address looks like this. We might get even more creative and recognize that my list of possible endings could include country codes as well. For most people, I don’t recommend opening the regular expression can-of-worms, but it is good to know that these pattern matching utilities are available within R and you don’t need to export your pattern matching problems to Perl or Python. 11.5 Exercises The following file names were used in a camera trap study. The S number represents the site, P is the plot within a site, C is the camera number within the plot, the first string of numbers is the YearMonthDay and the second string of numbers is the HourMinuteSecond. file.names &lt;- c( &#39;S123.P2.C10_20120621_213422.jpg&#39;, &#39;S10.P1.C1_20120622_050148.jpg&#39;, &#39;S187.P2.C2_20120702_023501.jpg&#39;) Use a combination of str_sub() and str_split() to produce a data frame with columns corresponding to the site, plot, camera, year, month, day, hour, minute, and second for these three file names. So we want to produce code that will create the data frame: Site Plot Camera Year Month Day Hour Minute Second S123 P2 C10 2012 06 21 21 34 22 S10 P1 C1 2012 06 22 05 01 48 S187 P2 C2 2012 07 02 02 35 01 Hint: Convert all the dashes to periods and then split on the dots. After that you’ll have to further tear apart the date and time columns using str_sub(). "],
["12-dates-and-times.html", "Chapter 12 Dates and Times 12.1 Creating Date and Time objects 12.2 Extracting information 12.3 Arithmetic on Dates 12.4 Exercises", " Chapter 12 Dates and Times library( lubridate ) Dates within a computer require some special organization because there are several competing conventions for how to write a date (some of them more confusing than others) and because the sort order should be in the order that the dates occur in time. One useful tidbit of knowledge is that computer systems store a time point as the number of seconds from set point in time, called the epoch. So long as you always use the same epoch, you doesn’t have to worry about when the epoch is, but if you are switching between software systems, you might run into problems if they use different epochs. In R, we use midnight on Jan 1, 1970. In Microsoft Excel, they use Jan 0, 1900. For many years, R users hated dealing with dates because it was difficult to remember how to get R to take a string that represents a date (e.g. “June 26, 1997”) because users were required to specify how the format was arranged using a relatively complex set of rules. For example %y represents the two digit year, %Y represents the four digit year, %m represents the month, but %b represents the month written as Jan or Mar. Into this mess came Hadley Wickham (of ggplot2 and dplyr fame) and his student Garrett Grolemund. The internal structure of R dates and times is quite robust, but the functions we use to manipulate them are horrible. To fix this, Dr Wickham and his then PhD student Dr Grolemund introduced the lubridate package. 12.1 Creating Date and Time objects To create a Date object, we need to take a string or number that represents a date and tell the computer how to figure out which bits are the year, which are the month, and which are the day. The lubridate package uses the following functions: Common Orders Uncommon Orders ymd() Year Month Day dym() Day Year Month mdy() Month Day Year myd() Month Year Day dmy() Day Month Year ydm() Year Day Month The uncommon orders aren’t likely to be used, but the lubridate package includes them for completeness. Once the order has been specified, the lubridate package will try as many different ways to parse the date that make sense. As a result, so long as the order is consistent, all of the following will work: mdy( &#39;June 26, 1997&#39;, &#39;Jun 26 97&#39;, &#39;6-26-97&#39;, &#39;6-26-1997&#39;, &#39;6/26/97&#39;, &#39;6-26/97&#39; ) ## [1] &quot;1997-06-26&quot; &quot;1997-06-26&quot; &quot;1997-06-26&quot; &quot;1997-06-26&quot; &quot;1997-06-26&quot; ## [6] &quot;1997-06-26&quot; mdy(&#39;June 26, 0097&#39;, &#39;June 26, 97&#39;, &#39;June 26, 68&#39;, &#39;June 26, 69&#39;) ## [1] &quot;0097-06-26&quot; &quot;1997-06-26&quot; &quot;2068-06-26&quot; &quot;1969-06-26&quot; This shows by default if you only specify the year using two digits, lubridate() will try to do something clever. It will default to either a 19XX or 20XX and it picks whichever is the closer date. This illustrates that you should ALWAYS fully specify the year using four digits. The lubridate functions will also accommodate if an integer representation of the date, but it has to have enough digits to uniquely identify the month and day. ymd(20090110) ## [1] &quot;2009-01-10&quot; ymd(2009722) # only one digit for month --- error! ## Warning: All formats failed to parse. No formats found. ## [1] NA ymd(2009116) # this is ambiguous! 1-16 or 11-6? ## Warning: All formats failed to parse. No formats found. ## [1] NA If we want to add a time to a date, we will use a function with the suffix _hm or _hms. Suppose that we want to encode a date and time, for example, the date and time of my wedding ceremony mdy_hm(&#39;Sept 18, 2010 5:30 PM&#39;, &#39;9-18-2010 17:30&#39;) ## [1] NA &quot;2010-09-18 17:30:00 UTC&quot; In the above case, lubridate is having trouble understanding AM/PM differences and it is better to always specify times using 24 hour notation and skip the AM/PM designations. By default, R codes the time of day using as if the event occurred in the UMT time zone (also known as Greenwich Mean Time GMT). To specify a different time zone, use the tz= option. For example: mdy_hm(&#39;9-18-2010 17:30&#39;, tz=&#39;MST&#39;) # Mountain Standard Time ## [1] &quot;2010-09-18 17:30:00 MST&quot; This isn’t bad, but Loveland, Colorado is on MST in the winter and MDT in the summer because of daylight savings time. So to specify the time zone that could switch between standard time and daylight savings time, I should specify tz='US/Mountain' mdy_hm(&#39;9-18-2010 17:30&#39;, tz=&#39;US/Mountain&#39;) # US mountain time ## [1] &quot;2010-09-18 17:30:00 MDT&quot; As Arizonans, we recognize that Arizona is weird and doesn’t use daylight savings time. Fortunately R has a built-in time zone just for us. mdy_hm(&#39;9-18-2010 17:30&#39;, tz=&#39;US/Arizona&#39;) # US Arizona time ## [1] &quot;2010-09-18 17:30:00 MST&quot; R recognizes 582 different time zone locals and you can find these using the function OlsonNames(). To find out more about what these mean you can check out the Wikipedia page on timezones [http://en.wikipedia.org/wiki/List_of_tz_database_time_zones||http://en.wikipedia.org/wiki/List_of_tz_database_time_zones]. 12.2 Extracting information The lubridate package provides many functions for extracting information from the date. Suppose we have defined # Derek&#39;s wedding! x &lt;- mdy_hm(&#39;9-18-2010 17:30&#39;, tz=&#39;US/Mountain&#39;) # US Mountain time Command Ouput Description year(x) 2010 Year month(x) 9 Month day(x) 18 Day hour(x) 17 Hour of the day minute(x) 30 Minute of the hour second(x) 0 Seconds wday(x) 7 Day of the week (Sunday = 1) mday(x) 18 Day of the month yday(x) 261 Day of the year Here we get the output as digits, where September is represented as a 9 and the day of the week is a number between 1-7. To get nicer labels, we can use label=TRUE for some commands. Command Ouput wday(x, label=TRUE) Sat month(x, label=TRUE) Sep All of these functions can also be used to update the value. For example, we could move the day of the wedding from September \\(18^{th}\\) to October \\(18^{th}\\) by changing the month. month(x) &lt;- 10 x ## [1] &quot;2010-10-18 17:30:00 MDT&quot; Often I want to consider some point in time, but need to convert the timezone the date was specified into another timezone. The function with_tz() will take a given moment in time and figure out when that same moment is in another timezone. For example, Game of Thrones is made available on HBO’s streaming service at 9pm on Sunday evenings Eastern time. I need to know when I can start watching it here in Arizona. GoT &lt;- ymd_hm(&#39;2015-4-26 21:00&#39;, tz=&#39;US/Eastern&#39;) with_tz(GoT, tz=&#39;US/Arizona&#39;) ## [1] &quot;2015-04-26 18:00:00 MST&quot; This means that Game of Thrones is available for streaming at 6 pm Arizona time. 12.3 Arithmetic on Dates Once we have two or more Date objects defined, we can perform appropriate mathematical operations. For example, we might want to the know the number of days there are between two dates. Wedding &lt;- ymd(&#39;2010-Sep-18&#39;) Elise &lt;- ymd(&#39;2013-Jan-11&#39;) Childless &lt;- Elise - Wedding Childless ## Time difference of 846 days Because both dates were recorded without the hours or seconds, R defaults to just reporting the difference in number of days. Often I want to add two weeks, or 3 months, or one year to a date. However it is not completely obvious what I mean by “add 1 year”. Do we mean to increment the year number (eg Feb 2, 2011 -&gt; Feb 2, 2012) or do we mean to add 31,536,000 seconds? To get around this, lubridate includes functions of the form dunits() and units() where the “unit” portion could be year, month, week, etc. The “d” prefix will stand for duration when appropriate. x &lt;- ymd(&quot;2011-Feb-21&quot;) x + years(2) # Just add two to the year ## [1] &quot;2013-02-21&quot; x + dyears(2) # Add 2*365 days; 2012 was a leap year ## [1] &quot;2013-02-20&quot; 12.4 Exercises For the following formats for a date, transform them into a date/time object. Which formats can be handled nicely and which are not? birthday &lt;- c( &#39;September 13, 1978&#39;, &#39;Sept 13, 1978&#39;, &#39;Sep 13, 1978&#39;, &#39;9-13-78&#39;, &#39;9/13/78&#39;) Suppose you have arranged for a phone call to be at 3 pm on May 8, 2015 at Arizona time. However, the recipient will be in Auckland, NZ. What time will it be there? It turns out there is some interesting periodicity regarding the number of births on particular days of the year. Using the mosaicData package, load the data set Births78 which records the number of children born on each day in the United States in 1978. There is already a date column in the data set that is called, appropriately, date. Notice that ggplot2 knows how to represent dates in a pretty fashion and the following chart looks nice. library(ggplot2) data(&#39;Births78&#39;, package=&quot;mosaicData&quot;) ggplot(Births78, aes(x=date, y=births)) + geom_point() What stands out to you? Why do you think we have this trend? To test your assumption, we need to figure out the what day of the week each observation is. Use dplyr::mutate to add a new column named dow that is the day of the week (Monday, Tuesday, etc). This calculation will involve some function in the lubridate package. Plot the data with the point color being determined by the dow variable. "],
["13-speeding-up-r.html", "Chapter 13 Speeding up R 13.1 Faster for loops? 13.2 Vectorizing loops 13.3 Parallel Processing 13.4 Parallelizing for loops 13.5 Parallel Aware Functions", " Chapter 13 Speeding up R library(microbenchmark) # for measuring how long stuff takes library(doMC) # do multi-core stuff library(foreach) # parallelizable for loops library(tidyverse) # dplyr, ggplot2, etc... library(faraway) # some examples library(boot) library(caret) library(glmnet) Eventually if you have large enough data sets, an R user eventually writes code that is slow to execute and needs to be sped up. This chapter tries to lay out common problems and bad habits and shows how to correct them. However, the correctness and maintainability of code should take precendence over speed. Too often, misguided attempts to obtain efficient code results in an unmaintainable mess that is no faster that the initial code. Hadley Wickham has a book aimed at advanced R user that describes many of the finer details about R. One section in the book describes his process for building fast, maintainable software projects and if you have the time, I highly suggest reading the on-line version, Advanced R. First we need some way of measuring how long our code took to run. For this we will use the package microbenchmark. The idea is that we want to evaluate two or three expressions that solve a problem. x &lt;- runif(1000) microbenchmark( sqrt(x), # First expression to compare x^(0.5) # second expression to compare ) %&gt;% print(digits=3) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## sqrt(x) 2.31 2.42 2.6 2.46 2.53 10.2 100 a ## x^(0.5) 23.91 24.01 24.8 24.08 24.27 42.9 100 b What microbenchmark does is run the two expressions a number of times and then produces the 5-number summary of those times. By running it multiple times, we account for the randomness associated with a operating system that is also running at the same time. 13.1 Faster for loops? Often we need to perform some simple action repeatedly. It is natural to write a for loop to do the action and we wish to speed the up. In this first case, we will consider having to do the action millions of times and each chunk of computation within the for takes very little time. Consider frame of 4 columns, and for each of \\(n\\) rows, we wish to know which column has the largest value. make.data &lt;- function(n){ data &lt;- cbind( rnorm(n, mean=5, sd=2), rpois(n, lambda = 5), rgamma(n, shape = 2, scale = 3), rexp(n, rate = 1/5)) data &lt;- data.frame(data) return(data) } data &lt;- make.data(100) The way that you might first think about solving this problem is to write a for loop and, for each row, figure it out. f1 &lt;- function( input ){ output &lt;- NULL for( i in 1:nrow(input) ){ output[i] &lt;- which.max( input[i,] ) } return(output) } We might consider that there are two ways to return a value from a function (using the return function and just printing it). In fact, I’ve always heard that using the return statment is a touch slower. f2.noReturn &lt;- function( input ){ output &lt;- NULL for( i in 1:nrow(input) ){ output[i] &lt;- which.max( input[i,] ) } output } data &lt;- make.data(100) microbenchmark( f1(data), f2.noReturn(data) ) %&gt;% print(digits=3) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## f1(data) 3.35 3.74 5.70 4.31 6.83 21.6 100 a ## f2.noReturn(data) 3.36 3.71 5.21 4.20 5.78 16.0 100 a In fact, it looks like it is a touch slower, but not massively compared to the run-to-run variability. I prefer to use the return statement for readability, but if we agree have the last line of code in the function be whatever needs to be returned, readability isn’t strongly effected. We next consider whether it would be faster to allocate the output vector once we figure out the number of rows needed, or just build it on the fly? f3.AllocOutput &lt;- function( input ){ n &lt;- nrow(input) output &lt;- rep(NULL, n) for( i in 1:nrow(input) ){ output[i] &lt;- which.max( input[i,] ) } return(output) } microbenchmark( f1(data), f3.AllocOutput(data) ) %&gt;% print(digits=3) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## f1(data) 3.28 3.55 4.34 3.77 4.42 12.8 100 a ## f3.AllocOutput(data) 3.31 3.54 4.82 4.00 5.26 11.8 100 a If anything, allocating the size of output first was slower. So given this, we shouldn’t feel to bad being lazy and using output &lt;- NULL to initiallize things. 13.2 Vectorizing loops In general, for loops in R are very slow and we want to avoid them as much as possible. The apply family of functions can be quite helpful for applying a function to each row or column of a matrix or data.frame or to each element of a list. To test this, instead of a for loop, we will use apply. f4.apply &lt;- function( input ){ output &lt;- apply(input, 1, which.max) return(output) } microbenchmark( f1(data), f4.apply(data) ) %&gt;% print(digits=3) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## f1(data) 3203 3409 3904 3570 3989 8737 100 b ## f4.apply(data) 272 292 376 330 377 2567 100 a This is the type of speed up that matters. We have a 10-fold speed up in execution time and particularly the maximum time has dropped impressively. Unfortunately, I have always found the apply functions a little cumbersome and I prefer to use dplyr instead strictly for readability. f5.dplyr &lt;- function( input ){ output &lt;- input %&gt;% mutate( max.col=which.max( c(X1, X2, X3, X4) ) ) return(output$max.col) } microbenchmark( f4.apply(data), f5.dplyr(data) ) %&gt;% print(digits=3) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## f4.apply(data) 287 326 384 350 408 716 100 a ## f5.dplyr(data) 513 597 812 644 719 7420 100 b Unfortunately dplyr is a lot slower than apply in this case. I wonder if the dynamics would change with a larger n? data &lt;- make.data(10000) microbenchmark( f4.apply(data), f5.dplyr(data) ) %&gt;% print(digits=3) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## f4.apply(data) 22191 25703 30194 27244 32331 62376 100 b ## f5.dplyr(data) 693 910 1130 1056 1219 2880 100 a data &lt;- make.data(100000) microbenchmark( f4.apply(data), f5.dplyr(data) ) %&gt;% print(digits=3) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## f4.apply(data) 268.47 344.07 479.27 434.61 568.96 1253.9 100 b ## f5.dplyr(data) 2.38 2.86 3.85 3.14 3.67 29.9 100 a What just happened? The package dplyr is designed to work well for large data sets, and utilizes a modified structure, called a tibble, which provides massive benefits for large tables, but at the small scale, the overhead of converting the data.frame to a tibble overwhelms any speed up. But because the small sample case is already fast enough to not be noticable, we don’t really care about the small n case. 13.3 Parallel Processing Most modern computers have multiple computing cores, and can run muliple processes at the same time. Sometimes this means that you can run multiple programs and switch back and forth easily without lag, but we are now interested in using as many cores as possible to get our statistical calculations completed by using muliple processing cores at the same time. This is referred to as running the process “in parallel” and there are many tasks in modern statistical computing that are “embarrasingly easily parallelized”. In particular bootstrapping and cross validation techniques are extremely easy to implement in a parallel fashion. However, running commands in parallel incurs some overhead cost in set up computation, as well as all the message passing from core to core. For example, to have 5 cores all perform an analysis on a set of data, all 5 cores must have access to the data, and not overwrite any of it. So parallelizing code only makes sense if the individual steps that we pass to each core is of sufficient size that the overhead incurred is substantially less than the time to run the job. We should think of executing code in parallel as having three major steps: 1. Tell R that there are multiple computing cores available and to set up a useable cluster to which we can pass jobs to. 2. Decide what ‘computational chunk’ should be sent to each core and distribute all necessary data, libraries, etc to each core. 3. Combine the results of each core back into a unified object. 13.4 Parallelizing for loops There are a number of packages that allow you to tell R how many cores you have access to. One of the easiest ways to parallelize a for loop is using a package called foreach. The registration of multiple cores is actually pretty easy. doMC::registerDoMC(cores = 2) # my laptop only has two cores. We will consider an example that is common in modern statistics. We will examine parallel computing utilizing a bootstrap example where we create bootstrap samples for calculating confidence intervals for regression coefficients. ggplot(trees, aes(x=Girth, y=Volume)) + geom_point() + geom_smooth(method=&#39;lm&#39;) model &lt;- lm( Volume ~ Girth, data=trees) This is how we would do this previously. # f is a formula # df is the input data frame # M is the number of bootstrap iterations boot.for &lt;- function( f, df, M=999){ output &lt;- list() for( i in 1:100 ){ # Do stuff model.star &lt;- lm( f, data=df %&gt;% sample_frac(1, replace=TRUE) ) output[[i]] &lt;- model.star$coefficients } # use rbind to put the list of results together into a data.frame output &lt;- sapply(output, rbind) %&gt;% t() %&gt;% data.frame() return(output) } We will first ask about how to do the same thing using the function foreach # f is a formula # df is the input data frame # M is the number of bootstrap iterations boot.foreach &lt;- function(f, df, M=999){ output &lt;- foreach( i=1:100 ) %dopar% { # Do stuff model.star &lt;- lm( f, data=df %&gt;% sample_frac(1, replace=TRUE) ) model.star$coefficients } # use rbind to put the list of results together into a data.frame output &lt;- sapply(output, rbind) %&gt;% t() %&gt;% data.frame() return(output) } Not much has changed in our code. Lets see which is faster. microbenchmark( boot.for( Volume~Girth, trees ), boot.foreach( Volume~Girth, trees ) ) %&gt;% print(digits=3) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## boot.for(Volume ~ Girth, trees) 113 138 164 153 166 369 100 a ## boot.foreach(Volume ~ Girth, trees) 161 185 233 198 235 718 100 b In this case, the overhead associated with splitting the job across two cores, copying the data over, and then combining the results back together was more than we saved by using both cores. If the nugget of computation within each pass of the for loop was larger, then it would pay to use both cores. # massiveTrees has 31000 observations massiveTrees &lt;- NULL for( i in 1:1000 ){ massiveTrees &lt;- rbind(massiveTrees, trees) } microbenchmark( boot.for( Volume~Girth, massiveTrees ) , boot.foreach( Volume~Girth, massiveTrees ) ) %&gt;% print(digits=3) ## Unit: milliseconds ## expr min lq mean median uq ## boot.for(Volume ~ Girth, massiveTrees) 1111 1393 1518 1486 1629 ## boot.foreach(Volume ~ Girth, massiveTrees) 677 729 910 772 1060 ## max neval cld ## 2086 100 b ## 1841 100 a Because we often generate a bunch of results that we want to see as a data.frame, the foreach function includes and option to do it for us. output &lt;- foreach( i=1:100, .combine=data.frame ) %dopar% { # Do stuff model.star &lt;- lm( Volume ~ Girth, data= trees %&gt;% sample_frac(1, replace=TRUE) ) model$coefficients } It is important to recognize that the data.frame trees was utilized inside the foreach loop. So when we called the foreach loop and distributed the workload across the cores, it was smart enough to distribute the data to each core. However, if there were functions that we utilized inside the foor loop that came from a packege, we need to tell each core to load the function. output &lt;- foreach( i=1:1000, .combine=data.frame, .packages=&#39;dplyr&#39; ) %dopar% { # Do stuff model.star &lt;- lm( Volume ~ Girth, data= trees %&gt;% sample_frac(1, replace=TRUE) ) model.star$coefficients } 13.5 Parallel Aware Functions There are many packages that address problems that are “embarassingly easily parallelized” and they will happily work with multiple cores. Methods that rely on resampling certainly fit into this category. 13.5.1 boot::boot Bootstrapping relys on resampling the dataset and calculating test statistics from each resample. In R, the most common way to do this is using the package boot and we just need to tell the boot function, to use the multiple cores available. (Note, we have to have registered the cores first!) model &lt;- lm( Volume ~ Girth, data=trees) my.fun &lt;- function(df, index){ model.star &lt;- lm( Volume ~ Girth, data= trees[index,] ) model.star$coefficients } microbenchmark( serial = boot::boot( trees, my.fun, R=1000 ), parallel = boot::boot( trees, my.fun, R=1000, parallel=&#39;multicore&#39;, ncpus=2 ) ) %&gt;% print(digits=3) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## serial 616 655 730 722 774 1029 100 b ## parallel 562 594 673 624 689 1174 100 a In this case, we had a bit of a spead up, but not a factor of 2. This is due to the overhead of splitting the job across both cores. 13.5.2 caret::train The statistical learning package caret also handles all the work to do cross validation in a parallel computing environment. The functions in caret have an option allowParallel which by default is TRUE, which controls if we should use all the cores. Assuming we have already registered the number of cores, then by default caret will use them all. library(faraway) library(caret) ctrl.serial &lt;- trainControl( method=&#39;repeatedcv&#39;, number=5, repeats=4, preProcOptions = c(&#39;center&#39;,&#39;scale&#39;), allowParallel = FALSE) ctrl.parallel &lt;- trainControl( method=&#39;repeatedcv&#39;, number=5, repeats=4, preProcOptions = c(&#39;center&#39;,&#39;scale&#39;), allowParallel = TRUE) grid &lt;- data.frame( alpha = 1, # 1 =&gt; Lasso Regression lambda = exp(seq(-6, 1, length=50))) microbenchmark( model &lt;- train( lpsa ~ ., data=prostate, method=&#39;glmnet&#39;, trControl=ctrl.serial, tuneGrid=grid, lambda = grid$lambda ), model &lt;- train( lpsa ~ ., data=prostate, method=&#39;glmnet&#39;, trControl=ctrl.parallel, tuneGrid=grid, lambda = grid$lambda ) ) %&gt;% print(digits=3) ## Unit: seconds ## expr ## model &lt;- train(lpsa ~ ., data = prostate, method = &quot;glmnet&quot;, trControl = ctrl.serial, tuneGrid = grid, lambda = grid$lambda) ## model &lt;- train(lpsa ~ ., data = prostate, method = &quot;glmnet&quot;, trControl = ctrl.parallel, tuneGrid = grid, lambda = grid$lambda) ## min lq mean median uq max neval cld ## 1.15 1.17 1.20 1.18 1.20 1.47 100 a ## 1.19 1.21 1.25 1.22 1.25 1.68 100 b Again, we saw only moderate gains by using both cores, however it didn’t really cost us anything. Because the caret package by default allows parallel processing, it doesn’t hurt to just load the doMC package and register the number of cores. Even in just the two core case, it is a good habit to get into so that when you port your code to a huge computer with many cores, the only thing to change is how many cores you have access to. "],
["14-rmarkdown-tricks.html", "Chapter 14 Rmarkdown Tricks 14.1 Mathematical expressions 14.2 Tables 14.3 R functions to produce table code.", " Chapter 14 Rmarkdown Tricks We have been using RMarkdown files to combine the analysis and discussion into one nice document that contains all the analysis steps so that your research is reproducible. There are many resources on the web about Markdown and the variant that RStudio uses (called RMarkdown), but the easiest reference is to just use the RStudio help tab to access the help. I particular like Help -&gt; Cheatsheets -&gt; RMarkdown Reference Guide because it gives me the standard Markdown information but also a bunch of information about the options I can use to customize the behavior of individual R code chunks. Two topics that aren’t covered in the RStudio help files are how to insert mathematical text symbols and how to produce decent looking tables without too much fuss. Most of what is presented here isn’t primarily about how to use R, but rather how to work with tools in RMarkdown so that the final product is neat and tidy. While you could print out your RMarkdown file and then clean it up in MS Word, sometimes there is a good to want as nice a starting point as possible. 14.1 Mathematical expressions The primary way to insert a mathematical expression is to use a markup language called LaTeX. This is a very powerful system and it is what most Mathematicians use to write their documents. The downside is that there is a lot to learn. However, you can get most of what you need pretty easily. For RMarkdown to recognize you are writing math using LaTeX, you need to enclose the LaTeX with dollar signs ($). Some examples of common LaTeX patterns are given below: Goal LaTeX Output LaTeX Output power $x^2$ \\(x^2\\) $y^{0.95}$ \\(y^{0.95}\\) Subscript $x_i$ \\(x_i\\) $t_{24}$ \\(t_{24}\\) Greek $\\alpha$ $\\beta$ \\(\\alpha\\) \\(\\beta\\) $\\theta$ $\\Theta$ \\(\\theta\\) \\(\\Theta\\) Bar $\\bar{x}$ \\(\\bar{x}\\) $\\bar{mu}_i$ \\(\\bar{\\mu}_i\\) Hat $\\hat{mu}$ \\(\\hat{\\mu}\\) $\\hat{y}_i$ \\(\\hat{y}_i\\) Star $y^*$ \\(y^*\\) $\\hat{\\mu}^*_i$ \\(\\hat{\\mu}^*_i\\) Centered Dot $\\cdot$ \\(\\cdot\\) $\\bar{y}_{i\\cdot}$ \\(\\bar{y}_{i\\cdot}\\) Sum $\\sum x_i$ \\(\\sum x_i\\) $\\sum_{i=0}^N x_i$ \\(\\sum_{i=0}^N x_i\\) Square Root $\\sqrt{a}$ \\(\\sqrt{a}\\) $\\sqrt{a^2 + b^2}$ \\(\\sqrt{a^2 + b^2}\\) Fractions $\\frac{a}{b}$ \\(\\frac{a}{b}\\) $\\frac{x_i - \\bar{x}{s/\\sqrt{n}$ \\(\\frac{x_i - \\bar{x}}{s/\\sqrt{n}}\\) Within your RMarkdown document, you can include LaTeX code by enclosing it with dollar signs. So you might write $\\alpha=0.05$ in your text, but after it is knitted to a pdf, html, or Word, you’ll see \\(\\alpha=0.05\\). If you want your mathematical equation to be on its own line, all by itself, enclose it with double dollar signs. So $$z_i = \\frac{z_i-\\bar{x}}{\\sigma / \\sqrt{n}}$$ would be displayed as \\[ z_{i}=\\frac{x_{i}-\\bar{X}}{\\sigma/\\sqrt{n}} \\] Unfortunately RMarkdown is a little picky about spaces near the $ and $$ signs and you can’t have any spaces between them and the LaTeX command. For a more information about all the different symbols you can use, google ‘LaTeX math symbols’. 14.2 Tables For the following descriptions of the simple, grid, and pipe tables, I’ve shamelessly stolen from the Pandoc documentation. [http://pandoc.org/README.html#tables] One way to print a table is to just print in in R and have the table presented in the code chunk. For example, suppose I want to print out the first 4 rows of the trees dataset. data &lt;- trees[1:4, ] data ## Girth Height Volume ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 Usually this is sufficient, but suppose you want something a bit nicer because you are generating tables regularly and you don’t want to have to clean them up by hand. Tables in RMarkdown follow the table conventions from the Markdown class with a few minor exceptions. Markdown provides 4 ways to define a table and RMarkdown supports 3 of those. 14.2.1 Simple Tables Simple tables look like this (Notice I don’t wrap these dollar signs or anything, just a blank line above and below the table): Right Left Center Default ------- ------ ---------- ------- 12 12 hmmm 12 123 123 123 123 1 1 1 1 and would be rendered like this: Right Left Center Default 12 12 hmmm 12 123 123 123 123 1 1 1 1 The headers and table rows must each fit on one line. Column alignments are determined by the position of the header text relative to the dashed line below it. If the dashed line is flush with the header text on the right side but extends beyond it on the left, the column is right-aligned. If the dashed line is flush with the header text on the left side but extends beyond it on the right, the column is left-aligned. If the dashed line extends beyond the header text on both sides, the column is centered. If the dashed line is flush with the header text on both sides, the default alignment is used (in most cases, this will be left). The table must end with a blank line, or a line of dashes followed by a blank line. 14.2.2 Grid Tables Grid tables are a little more flexible and each cell can take an arbitrary Markdown block elements (such as lists). +---------------+---------------+--------------------+ | Fruit | Price | Advantages | +===============+===============+====================+ | Bananas | $1.34 | - built-in wrapper | | | | - bright color | +---------------+---------------+--------------------+ | Oranges | $2.10 | - cures scurvy | | | | - tasty | +---------------+---------------+--------------------+ which is rendered as the following: Fruit Price Advantages Bananas $1.34 built-in wrapper bright color Oranges $2.10 cures scurvy tasty Grid table doesn’t support Left/Center/Right alignment. Both Simple tables and Grid tables require you to format the blocks nicely inside the RMarkdown file and that can be a bit annoying if something changes and you have to fix the spacing in the rest of the table. Both Simple and Grid tables don’t require column headers. 14.2.3 Pipe Tables Pipe tables look quite similar to grid tables but Markdown isn’t as picky about the pipes lining up. However, it does require a header row (which you could leave the elements blank in). | Right | Left | Default | Center | |------:|:-----|---------|:------:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1 | 1 | 1 | 1 | which will render as the following: Right Left Default Center 12 12 12 12 123 123 123 123 1 1 1 1 In general I prefer to use the pipe tables because it seems a little less picky about getting everything correct. However it is still pretty annoying to get the table laid out correctly. In all of these tables, you can use the regular RMarkdown formatting tricks for italicizing and bolding. So I could have a table such as the following: | Source | df | Sum of Sq | Mean Sq | F | $Pr(&gt;F_{1,29})$ | |:------------|-----:|--------------:|--------------:|-------:|--------------------:| | Girth | *1* | 7581.8 | 7581.8 | 419.26 | **&lt; 2.2e-16** | | Residual | 29 | 524.3 | 18.1 | | | and have it look like this: Source df Sum of Sq Mean Sq F \\(Pr(&gt;F_{1,29})\\) Girth 1 7581.8 7581.8 419.26 &lt; 2.2e-16 Residual 29 524.3 18.1 The problem with all of this is that I don’t want to create these by hand. Instead I would like functions that take a data frame or matrix and spit out the RMarkdown code for the table. 14.3 R functions to produce table code. There are a couple of different packages that convert a data frame to simple/grid/pipe table. We will explore a couple of these, starting with the most basic and moving to the more complicated. The general idea is that we’ll produce the appropriate simple/grid/pipe table syntax in R, and when it gets knitted, then RMarkdown will turn our simple/grid/pipe table into something pretty. 14.3.1 knitr::kable The knitr package includes a function that produces simple tables. It doesn’t have much customizability, but it gets the job done. knitr::kable( data ) Girth Height Volume 8.3 70 10.3 8.6 65 10.3 8.8 63 10.2 10.5 72 16.4 14.3.2 Package pander The package pander seems to be a nice compromise between customization and not having to learn too much. It is relatively powerful in that it will take summary() and anova() output and produce tables for them. By default pander will produce simple tables, but you can ask for Grid or Pipe tables. library(pander) pander( data, style=&#39;rmarkdown&#39; ) # style is pipe tables... Girth Height Volume 8.3 70 10.3 8.6 65 10.3 8.8 63 10.2 10.5 72 16.4 The pander package deals with summary and anova tables from a variety of different analyses. So you can simply ask for a nice looking version using the following: model &lt;- lm( Volume ~ Girth, data=trees ) # a simple regression pander( summary(model) ) # my usual summary table pander( anova( model ) ) # my usual anova table Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -36.94 3.365 -10.98 7.621e-12 Girth 5.066 0.2474 20.48 8.644e-19 Fitting linear model: Volume ~ Girth Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 31 4.252 0.9353 0.9331 Analysis of Variance Table Df Sum Sq Mean Sq F value Pr(&gt;F) Girth 1 7582 7582 419.4 8.644e-19 Residuals 29 524.3 18.08 NA NA "]
]
