[
["index.html", "STA 444/5 - Introductory Data Science using R Preface Other Resources Acknowledgements", " STA 444/5 - Introductory Data Science using R Derek L. Sonderegger August 20, 2019 Preface This book is intended to provide students with a resource for learning R while using it during an introductory statistics course. The Introduction section covers common issues that students in a typical statistics course will encounter and provides a simple examples and does not attempt to be exhaustive. The Deeper Details section addresses issues that commonly arise in many data wrangling situations and is intended to give students a deep enough understanding of R that they will be able to use it as their primary computing resource to manipulate, graph and model data. Other Resources There are a great number of very good online and physical resources for learning R. Hadley Wickham and Garrett Grolemund’s free online book R for Data Science. This is a wonderful introduction to the tidyverse and is free. If there is any book I’d recommend buying, this would be it. Michael Freeman’s book Programming Skills for Data Science. This book covers much of what we’ll do in this class and is quite readable. Hadley Wickham and Jenny Bryan have a whole book on R packages to effectively manage large projects. Acknowledgements These online books are used a huge amount of work and I appreciative the support of my wife Aubrey and the love our our two children Elise and Casey. "],
["1-familiarization.html", "Chapter 1 Familiarization 1.1 R file Types 1.2 R as a simple calculator 1.3 Assignment 1.4 Vectors 1.5 Packages 1.6 Finding Help 1.7 Exercises", " Chapter 1 Familiarization R is a open-source program that is commonly used in statistics and machine learning. It runs on almost every platform and is completely free and is available at www.r-project.org. Most cutting-edge statistical research is first available on R. The basic editor that comes with R works fairly well, but you should consider running R through the program RStudio which is located at rstudio.com. This is a completely free Integrated Developement Environment that works on Macs, Windows and a couple of flavors of Linux. It simplifies a bunch of more annoying aspects of the standard R GUI and supports things like tab completion. R is a script based language, and there isn’t a point-and-click interface for data wrangling and statistical modeling. While the initial learning curve will be steeper, understanding how to write scripts will be valuable because scripts leave a clear description of what steps were performed. This is a critical aspect of what is known as reproducable research and a good practice. While it may seem tempting to type commands into the console directly, but because the goal is to create a script that contains all of the necessary commands to perform an analysis, users should get into the habit of always writing their commands into their R script (or Rmarkdown file) and executing the command from there. 1.1 R file Types One of the worst things about a pocket calculator is there is no good way to go several steps and easily see what you did or fix a mistake (there is nothing more annoying than re-typing something because of a typo. To avoid these issues I always work with RMarkdown (or script) files instead of typing directly into the console. You will quickly learn that it is impossible to write R code correctly the first time and you’ll save yourself a huge amount of work by just embracing this from the beginning. Furthermore, having an R file fully documents how you did your analysis, which can help when writing the methods section of a paper. Finally, having a file makes it easy to re-run an analysis after a change in the data (additional data values, transformed data, or removal of outliers). There are three common ways to store R commands in some file: scripts, notebooks, and Rmarkdown files. The distinction between the R scripts and the other two is substantial as R scripts just store R commands, but don’t make any attampt to save the results in any distinct format. Both notebooks and Rmarkdown files save the results of an analysis and present the results in a nice readable fashion. I encourage people to use Rmarkdown files over notebooks because the Rmarkdown knitting enforces a reproducable workflow whereas notebooks can be run out of order. Rmarkdown files are written in a way to combine the R commands, commentary, and the command outputs all together into one coherent document. For most people that use R to advance their research, using Rmarkdown is the most useful. 1.1.1 R Scripts (.R files) The first type of file that we’ll discuss is a traditional script file. To create a new .R script in RStudio go to File -&gt; New File -&gt; R Script. This opens a new window in RStudio where you can type commands and functions as a common text editor. Type whatever you like in the script window and then you can execute the code line by line (using the run button or its keyboard shortcut to run the highlighted region or whatever line the curser is on) or the entire script (using the source button). Other options for what piece of code to run are available under the Code dropdown box. It often makes your R files more readable if you break a single command up into multiple lines. R scripts will disregard all whitespace (including line breaks) so you can safely spread your command over as multiple lines. Finally, it is useful to leave comments in the script for things such as explaining a tricky step, who wrote the code and when, or why you chose a particular name for a variable. The # sign will denote that the rest of the line is a comment and R will ignore it. An R script for a homework assignment might look something like this: # Problem 1 # Calculate the log of a couple of values and make a plot # of the log function from 0 to 3 log(0) log(1) log(2) x &lt;- seq(.1,3, length=1000) plot(x, log(x)) # Problem 2 # Calculate the exponential function of a couple of values # and make a plot of the function from -2 to 2 exp(-2) exp(0) exp(2) x &lt;- seq(-2, 2, length=1000) plot(x, exp(x)) This looks perfectly acceptable as a way of documenting what you did, but this script file doesn’t contain the actual results of commands I ran, nor does it show you the plots. Also anytime I want to comment on some output, it needs to be offset with the commenting character #. It would be nice to have both the commands and the results merged into one document. This is what the R Markdown file does for us. 1.1.2 R Markdown (.Rmd files) When I was a graduate student, I had to tediously copy and past tables of output from the R console and figures I had made into my Microsoft Word document. Far too often I would realize I had made a small mistake in part (b) of a problem and would have to go back, correct my mistake, and then redo all the laborious copying. I often wished that I could write both the code for my statistical analysis and the long discussion about the interpretation all in the same document so that I could just re-run the analysis with a click of a button and all the tables and figures would be updated by magic. Fortunately that magic now exists. To create a new R Markdown document, we use the File -&gt; New File -&gt; R Markdown... dropdown option and a menu will appear asking you for the document title, author, and preferred output type. In order to create a PDF, you’ll need to have LaTeX installed, but the HTML output nearly always works and I’ve had good luck with the MS Word output as well. The R Markdown is an implementation of the Markdown syntax that makes it extremely easy to write webpages and give instructions for how to do typesetting sorts of things. This syntax was extended to allow use to embed R commands directly into the document. Perhaps the easiest way to understand the syntax is to look at an at the RMarkdown website. The R code in my document is nicely separated from my regular text using the three backticks and an instruction that it is R code that needs to be evaluated. The output of this document looks good as a HTML, PDF, or MS Word document. I have actually created this entire book using RMarkdown. To see what the the Rmarkdown file looks like for any chapter, just click on the pencil icon at the top of the online notes. While writing an Rmarkdown file, each of the code chunks can be executed in a couple of different ways. 1. Press the green arrow at the top of the code chunk to run the entire chunk. 2. The run button has several options has several options. 3. There are keyboard shortcuts, on the Mac it is Cmd-Return. To insert a new code chunk, a user can type it in directly, use the green Insert button, or the keyboard shortcut. To produce a final output document that you’ll present to your boss/collegues/client where you want to combine the code, output, and commentary you’ll “knit” the document which causes all of the R code to be run in a new R session, and then weave together the output into your document. This can be done using the knit button at the top of the Editor Window. 1.1.3 R Notebooks (.Rmd files) Notebooks are just very specialized types of Rmarkdown file. Here, the result of each code chunk that is run manually is saved, but when previewing the output, all of the R code is NOT re-run. Therefore it is possible to run the code, then modify the code, and then produce a document where the written code and output do not match up. As a result of this “feature” I strongly discourage the use of notebooks in favor of the standard Rmarkdown files. 1.2 R as a simple calculator Assuming that you have started R on whatever platform you like, you can use R as a simple calculator. In either your Rmarkdown file code chunk (or just run this in the console), run the following # Some simple addition 2+3 ## [1] 5 In this fashion you can use R as a very capable calculator. 6*8 ## [1] 48 4^3 ## [1] 64 exp(1) # exp() is the exponential function ## [1] 2.718282 R has most constants and common mathematical functions you could ever want. sin(), cos(), and other trigonometry functions are available, as are the exponential and log functions exp(), log(). The absolute value is given by abs(), and round() will round a value to the nearest integer. pi # the constant 3.14159265... ## [1] 3.141593 sin(0) ## [1] 0 log(5) # unless you specify the base, R will assume base e ## [1] 1.609438 log(5, base=10) # base 10 ## [1] 0.69897 Whenever I call a function, there will be some arguments that are mandatory, and some that are optional and the arguments are separated by a comma. In the above statements the function log() requires at least one argument, and that is the number(s) to take the log of. However, the base argument is optional. If you do not specify what base to use, R will use a default value. You can see that R will default to using base \\(e\\) by looking at the help page (by typing help(log) or ?log at the command prompt). Arguments can be specified via the order in which they are passed or by naming the arguments. So for the log() function which has arguments log(x, base=exp(1)). If I specify which arguments are which using the named values, then order doesn’t matter. # Demonstrating order does not matter if you specify # which argument is which log(x=5, base=10) ## [1] 0.69897 log(base=10, x=5) ## [1] 0.69897 But if we don’t specify which argument is which, R will decide that x is the first argument, and base is the second. # If not specified, R will assume the second value is the base... log(5, 10) ## [1] 0.69897 log(10, 5) ## [1] 1.430677 When I specify the arguments, I have been using the name=value notation and a student might be tempted to use the &lt;- notation here. Don’t do that as the name=value notation is making an association mapping and not a permanent assignment. 1.3 Assignment We need to be able to assign a value to a variable to be able to use it later. R does this by using an arrow &lt;- or an equal sign =. While R supports either, for readability, I suggest people pick one assignment operator and stick with it. I personally prefer to use the arrow. Variable names cannot start with a number, may not include spaces, and are case sensitive. tau &lt;- 2*pi # create two variables my.test.var = 5 # notice they show up in &#39;Environment&#39; tab in RStudio! tau ## [1] 6.283185 my.test.var ## [1] 5 tau * my.test.var ## [1] 31.41593 As your analysis gets more complicated, you’ll want to save the results to a variable so that you can access the results later. If you don’t assign the result to a variable, you have no way of accessing the result. 1.4 Vectors While single values are useful, it is very important that we are able to make groups of values. The most fundamental aggregation of values is called a vector. In R, we will require vectors to always be of the same type (e.g. all integers or all character strings). To create a vector, we just need to use the collection function c(). x &lt;- c(&#39;A&#39;,&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) x ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; y &lt;- c( 4, 3, 8, 10 ) y ## [1] 4 3 8 10 It is very common to have to make sequences of integers, and R has a shortcut to do this. The notation A:B will produce a vector starting with A and incrementing by one until we get to B. 2:6 ## [1] 2 3 4 5 6 1.5 Packages One of the greatest strengths about R is that so many people have devloped add-on packages to do some additional function. For example, plant community ecologists have a large number of multivariate methods that are useful but were not part of R. So Jari Oksanen got together with some other folks and put together a package of functions that they found useful. The result is the package vegan. To download and install the package from the Comprehensive R Archive Network (CRAN), you just need to ask RStudio it to install it via the menu Tools -&gt; Install Packages.... Once there, you just need to give the name of the package and RStudio will download and install the package on your computer. Many major analysis types are available via downloaded packages as well as problem sets from various books (e.g. Sleuth3 or faraway) and can be easily downloaded and installed from CRAN via the menu. Once a package is downloaded and installed on your computer, it is available, but it is not loaded into your current R session by default. The reason it isn’t loaded is that there are thousands of packages, some of which are quite large and only used occasionally. So to improve overall performance only a few packages are loaded by default and the you must explicitly load packages whenever you want to use them. You only need to load them once per session/script. library(vegan) # load the vegan library For a similar performance reason, many packages do not automatically load their datasets unless explicitly asked. Therefore when loading datasets from a package, you might need to do a two-step process of loading the package and then loading the dataset. library(faraway) # load the package into memory ## ## Attaching package: &#39;faraway&#39; ## The following object is masked from &#39;package:lattice&#39;: ## ## melanoma data(&quot;butterfat&quot;) # load the dataset into memory If you don’t need to load any functions from a package and you just want the datasets, you can do it in one step. data(&#39;butterfat&#39;, package=&#39;faraway&#39;) # just load the dataset, not anything else butterfat[1:6, ] # print out the first 6 rows of the data ## Butterfat Breed Age ## 1 3.74 Ayrshire Mature ## 2 4.01 Ayrshire 2year ## 3 3.77 Ayrshire Mature ## 4 3.78 Ayrshire 2year ## 5 4.10 Ayrshire Mature ## 6 4.06 Ayrshire 2year Similarly, if I am not using many functions from a package, I might choose call the functions using the notation package::function(). This is particularly important when two packages both have functions with the same name and it gets confusing which function you want to use. For example the packages mosaic and dplyr both have a function tally. So if I’ve already loaded the dplyr package but want to use the mosaic::tally() function I would use the following: mosaic::tally( c(0,0,0,1,1,1,1,2) ) ## X ## 0 1 2 ## 3 4 1 Finally, many researchers and programmers host their packages on GitHub (or equivalent site) and those packages can easily downloaded using tools from the devtools pacakge, which can be downloaded from CRAN. devtools::install_github(&#39;dereksonderegger/SiZer&#39;) ## Skipping install of &#39;SiZer&#39; from a github remote, the SHA1 (8745f2e4) has not changed since last install. ## Use `force = TRUE` to force installation 1.6 Finding Help There are many complicated details about R and nobody knows everything about how each individual package works. As a result, a robust collection of resources has been developed and you are undoubtably not the first person to wonder how to do something. 1.6.1 How does this function work? If you know the function you need, but just don’t know how to use it, the built-in documentation is really quite good. Suppose I am interested in how the rep function works. We could access the rep help page by searching in the help window or from the console via help(rep). The document that is displayed shows what arguments the function expects and what it will return. At the bottom of the help page is often a set of examples demonstrating different ways to use the function. As you get more proficient in R, these help files become quite handy, but initially they feel quite overwhelming. 1.6.2 How does this package work? If a package author really wants their package to be used by a wide audience, they will provide a “vignette”. These are a set of notes that explain enough of how a package works to get a user able to utilize the package effectively. This documentation is targetted towards people the know some R, but deep technical knowledge is not expected. Whenever I encounter a new package that might be applicable to me, the first thing I do is see if it has a vignette, and if so, I start reading it. If a package doesn’t have a vignette, I’ll google “R package XXXX” and that will lead to documentation on CRAN that gives a list of functions in the package. 1.6.3 How do I do XXX? Often I find myself asking how to do something but I don’t know the function or package to use. In those cases, I will use the coding question and answer site stackoverflow. This is particularly effective and I encourage students to spend some time to understand the solutions presented instead of just copying working code. By digging into why a particular code chunk works, you’ll learn all sorts of neat tricks and you’ll find yourself utilizing the site less frequently. 1.7 Exercises Create an RMarkdown file that solves the following exercises. Calculate \\(\\log\\left(6.2\\right)\\) first using base \\(e\\) and second using base \\(10\\). To figure out how to do different bases, it might be helpful to look at the help page for the log function. Calculate the square root of 2 and save the result as the variable named sqrt2. Have R display the decimal value of sqrt2. Hint: use Google to find the square root function. Perhaps search on the keywords “R square root function”. This exercise walks you through installing a package with all the datasets used in the textbook The Statistical Sleuth. Install the package Sleuth3 on your computer using RStudio. Load the package using the library() command. Print out the dataset case0101 "],
["2-data-frames.html", "Chapter 2 Data Frames 2.1 Introduction to Importing Data 2.2 Data Types 2.3 Basic Manipulation 2.4 Exercises", " Chapter 2 Data Frames # Load my favorite packages: dplyr, ggplot2, forcats, readr, and stringr library(tidyverse, quietly = TRUE) Data frames are the fundamental unit of data storage that casual users of R need to work with. Conceptually they are just like a single tab in a spreadsheet (e.g. Excel) file. There are multiple rows and columns and each column is of the same type of information (e.g. numerical values, dates, or character strings) and each row represents a single observation. Because the columns have meaning and we generally give them column names, it is desirable to want to access an element by the name of the column as opposed to the column number. While writing formulas in large Excel spreadsheets I often get annoyed trying to remember which column something was in and muttering “Was total biomass in column P or Q?” A system where I could just name the column Total_Biomass and then always refer to it that way, is much nicer to work with and I make fewer dumb mistakes. In this chapter we will briefly cover the minimal set of tools for working with data frames. First we discuss how to import data sets, both packages from packages and from appropriately formated Excel and .csv files. Finally we’ll see how to create a data frame “by hand” and to access columns and do simple manipulations. In this chapter, we will focus on standard R data frame manipulations so that readers gain basic familiarity with non-tidyverse accessor methods. 2.1 Introduction to Importing Data 2.1.1 From a Package For many students, they will be assigned homework that utilizes data sets that are stored in some package. To access those, we would need to first install the package if we haven’t already. Recall to do that, we can use the Rstudio menu bar “Tools -&gt; Install Packages…” mouse action. Because we might have thousands of packages installed on a computer, and those packages might all have data sets associated with them, they aren’t loaded into memory by default. Instead we have to go through a two-step process of making sure that the package is installed on the computer, and then load the desired data set into the running session of R. Once the package is intalled, we can load the data into our session via the following command: data(&#39;alfalfa&#39;, package=&#39;faraway&#39;) # load the data set &#39;alfalfa&#39; from the package &#39;faraway&#39; Because R tries to avoid loading datasets until it is sure that you need them, the object alfalfa isn’t initially loaded as a data.frame but rather as a “promise” that it eventually will be loaded whenever you first use it. So lets first access it by viewing it. View(alfalfa) There are two ways to enter the view command. Either executing the View() function from the console, or clicking on either the white table or the object name in the Environment tab. # Show the image of the environment tab with the white table highlighted 2.1.2 Import from .csv or .xls files Often times data is stored in a “Comma Separated Values” file (with the file suffix of .csv) where the rows in the file represent the data frame rows, and the columns are just separated by commas. The first row of the file is usually the column titles. Alternatively, the data might be stored in an Excel file and we just need to tell R where the file is and which worksheet tab to import. The hardest part for people that are new to programming is giving the path to the data file. In this case, I recommend students use the data import wizard that RStudio includes which is accessed via ‘File -&gt; Import Dataset’. This will then give you a choice of file types to read from (.csv files are in the “Text” options). Once you have selected the file type to import, the user is presented with a file browser window where the desired file should be located. Once the file is chosen, we can import of the file. Critically, we should notice that the import wizard generates R code that does the actual import. We MUST copy that code into our Rmarkdown file or else the import won’t happen when we try to knit the Rmarkdown into an output document because knitting always occurs in a completely fresh R session. So only use the import wizard to generate the import code! The code generated by the import wizard ends with a View() command and I typically remove that as it can interfer with the knitting process. The code that I’ll paste into my RMarkdown file typically looks like this: library(readxl) Melioid_IgG &lt;- read_excel(&quot;~/Dropbox/NAU/MAGPIX serology/Data/Melioid_IgG.xlsx&quot;) # View(Melioid_IgG) 2.2 Data Types Data frames are required that each column have the same type. That is to say, if a column is numeric, you can just change one value to a character string. Below are the most common data types that are commonly used within R. Integers - These are the integer numbers \\(\\left(\\dots,-2,-1,0,1,2,\\dots\\right)\\). To convert a numeric value to an integer you may use the function as.integer(). Numeric - These could be any number (whole number or decimal). To convert another type to numeric you may use the function as.numeric(). Strings - These are a collection of characters (example: Storing a student’s last name). To convert another type to a string, use as.character(). Factors - These are strings that can only values from a finite set. For example we might wish to store a variable that records home department of a student. Since the department can only come from a finite set of possibilities, I would use a factor. Factors are categorical variables, but R calls them factors instead of categorical variable. A vector of values of another type can always be converted to a factor using the as.factor() command. For converting numeric values to factors, I will often use the function cut(). Logicals - This is a special case of a factor that can only take on the values TRUE and FALSE. (Be careful to always capitalize TRUE and FALSE. Because R is case-sensitive, TRUE is not the same as true.) Using the function as.logical() you can convert numeric values to TRUE and FALSE where 0 is FALSE and anything else is TRUE. Depending on the command, R will coerce your data from one type to another if necessary, but it is a good habit to do the coercion yourself. If a variable is a number, R will automatically assume that it is continuous numerical variable. If it is a character string, then R will assume it is a factor when doing any statistical analysis. Most of these types are familiar to beginning R users except for factors. Factors are how R keeps track of categorical variables. R does this in a two step pattern. First it figures out how many categories there are and remembers which category an observation belongs two and second, it keeps a vector character strings that correspond to the names of each of the categories. # A character vector y &lt;- c(&#39;B&#39;,&#39;B&#39;,&#39;A&#39;,&#39;A&#39;,&#39;C&#39;) y ## [1] &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;C&quot; # convert the vector of characters into a vector of factors z &lt;- factor(y) str(z) ## Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 2 2 1 1 3 Notice that the vector z is actually the combination of group assignment vector 2,2,1,1,3 and the group names vector “A”,”B”,”C”. So we could convert z to a vector of numerics or to a vector of character strings. as.numeric(z) ## [1] 2 2 1 1 3 as.character(z) ## [1] &quot;B&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;C&quot; Often we need to know what possible groups there are, and this is done using the levels() command. levels(z) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; Notice that the order of the group names was done alphabetically, which we did not chose. This ordering of the levels has implications when we do an analysis or make a plot and R will always display information about the factor levels using this order. It would be nice to be able to change the order. Also it would be really nice to give more descriptive names to the groups rather than just the group code in my raw data. Useful functions for controling the order and labels of the factor can be found in the forcats package which we use in a later chapter. 2.3 Basic Manipulation Occasionally I’ll need to create a small data frame “by hand” to facilitate creating graphs in ggplot2. In this final section, we’ll cover creating a data frame and doing simple manipulations using the base R commands and syntax. To create a data frame, we have to squish together a bunch of columns vectors. The command data.frame() does exactly that. In the example below, I list the names, ages and heights (in inches) of my family. family &lt;- data.frame( Names = c(&#39;Derek&#39;, &#39;Aubrey&#39;, &#39;Elise&#39;, &#39;Casey&#39;), Age = c(42, 39, 6, 3), Height.in = c(64, 66, 43, 39) ) family ## Names Age Height.in ## 1 Derek 42 64 ## 2 Aubrey 39 66 ## 3 Elise 6 43 ## 4 Casey 3 39 To access a particular column, we could use the $ operator. We could then do something like calculate the mean or standard deviation. family$Age ## [1] 42 39 6 3 mean( family$Age ) ## [1] 22.5 sd( family$Age ) ## [1] 20.85665 As an alternative to the “$” operator, we could use the [row, column] notation. To select a particular row or column, we can select them by either name or location. family[ , &#39;Age&#39;] # all the rows, Age column ## [1] 42 39 6 3 family[ 2, &#39;Age&#39;] # age of person in row 2 ## [1] 39 Next we could calculate everybodies height in centimeters by multiplying the heights by 2.54 and saving the result in column appropriately named. family$Height.cm &lt;- family$Height.in * 2.54 # calculate the heights and save them! family # view our result! ## Names Age Height.in Height.cm ## 1 Derek 42 64 162.56 ## 2 Aubrey 39 66 167.64 ## 3 Elise 6 43 109.22 ## 4 Casey 3 39 99.06 2.4 Exercises Create a data frame “by hand” with the names and heights of your own family. Calculate the mean age amongst your family. I have a spreadsheet file hosted on GitHub at https://raw.githubusercontent.com/dereksonderegger/570L/master/data-raw/Example_1.csv. Because the readr package doesn’t care whether a file is on your local computer or on the Internet, we’ll use this file. Start the import wizard using: “File -&gt; Import Dataset -&gt; From Text (readr) …” and input the above web URL. Click the update button near the top to cause the wizard to preview the result. Save the generated code to your Rmarkdown file and show the first few rows using the head() command. "],
["3-graphing.html", "Chapter 3 Graphing 3.1 Basic Graphs 3.2 Labels 3.3 Faceting 3.4 Exercises", " Chapter 3 Graphing library(tidyverse, quietly = TRUE) # loading ggplot2 and dplyr There are three major “systems” of making graphs in R. The basic plotting commands in R are quite effective but the commands do not have a way of being combined in easy ways. Lattice graphics (which the mosaic package uses) makes it possible to create some quite complicated graphs but it is very difficult to do make non-standard graphs. The last package, ggplot2 tries to not anticipate what the user wants to do, but rather provide the mechanisms for pulling together different graphical concepts and the user gets to decide which elements to combine. To make the most of ggplot2 it is important to wrap your mind around “The Grammar of Graphics”. Briefly, the act of building a graph can be broken down into three steps. Define what data set we are using. What is the major relationship we wish to examine? In what way should we present that relationship? These relationships can be presented in multiple ways, and the process of creating a good graph relies on building layers upon layers of information. For example, we might start with printing the raw data and then overlay a regression line over the top. Next, it should be noted that ggplot2 is designed to act on data frames. It is actually hard to just draw three data points and for simple graphs it might be easier to use the base graphing system in R. However for any real data analysis project, the data will already be in a data frame and this is not an annoyance. These notes are sufficient for creating simple graphs using ggplot2, but are not intended to be exhaustive. There are many places online to get help with ggplot2. One very nice resource is the website, http://www.cookbook-r.com/Graphs/, which gives much of the information available in the book R Graphics Cookbook which I highly recommend. Second is just googling your problems and see what you can find on websites such as StackExchange. One way that ggplot2 makes it easy to form very complicated graphs is that it provides a large number of basic building blocks that, when stacked upon each other, can produce extremely complicated graphs. A full list is available at http://docs.ggplot2.org/current/ but the following list gives some idea of different building blocks. These different geometries are different ways to display the relationship between variables and can be combined in many interesting ways. Geom Description Required Aesthetics geom_histogram A histogram x geom_bar A barplot x geom_density A density plot of data. (smoothed histogram) x geom_boxplot Boxplots x, y geom_line Draw a line (after sorting x-values) x, y geom_path Draw a line (without sorting x-values) x, y geom_point Draw points (for a scatterplot) x, y geom_smooth Add a ribbon that summarizes a scatterplot x, y geom_ribbon Enclose a region, and color the interior ymin, ymax geom_errorbar Error bars ymin, ymax geom_text Add text to a graph x, y, label geom_label Add text to a graph x, y, label geom_tile Create Heat map x, y, fill A graph can be built up layer by layer, where: Each layer corresponds to a geom, each of which requires a dataset and a mapping between an aesthetic and a column of the data set. If you don’t specify either, then the layer inherits everything defined in the ggplot() command. You can have different datasets for each layer! Layers can be added with a +, or you can define two plots and add them together (second one over-writes anything that conflicts). 3.1 Basic Graphs 3.1.1 Scatterplots To start with, we’ll make a very simple scatterplot using the iris dataset. The iris dataset contains observations on three species of iris plants where we’ve measured the length and width of both the petals and sepals. We will make a scatterplot of Sepal.Length versus Petal.Length, which are two columns in the dataset. data(iris) # load the iris dataset that comes with R str(iris) # what columns do we have to play with... ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ggplot( data=iris, aes(x=Sepal.Length, y=Petal.Length) ) + geom_point( ) The data set we wish to use is specified using data=iris. The relationship we want to explore is x=Sepal.Length and y=Petal.Length. This means the x-axis will be the Sepal Length and the y-axis will be the Petal Length. The way we want to display this relationship is through graphing 1 point for every observation. We can define other attributes that might reflect other aspects of the data. For example, we might want for the color of the data point to change dynamically based on the species of iris. ggplot( data=iris, aes(x=Sepal.Length, y=Petal.Length, color=Species) ) + geom_point( ) The aes() command inside the previous section of code is quite mysterious. The way to think about the aes() is that it gives you a way to define relationships that are data dependent. In the previous graph, the x-value and y-value for each point was defined dynamically by the data, as was the color. If we just wanted all the data points to be colored blue and larger, then the following code would do that ggplot( data=iris, aes(x=Sepal.Length, y=Petal.Length) ) + geom_point( color=&#39;blue&#39;, size=4 ) The important part isn’t that color and size were defined in the geom_point() but that they were defined outside of an aes() function! Anything set inside an aes() command will be of the form attribute=Column_Name and will change based on the data. Anything set outside an aes() command will be in the form attribute=value and will be fixed. 3.1.2 Box Plots Boxplots are a common way to show a categorical variable on the x-axis and continuous on the y-axis. ggplot(iris, aes(x=Species, y=Petal.Length)) + geom_boxplot() The boxes show the \\(25^{th}\\), \\(50^{th}\\), and \\(75^{th}\\) percentile and the lines coming off the box extend to the smallest and largest non-outlier observation. 3.2 Labels To make a graph more understandable, it is necessary to tweak labels for the axes and add a main title and such. Here we’ll adjust labels in a graph, including the legend labels. # Treat the number of cylinders in a car as a categorical variable (4,6 or 8) ggplot( data=iris, aes(x=Sepal.Length, y=Petal.Length, color=Species) ) + geom_point( ) + labs( title=&#39;Sepal Length vs Petal Length&#39;) + labs( x=&quot;Sepal Length (cm)&quot;, y=&quot;Petal Length (cm)&quot; ) + labs( color=&quot;Species Name&quot;) You could either call the labs() command repeatedly with each label, or you could provide multiple arguements to just one labs() call. 3.3 Faceting The goal with faceting is to make many panels of graphics where each panel represents the same relationship between variables, but something changes between each panel. For example using the iris dataset we could look at the relationship between Sepal.Length and Petal.Length either with all the data in one graph, or one panel per species. library(ggplot2) ggplot(iris, aes(x=Sepal.Length, y=Petal.Length)) + geom_point() + facet_grid( . ~ Species ) The line facet_grid( formula ) tells ggplot2 to make panels, and the formula tells how to orient the panels. In R formulas are always interpretated in the order y ~ x. Because I want the species to change as we go across the page, but don’t have anything I want to change vertically we use . ~ Species to represent that. If we had wanted three graphs stacked then we could use Species ~ .. For a second example, we look at a dataset that examines the amount a waiter was tipped by 244 parties. Covariates that were measured include the day of the week, size of the party, total amount of the bill, amount tipped, whether there were smokers in the group and the gender of the person paying the bill data(tips, package=&#39;reshape&#39;) head(tips) ## total_bill tip sex smoker day time size ## 1 16.99 1.01 Female No Sun Dinner 2 ## 2 10.34 1.66 Male No Sun Dinner 3 ## 3 21.01 3.50 Male No Sun Dinner 3 ## 4 23.68 3.31 Male No Sun Dinner 2 ## 5 24.59 3.61 Female No Sun Dinner 4 ## 6 25.29 4.71 Male No Sun Dinner 4 It is easy to look at the relationship between the size of the bill and the percent tipped. ggplot(tips, aes(x = total_bill, y = tip / total_bill )) + geom_point() Next we ask if there is a difference in tipping percent based on gender or day of the week by plotting this relationship for each combination of gender and day. ggplot(tips, aes(x = total_bill, y = tip / total_bill )) + geom_point() + facet_grid( sex ~ day ) Sometimes we want multiple rows and columns of facets, but there is only one categorical variable with many levels. In that case we use facet_wrap which takes a one-sided formula. ggplot(tips, aes(x = total_bill, y = tip / total_bill )) + geom_point() + facet_wrap( ~ day ) Finally we can allow the x and y scales to vary between the panels by setting “free”, “free_x”, or “free_y”. In the following code, the y-axis scale changes between the gender groups. ggplot(tips, aes(x = total_bill, y = tip / total_bill )) + geom_point() + facet_grid( sex ~ day, scales=&quot;free_y&quot; ) 3.4 Exercises For the dataset trees, which should already be pre-loaded. Look at the help file using ?trees for more information about this data set. We wish to build a scatterplot that compares the height and girth of these cherry trees to the volume of lumber that was produced. Create a graph using ggplot2 with Height on the x-axis, Volume on the y-axis, and Girth as the either the size of the data point or the color of the data point. Which do you think is a more intuitive representation? Add appropriate labels for the main title and the x and y axes. Consider the following small dataset that represents the number of times per day my wife played “Ring around the Rosy” with my daughter relative to the number of days since she has learned this game. The column yhat represents the best fitting line through the data, and lwr and upr represent a 95% confidence interval for the predicted value on that day. Rosy &lt;- data.frame( times = c(15, 11, 9, 12, 5, 2, 3), day = 1:7, yhat = c(14.36, 12.29, 10.21, 8.14, 6.07, 4.00, 1.93), lwr = c( 9.54, 8.5, 7.22, 5.47, 3.08, 0.22, -2.89), upr = c(19.18, 16.07, 13.2, 10.82, 9.06, 7.78, 6.75)) Using ggplot() and geom_point(), create a scatterplot with day along the x-axis and times along the y-axis. Add a line to the graph where the x-values are the day values but now the y-values are the predicted values which we’ve called yhat. Notice that you have to set the aesthetic y=times for the points and y=yhat for the line. Because each geom_ will accept an aes() command, you can specify the y attribute to be different for different layers of the graph. Add a ribbon that represents the confidence region of the regression line. The geom_ribbon() function requires an x, ymin, and ymax columns to be defined. For examples of using geom_ribbon() see the online documentation: http://docs.ggplot2.org/current/geom_ribbon.html. ggplot(Rosy, aes(x=day)) + geom_point(aes(y=times)) + geom_line( aes(y=yhat)) + geom_ribbon( aes(ymin=lwr, ymax=upr), fill=&#39;salmon&#39;) What happened when you added the ribbon? Did some points get hidden? If so, why? Reorder the statements that created the graph so that the ribbon is on the bottom and the data points are on top and the regression line is visible. The color of the ribbon fill is ugly. Use Google to find a list of named colors available to ggplot2. For example, I googled “ggplot2 named colors” and found the following link: http://sape.inf.usi.ch/quick-reference/ggplot2/colour. Choose a color for the fill that is pleasing to you. Add labels for the x-axis and y-axis that are appropriate along with a main title. We’ll next make some density plots that relate several factors towards the birthweight of a child. The MASS package contains a dataset called birthwt which contains information about 189 babies and their mothers. In particular there are columns for the mother’s race and smoking status during the pregnancy. Load the birthwt by either using the data() command or loading the MASS library. Read the help file for the dataset using MASS::birthwt. The covariates race and smoke are not stored in a user friendly manner. For example, smoking status is labeled using a 0 or a 1. Because it is not obvious which should represent that the mother smoked, we’ll add better labels to the race and smoke variables. For more information about dealing with factors and their levels, see the Factors chapter in these notes. library(tidyverse) data(&#39;birthwt&#39;, package=&#39;MASS&#39;) birthwt &lt;- birthwt %&gt;% mutate( race = factor(race, labels=c(&#39;White&#39;,&#39;Black&#39;,&#39;Other&#39;)), smoke = factor(smoke, labels=c(&#39;No Smoke&#39;, &#39;Smoke&#39;))) Graph a histogram of the birthweights bwt using ggplot(birthwt, aes(x=bwt)) + geom_histogram(). Make separate graphs that denote whether a mother smoked during pregnancy by appending + facet_grid() command to your original graphing command. Perhaps race matters in relation to smoking. Make our grid of graphs vary with smoking status changing vertically, and race changing horizontally (that is the formula in facet_grid() should have smoking be the y variable and race as the x). Remove race from the facet grid, (so go back to the graph you had in part d). I’d like to next add an estimated density line to the graphs, but to do that, I need to first change the y-axis to be density (instead of counts), which we do by using aes(y=..density..) in the ggplot() aesthetics command. Next we can add the estimated smooth density using the geom_density() command. To really make this look nice, lets change the fill color of the histograms to be something less dark, lets use fill='cornsilk' and color='grey60'. To play with different colors that have names, check out the following: [http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf]. Change the order in which the histogram and the density line are added to the plot. Does it matter and which do you prefer? Finally consider if you should have the histograms side-by-side or one ontop of the other (i.e. . ~ smoke or smoke ~ .). Which do you think better displayes the decrease in mean birthweight and why? Load the dataset ChickWeight which comes preloaded in R and get the background on the dataset by reading the manual page ?ChickWeight. Produce a separate scatter plot of weight vs age for each chick. Use color to distinguish the four different Diet treatments. We could examine this data by producing a scatterplot for each diet. Most of the code below is readable, but if we don’t add the group aesthetic the lines would not connect the dots for each Chick but would instead connect the dots across different chicks. data(ChickWeight) ggplot(ChickWeight, aes(x=Time, y=weight, group=Chick )) + geom_point() + geom_line() + facet_grid( ~ Diet) "],
["4-data-wrangling.html", "Chapter 4 Data Wrangling 4.1 Verbs 4.2 Split, apply, combine 4.3 Exercises", " Chapter 4 Data Wrangling library(tidyverse, quietly = TRUE) # loading ggplot2 and dplyr Many of the tools to manipulate data frames in R were written without a consistent syntax and are difficult use together. To remedy this, Hadley Wickham (the writer of ggplot2) introduced a package called plyr which was quite useful. As with many projects, his first version was good but not great and he introduced an improved version that works exclusively with data.frames called dplyr which we will investigate. The package dplyr strives to provide a convenient and consistent set of functions to handle the most common data frame manipulations and a mechanism for chaining these operations together to perform complex tasks. The Dr Wickham has put together a very nice introduction to the package that explains in more detail how the various pieces work and I encourage you to read it at some point. [http://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html]. One of the aspects about the data.frame object is that R does some simplification for you, but it does not do it in a consistent manner. Somewhat obnoxiously character strings are always converted to factors and subsetting might return a data.frame or a vector or a scalar. This is fine at the command line, but can be problematic when programming. Furthermore, many operations are pretty slow using data.frame. To get around this, Dr Wickham introduced a modified version of the data.frame called a tibble. A tibble is a data.frame but with a few extra bits. For now we can ignore the differences. The pipe command %&gt;% allows for very readable code. The idea is that the %&gt;% operator works by translating the command a %&gt;% f(b) to the expression f(a,b). This operator works on any function and was introduced in the magrittr package. The beauty of this comes when you have a suite of functions that takes input arguments of the same type as their output. For example, if we wanted to start with x, and first apply function f(), then g(), and then h(), the usual R command would be h(g(f(x))) which is hard to read because you have to start reading at the innermost set of parentheses. Using the pipe command %&gt;%, this sequence of operations becomes x %&gt;% f() %&gt;% g() %&gt;% h(). Written Meaning a %&gt;% f(b) f(a,b) b %&gt;% f(a, .) f(a, b) x %&gt;% f() %&gt;% g() g( f(x) ) In dplyr, all the functions below take a data set as its first argument and outputs an appropriately modified data set. This will allow me to chain together commands in a readable fashion. The pipe command works with any function, not just the dplyr functions and I often find myself using it all over the place. 4.1 Verbs The foundational operations to perform on a data set are: Subsetting - Returns a with only particular columns or rows – select - Selecting a subset of columns by name or column number. – filter - Selecting a subset of rows from a data frame based on logical expressions. – slice - Selecting a subset of rows by row number. arrange - Re-ordering the rows of a data frame. mutate - Add a new column that is some function of other columns. summarise - calculate some summary statistic of a column of data. This collapses a set of rows into a single row. Each of these operations is a function in the package dplyr. These functions all have a similar calling syntax, the first argument is a data set, subsequent arguments describe what to do with the input data frame and you can refer to the columns without using the df$column notation. All of these functions will return a data set. 4.1.1 Subsetting These function allows you select certain columns and rows of a data frame. 4.1.1.1 select() Often you only want to work with a small number of columns of a data frame and want to be able to select a subset of columns or perhaps remove a subset. The function to do that is dplyr::select() # Create a tiny data frame that is easy to see what is happening grades &lt;- data.frame( l.name = c(&#39;Cox&#39;, &#39;Dorian&#39;, &#39;Kelso&#39;, &#39;Turk&#39;), Exam1 = c(93, 89, 80, 70), Exam2 = c(98, 70, 82, 85), Final = c(96, 85, 81, 92) ) grades ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 ## 2 Dorian 89 70 85 ## 3 Kelso 80 82 81 ## 4 Turk 70 85 92 I could select the columns Exam columns by hand, or by using an extension of the : operator # select( grades, Exam1, Exam2 ) # from `grades`, select columns Exam1, Exam2 grades %&gt;% select( Exam1, Exam2 ) # Exam1 and Exam2 ## Exam1 Exam2 ## 1 93 98 ## 2 89 70 ## 3 80 82 ## 4 70 85 grades %&gt;% select( Exam1:Final ) # Columns Exam1 through Final ## Exam1 Exam2 Final ## 1 93 98 96 ## 2 89 70 85 ## 3 80 82 81 ## 4 70 85 92 grades %&gt;% select( -Exam1 ) # Negative indexing by name drops a column ## l.name Exam2 Final ## 1 Cox 98 96 ## 2 Dorian 70 85 ## 3 Kelso 82 81 ## 4 Turk 85 92 grades %&gt;% select( 1:2 ) # Can select column by column position ## l.name Exam1 ## 1 Cox 93 ## 2 Dorian 89 ## 3 Kelso 80 ## 4 Turk 70 The select() command has a few other tricks. There are functional calls that describe the columns you wish to select that take advantage of pattern matching. I generally can get by with starts_with(), ends_with(), and contains(), but there is a final operator matches() that takes a regular expression. grades %&gt;% select( starts_with(&#39;Exam&#39;) ) # Exam1 and Exam2 ## Exam1 Exam2 ## 1 93 98 ## 2 89 70 ## 3 80 82 ## 4 70 85 The dplyr::select function is quite handy, but there are several other packages out there that have a select function and we can get into trouble with loading other packages with the same function names. If I encounter the select function behaving in a weird manner or complaining about an input argument, my first remedy is to be explicit about it is the dplyr::select() function by appending the package name at the start. 4.1.1.2 filter() It is common to want to select particular rows where we have some logical expression to pick the rows. # select students with Final grades greater than 90 grades %&gt;% filter(Final &gt; 90) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 ## 2 Turk 70 85 92 You can have multiple logical expressions to select rows and they will be logically combined so that only rows that satisfy all of the conditions are selected. The logicals are joined together using &amp; (and) operator or the | (or) operator and you may explicitly use other logicals. For example a factor column type might be used to select rows where type is either one or two via the following: type==1 | type==2. # select students with Final grades above 90 and # average score also above 90 grades %&gt;% filter(Exam2 &gt; 90, Final &gt; 90) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 # we could also use an &quot;and&quot; condition grades %&gt;% filter(Exam2 &gt; 90 &amp; Final &gt; 90) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 4.1.1.3 slice() When you want to filter rows based on row number, this is called slicing. # grab the first 2 rows grades %&gt;% slice(1:2) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 ## 2 Dorian 89 70 85 4.1.2 arrange() We often need to re-order the rows of a data frame. For example, we might wish to take our grade book and sort the rows by the average score, or perhaps alphabetically. The arrange() function does exactly that. The first argument is the data frame to re-order, and the subsequent arguments are the columns to sort on. The order of the sorting column determines the precedent… the first sorting column is first used and the second sorting column is only used to break ties. grades %&gt;% arrange(l.name) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 ## 2 Dorian 89 70 85 ## 3 Kelso 80 82 81 ## 4 Turk 70 85 92 The default sorting is in ascending order, so to sort the grades with the highest scoring person in the first row, we must tell arrange to do it in descending order using desc(column.name). grades %&gt;% arrange(desc(Final)) ## l.name Exam1 Exam2 Final ## 1 Cox 93 98 96 ## 2 Turk 70 85 92 ## 3 Dorian 89 70 85 ## 4 Kelso 80 82 81 In a more complicated example, consider the following data and we want to order it first by Treatment Level and secondarily by the y-value. I want the Treatment level in the default ascending order (Low, Medium, High), but the y variable in descending order. # make some data dd &lt;- data.frame( Trt = factor(c(&quot;High&quot;, &quot;Med&quot;, &quot;High&quot;, &quot;Low&quot;), levels = c(&quot;Low&quot;, &quot;Med&quot;, &quot;High&quot;)), y = c(8, 3, 9, 9), z = c(1, 1, 1, 2)) dd ## Trt y z ## 1 High 8 1 ## 2 Med 3 1 ## 3 High 9 1 ## 4 Low 9 2 # arrange the rows first by treatment, and then by y (y in descending order) dd %&gt;% arrange(Trt, desc(y)) ## Trt y z ## 1 Low 9 2 ## 2 Med 3 1 ## 3 High 9 1 ## 4 High 8 1 4.1.3 mutate() I often need to create a new column that is some function of the old columns. In the dplyr package, this is a mutate command. To do ths, we give a mutate( NewColumn = Function of Old Columns ) command. # Modify the grades data frame and replace the old version with the new # that contains the newly created &quot;average&quot; column grades &lt;- grades %&gt;% mutate( average = (Exam1 + Exam2 + Final)/3 ) You can do multiple calculations within the same mutate() command, and you can even refer to columns that were created in the same mutate() command. grades %&gt;% mutate( average = (Exam1 + Exam2 + Final)/3, grade = cut(average, c(0, 60, 70, 80, 90, 100), # cut takes numeric variable c( &#39;F&#39;,&#39;D&#39;,&#39;C&#39;,&#39;B&#39;,&#39;A&#39;)) ) # and makes a factor ## l.name Exam1 Exam2 Final average grade ## 1 Cox 93 98 96 95.66667 A ## 2 Dorian 89 70 85 81.33333 B ## 3 Kelso 80 82 81 81.00000 B ## 4 Turk 70 85 92 82.33333 B 4.1.4 summarise() By itself, this function is quite boring, but will become useful later on. Its purpose is to calculate summary statistics using any or all of the data columns. Notice that we get to chose the name of the new column. The way to think about this is that we are collapsing information stored in multiple rows into a single row of values. # calculate the mean of exam 1 grades %&gt;% summarise( mean.E1=mean(Exam1) ) ## mean.E1 ## 1 83 We could calculate multiple summary statistics if we like. # calculate the mean and standard deviation grades %&gt;% summarise( mean.E1=mean(Exam1), stddev.E1=sd(Exam1) ) ## mean.E1 stddev.E1 ## 1 83 10.23067 4.2 Split, apply, combine Aside from unifying the syntax behind the common operations, the major strength of the dplyr package is the ability to split a data frame into a bunch of sub-data frames, apply a sequence of one or more of the operations we just described, and then combine results back together. We’ll consider data from an experiment from spinning wool into yarn. This experiment considered two different types of wool (A or B) and three different levels of tension on the thread. The response variable is the number of breaks in the resulting yarn. For each of the 6 wool:tension combinations, there are 9 replicated observations per wool:tension level. data(warpbreaks) str(warpbreaks) ## &#39;data.frame&#39;: 54 obs. of 3 variables: ## $ breaks : num 26 30 54 25 70 52 51 26 67 18 ... ## $ wool : Factor w/ 2 levels &quot;A&quot;,&quot;B&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ tension: Factor w/ 3 levels &quot;L&quot;,&quot;M&quot;,&quot;H&quot;: 1 1 1 1 1 1 1 1 1 2 ... The first we must do is to create a data frame with additional information about how to break the data into sub-data frames. In this case, I want to break the data up into the 6 wool-by-tension combinations. Initially we will just figure out how many rows are in each wool-by-tension combination. # group_by: what variable(s) shall we group on. # n() is a function that returns how many rows are in the # currently selected sub-dataframe warpbreaks %&gt;% group_by( wool, tension) %&gt;% # grouping summarise(n = n() ) # how many in each group ## # A tibble: 6 x 3 ## # Groups: wool [2] ## wool tension n ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 A L 9 ## 2 A M 9 ## 3 A H 9 ## 4 B L 9 ## 5 B M 9 ## 6 B H 9 The group_by function takes a data.frame and returns the same data.frame, but with some extra information so that any subsequent function acts on each unique combination defined in the group_by. If you wish to remove this behavior, use group_by() to reset the grouping to have no grouping variable. Using the same summarise function, we could calculate the group mean and standard deviation for each wool-by-tension group. warpbreaks %&gt;% group_by(wool, tension) %&gt;% summarise( n = n(), # I added some formatting to show the mean.breaks = mean(breaks), # reader I am calculating several sd.breaks = sd(breaks)) # statistics. ## # A tibble: 6 x 5 ## # Groups: wool [2] ## wool tension n mean.breaks sd.breaks ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A L 9 44.6 18.1 ## 2 A M 9 24 8.66 ## 3 A H 9 24.6 10.3 ## 4 B L 9 28.2 9.86 ## 5 B M 9 28.8 9.43 ## 6 B H 9 18.8 4.89 If instead of summarizing each split, we might want to just do some calculation and the output should have the same number of rows as the input data frame. In this case I’ll tell dplyr that we are mutating the data frame instead of summarizing it. For example, suppose that I want to calculate the residual value \\[e_{ijk}=y_{ijk}-\\bar{y}_{ij\\cdot}\\] where \\(\\bar{y}_{ij\\cdot}\\) is the mean of each wool:tension combination. warpbreaks %&gt;% group_by(wool, tension) %&gt;% # group by wool:tension mutate(resid = breaks - mean(breaks)) %&gt;% # mean(breaks) of the group! head( ) # show the first couple of rows ## # A tibble: 6 x 4 ## # Groups: wool, tension [1] ## breaks wool tension resid ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 26 A L -18.6 ## 2 30 A L -14.6 ## 3 54 A L 9.44 ## 4 25 A L -19.6 ## 5 70 A L 25.4 ## 6 52 A L 7.44 4.3 Exercises The dataset ChickWeight tracks the weights of 48 baby chickens (chicks) feed four different diets. Load the dataset using data(ChickWeight) Look at the help files for the description of the columns. Remove all the observations except for observations from day 10 or day 20. The tough part in this instruction is disguishing between “and” and “or”. Obviously there are no observations that occur from both day 10 AND day 20. Either google ‘R logical operators’ Calculate the mean and standard deviation of the chick weights for each diet group on days 10 and 20. The OpenIntro textbook on statistics includes a data set on body dimensions. Load the file using Body &lt;- read.csv(&#39;http://www.openintro.org/stat/data/bdims.csv&#39;) The column sex is coded as a 1 if the individual is male and 0 if female. This is a non-intuitive labeling system. Create a new column sex.MF that uses labels Male and Female. Hint: the ifelse() command will be very convenient here. The ifelse() command in R functions similarly to the same command in Excel. The columns wgt and hgt measure weight and height in kilograms and centimeters (respectively). Use these to calculate the Body Mass Index (BMI) for each individual where \\[BMI=\\frac{Weight\\,(kg)}{\\left[Height\\,(m)\\right]^{2}}\\] Double check that your calculated BMI column is correct by examining the summary statistics of the column (e.g. summary(Body)). BMI values should be between 18 to 40 or so. Did you make an error in your calculation? The function cut takes a vector of continuous numerical data and creates a factor based on your give cut-points. # Define a continuous vector to convert to a factor x &lt;- 1:10 # divide range of x into three groups of equal length cut(x, breaks=3) ## [1] (0.991,4] (0.991,4] (0.991,4] (0.991,4] (4,7] (4,7] (4,7] ## [8] (7,10] (7,10] (7,10] ## Levels: (0.991,4] (4,7] (7,10] # divide x into four groups, where I specify all 5 break points cut(x, breaks = c(0, 2.5, 5.0, 7.5, 10)) ## [1] (0,2.5] (0,2.5] (2.5,5] (2.5,5] (2.5,5] (5,7.5] (5,7.5] ## [8] (7.5,10] (7.5,10] (7.5,10] ## Levels: (0,2.5] (2.5,5] (5,7.5] (7.5,10] # (0,2.5] (2.5,5] means 2.5 is included in first group # right=FALSE changes this to make 2.5 included in the second # divide x into 3 groups, but give them a nicer # set of group names cut(x, breaks=3, labels=c(&#39;Low&#39;,&#39;Medium&#39;,&#39;High&#39;)) ## [1] Low Low Low Low Medium Medium Medium High High High ## Levels: Low Medium High Create a new column of in the data frame that divides the age into decades (10-19, 20-29, 30-39, etc). Notice the oldest person in the study is 67. Body &lt;- Body %&gt;% mutate( Age.Grp = cut(age, breaks=c(10,20,30,40,50,60,70), right=FALSE)) Find the average BMI for each Sex-by-Age combination. "],
["5-statistical-models.html", "Chapter 5 Statistical Models 5.1 Formula Notation 5.2 Basic Models 5.3 Accessor function using traditional functions 5.4 Accessing model results using broom", " Chapter 5 Statistical Models library(tidyverse, quietly = TRUE) # loading ggplot2 and dplyr While R is a full programming language, it was first developed by statisticians for statisticians. There are several functions to do common statistical tests but because those functions were developed early in R’s history, there is some inconsistency in how those functions work. There have been some attempts to standardize modeling object interfaces, but there were always be a little weirdness. 5.1 Formula Notation Most statistical modeling functions rely on a formula based interface. The primary purpose is to provide a consistent way to designate which columns in a data frame are the response variable and which are the explanatory variables. In particular the notation is \\[\\underbrace{y}_{\\textrm{LHS response}} \\;\\;\\; \\underbrace{\\sim}_{\\textrm{is a function of}} \\;\\;\\; \\underbrace{x}_{\\textrm{RHS explanatory}}\\] Mathematicians often refer to these terms as the Left Hand Side (LHS) and Right Hand Side (RHS). The LHS is always the response and the RHS contains the explanatory variables. In R, the LHS is usually just a single variable in the data. However the RHS can contain multiple variables and in complicated relationships. Right Hand Side Terms Meaning x1 + x2 Both x1 and x2 are additive explanatory variables. In this format, we are adding only the main effects of the x1 and x2 variables. x1:x2 This is the interaction term between x1 and x2 x1 * x2 Because whenever we add an interaction term to a model, we want to also have the main effects. So this is a short cut for adding the main effect of x1 and x2 and also the interaction term x1:x2. (x1 + x2 + x3)^2 This is the main effects of x1, x2, and x3 and also all of the second order interactions. poly(x, degree=2) This fits the degree 2 polynomial. When fit like this, R produces an orthogonal basis for the polynomial, which is more computationally stable, but won’t be appropiate for interpreting the polynomial coefficients. poly(x, degree=2, raw=TRUE) This fits the degree 2 polynomial using \\(\\beta_0 + \\beta_1 x + \\beta_2 x^2\\) and the polynomial polynomial coefficients are suitable for interpretation. I( x^2 ) Ignore the usual rules for interpreting formulas and do the mathematical calculation. This is not necessary for things like sqrt(x) or log(x) but required if there is a conflict between mathematics and the formula interpretation. 5.2 Basic Models The most common statistical models are generally referred to as linear models and the R function for creating a linear model is lm(). This section will introduce how to fit the model to data in a data frame as well as how to fit very specific t-test models. 5.2.1 t-tests In R, the historical function to fit both paired and unpaired t-tests is the function t.test(). ### lm objects 5.3 Accessor function using traditional functions 5.4 Accessing model results using broom "],
["6-graphing-data-and-model-results.html", "Chapter 6 Graphing data and model results 6.1 Process 6.2 Augmenting data with model output 6.3 Plotting using ggplot2", " Chapter 6 Graphing data and model results library(tidyverse, quietly = TRUE) # loading ggplot2 and dplyr 6.1 Process 6.2 Augmenting data with model output 6.3 Plotting using ggplot2 "],
["7-factors.html", "Chapter 7 Factors 7.1 How factors are stored 7.2 Using forcats", " Chapter 7 Factors 7.1 How factors are stored 7.2 Using forcats "],
["8-data-frames-tibbles-oh-my.html", "Chapter 8 Data Frames, tibbles, oh my! 8.1 Why extend the data frame", " Chapter 8 Data Frames, tibbles, oh my! 8.1 Why extend the data frame "],
["9-data-import.html", "Chapter 9 Data Import", " Chapter 9 Data Import "],
["10-data-frame-manipulation-using-dplyr.html", "Chapter 10 Data frame Manipulation using dplyr", " Chapter 10 Data frame Manipulation using dplyr "],
["11-data-frame.html", "Chapter 11 Data frame 11.1 reshaping 11.2 merges", " Chapter 11 Data frame 11.1 reshaping 11.2 merges "],
["12-more-ggplot2.html", "Chapter 12 More ggplot2", " Chapter 12 More ggplot2 "],
["13-flow-control.html", "Chapter 13 Flow Control", " Chapter 13 Flow Control "],
["14-functions.html", "Chapter 14 Functions", " Chapter 14 Functions "],
["15-strings.html", "Chapter 15 Strings", " Chapter 15 Strings "],
["16-dates.html", "Chapter 16 Dates", " Chapter 16 Dates "],
["17-performance-issues.html", "Chapter 17 Performance Issues", " Chapter 17 Performance Issues "]
]
