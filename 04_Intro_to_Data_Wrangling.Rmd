# Data Wrangling

```{r, echo=FALSE}
# Unattach any packages that happen to already be loaded. In general this is unecessary
# but is important for the creation of the book to not have package namespaces
# fighting unexpectedly.
pkgs = names(sessionInfo()$otherPkgs)
if( length(pkgs > 0)){
  pkgs = paste('package:', pkgs, sep = "")
  for( i in 1:length(pkgs)){
    detach(pkgs[i], character.only = TRUE, force=TRUE)
  }
}
```


```{r, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse, quietly = TRUE)    # loading ggplot2 and dplyr
options(dplyr.summarise.inform=FALSE) # Don't annoy me with summary messages
```

As always, there is a [Video Lecture](https://youtu.be/99Q7AunWuk0) that 
accompanies this chapter.
  
Many of the tools to manipulate data frames in R were written without a consistent 
syntax and are difficult use together. To remedy this, Hadley Wickham 
(the writer of `ggplot2`) introduced a package called `plyr` which was quite useful. 
As with many projects, his first version was good, but not great, and he introduced 
an improved version that works exclusively with data frames called `dplyr` which 
we will investigate. The package `dplyr` strives to provide a convenient and 
consistent set of functions to handle the most common data frame manipulations, 
and a mechanism for chaining these operations together to perform complex tasks. 

Dr. Wickham has put together a very nice introduction to the package that explains 
in more detail how the various pieces work and I encourage you to read it at some point. [https://cran.rstudio.com/web/packages/dplyr/vignettes/dplyr.html].

One of the aspects about the `data.frame` object is that R does some simplification 
for you, but it does not do it in a consistent manner. Somewhat obnoxiously, 
character strings are always converted to factors and subsetting might return a 
`data.frame` or a `vector` or a `scalar`. This is fine at the command line, but 
can be problematic when programming. Furthermore, many operations are pretty 
slow using `data.frame`. To get around this, Dr. Wickham introduced a modified 
version of the `data.frame` called a `tibble`. A `tibble` is a `data.frame` but 
with a few extra bits. For now we can ignore the differences.

The pipe command `%>%` allows for very readable code. The idea is that the `%>%` 
operator works by translating the command `a %>% f(b)` to the expression `f(a,b)`. 
This operator works on any function and was introduced in the `magrittr` package. 
The beauty of this comes when you have a suite of functions that takes input 
arguments of the same type as their output. 

For example, if we wanted to start with `x`, and first apply function `f()`, 
then `g()`, and then `h()`, the usual R command would be `h(g(f(x)))` which is 
hard to read because you have to start reading at the *innermost* set of 
parentheses. Using the pipe command `%>%`, this sequence of operations 
becomes `x %>% f() %>% g() %>% h()`.


|     Written         |  Meaning       |
|:-------------------:|:--------------:|
| `a %>% f(b)`        |   `f(a,b)`     |
| `b %>% f(a, .)`     |   `f(a, b)`    |
| `x %>% f() %>% g()` |  `g( f(x) )`   |

```{r}
# This code is not particularly readable because
# the order of summing vs taking absolute value isn't 
# completely obvious. 
sum(abs(c(-1,0,1)))

# But using the pipe function, it is blatantly obvious
# what order the operations are done in. 
c( -1, 0, 1) %>%  # take a vector of values
  abs()  %>%      # take the absolute value of each
  sum()           # add them up.
```


In `dplyr`, all the functions below take a _data set as its first argument_ 
and _outputs an appropriately modified data set_. This will allow me to chain 
together commands in a readable fashion. The pipe command works with any function, 
not just the `dplyr` functions and I often find myself using it all over the place.


## Verbs

The foundational operations to perform on a data set are:

| Adding rows   | Adds to a data set                           |
|--------------:|:--------------------------------------------|
| `add_rows()`  | Add an additional single row of data, specified by cell. |
| `bind_rows()` | Add additional row(s) of data, specified by the added data table. |


| Subsetting    |  Returns a data set with particular columns or rows |
|--------------:|:--------------------------------------------|
|  `select`     | Selecting a subset of columns by name or column number. Helper functions such as `starts_with()`, `ends_with()`, and `contains()` allows you pick columns that have certain attributes in their column names. |
| `filter()`      | Selecting a subset of rows from a data frame based on logical expressions. |
| `slice()`       | Selecting a subset of rows by row number. There are a few variants that allow for common tasks to such as `slice_head()` `slice_tail()` and `slice_sample()` |
| `drop_na`    |  Remove rows that contain any missing values. |


| Sorting | Returns a data table with the rows sorted according to a particular column(s). |
|--------------:|:--------------------------------------------|
| `arrange()`   | Re-ordering the rows of a data frame. The `desc()` function can be used on the selected column to reverse the sort direction. |

| Update/Add columns | Returns a data table updated and/or new column(s). | 
|--------------:|:--------------------------------------------|
|`mutate()`       | Add a new column that is some function of other columns. This function is used with an `ifelse()` command for updating particular cells and `across()` to apply some function to a variety of columns. |

| Summarize   | Returns a data table with many rows into summarized into one row. | 
|--------------:|:--------------------------------------------|
| `summarise()` | Calculate some summary statistic of a column of data. This collapses a set of rows into fewer (often one) rows. |

Each of these operations is a function in the package `dplyr`. These functions 
all have a similar calling syntax, the first argument is a data set, subsequent 
arguments describe what to do with the input data frame and you can refer to the 
columns without using the `df$column` notation. All of these functions will return 
a data set.


To demonstrate all of these actions, we will consider a tiny dataset of a 
gradebook of doctors at a Sacred Heart Hospital.

```{r}
# Create a tiny data frame that is easy to see what is happening
Mentors <- tribble(
  ~l.name, ~Gender, ~Exam1, ~Exam2, ~Final,
  'Cox',     'M',     93.2,   98.3,     96.4,
  'Kelso',   'M',     80.7,   82.8,     81.1)
Residents <- tribble(
  ~l.name, ~Gender, ~Exam1, ~Exam2, ~Final,
  'Dorian',  'M',     89.3,   70.2,   85.7,
  'Turk',    'M',     70.9,   85.5,   92.2)
```


### `add_row`
Suppose that we want to add a row to our dataset. We can give it as much or as 
little information as we want and any missing information will be denoted as 
missing using a `NA` which stands for *N*ot *A*vailable.
```{r}
Residents  %>% 
  add_row( l.name='Reid', Exam1=95.3, Exam2=92.0)
```

Because we didn't assign the result of our previous calculation to any object 
name, R just printed the result. Instead, lets add all of Dr Reid's information 
and save the result by *overwritting* the `grades` data.frame with the new version.
```{r}
Residents <- Residents %>%
  add_row( l.name='Reid', Gender='F', Exam1=95.3, Exam2=92.0, Final=100.0)
Residents
```

### `bind_rows`
To combine two data frames together, we'll bind them together using `bind_rows()`. 
We just need to specify the order to stack them. 
```{r}
# now to combine two data frames by stacking Mentors first and then Residents
grades <- Mentors %>%
  bind_rows(Residents)

grades
```


### Subsetting

These function allows you select certain columns and rows of a data frame.

#### `select()`

Often you only want to work with a small number of columns of a data frame and 
want to be able to *select* a subset of columns or perhaps remove a subset. 
The function to do that is `dplyr::select()`. 

I could select the Exam columns by hand, or by using an extension of the `:` operator.
```{r}
# select( grades,  Exam1, Exam2 )   # from `grades`, select columns Exam1, Exam2
grades %>% select( Exam1, Exam2 )   # Exam1 and Exam2
grades %>% select( Exam1:Final )    # Columns Exam1 through Final
grades %>% select( -Exam1 )         # Negative indexing by name drops a column
grades %>% select( 1:2 )            # Can select column by column position
```

The `select()` command has a few other tricks. There are functional calls that 
describe the columns you wish to select that take advantage of pattern matching. 
I generally can get by with `starts_with()`, `ends_with()`, and `contains()`, 
but there is a final operator `matches()` that takes a regular expression.
```{r}
grades %>% select( starts_with('Exam') )   # Exam1 and Exam2
grades %>% select( starts_with(c('Exam','F')) ) # All three exams 
```

The `select` function allows you to include multiple selector helpers.

The [help file](https://tidyselect.r-lib.org/reference/select_helpers.html) 
for `tidyselect` package describes a few other interesting selection helper 
functions. One final one is the `where()` command which will apply a function 
to each column and return the columns in which the values will evaluate to TRUE.
This is particularly handy for selecting all numeric columns or all columns that
are character strings.

```{r}
# select only the numerical numerical columns
grades %>% select( where(is.numeric) )
```

```{r, eval=FALSE}
# select numerical or character columns
grades %>% select( where(is.numeric), where(is.character) )
```


The `dplyr::select` function is quite handy, but there are several other packages 
out there that have a `select` function and we can get into trouble with loading 
other packages with the same function names.  If I encounter the `select` function 
behaving in a weird manner or complaining about an input argument, my first remedy 
is to be explicit about it is the `dplyr::select()` function by appending the 
package name at the start. 

#### `filter()`

It is common to want to select particular rows where we have some logical expression to pick the rows. 
```{r}
# select students with Final grades greater than 90
grades %>% filter(Final > 90)
```

You can have multiple logical expressions to select rows and they will be logically 
combined so that only rows that satisfy all of the conditions are selected. The 
logicals are joined together using `&` (and) operator or the `|` (or) operator and 
you may explicitly use other logicals. For example, a factor column type might be 
used to select rows where type is either one or two via the following: `type==1 | type==2`.
```{r}
# select students with Final grades above 90 and
# average score also above 90
grades %>% filter(Final > 90, ((Exam1 + Exam2 + Final)/3) > 90)

# we could also use an "and" condition
grades %>% filter(Final > 90 & ((Exam1 + Exam2 + Final)/3) > 90)
```

#### `slice()`

When you want to filter rows based on row number, this is called slicing.
```{r}
# grab the first 2 rows
grades %>% slice(1:2)
```

There are a few other slice variants that are useful. `slice_head()` and 
`slice_tail` grab the first and last few rows. The `slice_sample()` allows us
to randomly grab table rows.

```{r}
# sample with replacement, number of rows is 100% of the original number of rows
# This is super helpful for bootstrapping code
grades %>%
  slice_sample(prop=1, replace=TRUE)  
```


### `arrange()`

We often need to re-order the rows of a data frame. For example, we might wish 
to take our grade book and sort the rows by the average score, or perhaps 
alphabetically. The `arrange()` function does exactly that. The first argument 
is the data frame to re-order, and the subsequent arguments are the columns to 
sort on. The order of the sorting column determines the precedent: the first 
sorting column is first used and the second sorting column is only used to break ties.
```{r}
grades %>% arrange(l.name)
```

The default sorting is in ascending order, so to sort the grades with the highest 
scoring person in the first row, we must tell arrange to do it in descending order 
using `desc(column.name)`.
```{r}
grades %>% arrange(desc(Final))
```

We can also order a data frame by multiple columns.
```{r}
# Arrange by Gender first, then within each gender, order by Exam2
grades %>% arrange(Gender, desc(Exam2))  
```



### mutate()

The mutate command either creates a *new* column in the data frame or *updates* 
an already existing column.

I often need to create a new column that is some function of the old columns. 
In the `dplyr` package, this is a `mutate` command. To do this, we give a 
`mutate( NewColumn = Function of Old Columns )` command. You can do multiple 
calculations within the same `mutate()` command, and you can even refer to columns 
that were created in the same `mutate()` command.
```{r}
grades <- grades %>% mutate( 
  average = (Exam1 + Exam2 + Final)/3,
  grade = cut(average, c(0, 60, 70, 80, 90, 100),  # cut takes numeric variable
                       c( 'F','D','C','B','A')) )  # and makes a factor
grades
```

If we want to update some column information we will also use the `mutate` 
command, but we need some mechanism to select the rows to change, while keeping 
all the other row values the same. The functions `if_else()` and `case_when()` 
are ideal for this task. 

The `if_else` syntax is `if_else( logical.expression, TrueValue, FalseValue )`. 
For each row of the table, the logical expression will be evaluated, and if the
expression is TRUE, the `TrueValue` is selected, otherwise `FalseValue` is. 
We can use this to update a score in our gradebook.

```{r}
# Update Dr Reid's Final Exam score to 98, and leave everybody else's alone.
grades <- grades %>%
  mutate( Final = if_else(l.name == 'Reid', 98, Final ) )
grades

```

We could also use this to modify all the rows. For example, perhaps we want to 
change the `gender` column information to have levels `Male` and `Female`.

```{r}
# Update the Gender column labels
grades <- grades %>%
  mutate( Gender = if_else(Gender == 'M', 'Male', 'Female' ) )
grades
```


To do something similar for the case where we have 3 or more categories, 
we could use the `if_else()` command repeatedly to address each category level 
separately. However this is annoying to do because the `ifelse` command is limited 
to just two cases, it would be nice if there was a generalization to multiple categories. 
The  `dplyr::case_when` function is that generalization. 
The syntax is `case_when( logicalExpression1~Value1, logicalExpression2~Value2, ... )`. 
We can have as many `LogicalExpression ~ Value` pairs as we want. 

Consider the following data frame that has name, gender, and political party 
affiliation of six individuals. In this example, we've coded male/female as 1/0 
and political party as 1,2,3 for democratic, republican, and independent. 

```{r}
people <- data.frame(
  name = c('Barack','Michelle', 'George', 'Laura', 'Bernie', 'Deborah'),
  gender = c(1,0,1,0,1,0),
  party = c(1,1,2,2,3,3)
)
people
```

Now we'll update the gender and party columns to code these columns in a readable 
fashion.
```{r}
people <- people %>%
  mutate( gender = if_else( gender == 0, 'Female', 'Male') ) %>%
  mutate( party = case_when( party == 1 ~ 'Democratic', 
                             party == 2 ~ 'Republican', 
                             party == 3 ~ 'Independent',
                             TRUE       ~ 'None Stated' ) )
people
```

Often the last case is a catch all case where the logical expression will ALWAYS 
evaluate to TRUE and this is the value for all other input.

As another alternative to the problem of recoding factor levels, we could use the 
command `forcats::fct_recode()` function. See the Factors chapter in this book 
for more information about factors.

#### Modify Multiple Columns using `across()`
We often find that we want to modify multiple columns at once. For example in
the grades, we might want to round the exams so that we don't have to deal with
any decimal points. To do this, we need to have some code to: 1) select the 
desired columns, 2) indicate the function to use, and 3) combine those. The 
`dplyr::across()` function is designed to do this. The `across` function will
work within a `mutate` or `summarise()` function.

```{r}
grades %>%
  mutate( across(
    starts_with(c('Exam', 'Final')),  # anything that select can use...
    round,                # The function I want to use
    digits = 0            # additional arguments sent into round()
  ))
```

In most of the code examples you'll find online, this is usually written in a 
single line of code, which I find somewhat ugly.

```{r, eval=FALSE}
grades %>%
  mutate(across(starts_with(c('Exam', 'Final')),  round, digits = 0))
```

As before, any `select` helper function will work, and could have rounded
all the numeric columns via the following:

```{r}
grades %>%
  mutate(across( where(is.numeric), round, digits=0 ))
```

In earlier versions of `dplyr` there was no `across` function, but instead there
where variants of `mutate` and `summarise` such as `mutate_if()` that would apply 
the desired function to some set of columns. However these made some pretty strong
assumptions about what a user was likely to want to do and, as a result, lacked the
flexibility to handle more complicated scenarios. Those scoped variant functions
have been superseded and users are encouraged to use the `across` function.



### summarise()

By itself, this function is quite boring, but will become useful later on. Its 
purpose is to calculate summary statistics using any or all of the data columns. 
Notice that we get to chose the name of the new column. The way to think about 
this is that we are collapsing information stored in multiple rows into a single 
row of values.

```{r}
# calculate the mean of exam 1
grades %>% summarise( mean.E1=mean(Exam1) )
```

We could calculate multiple summary statistics if we like.
```{r}
# calculate the mean and standard deviation 
grades %>% summarise( mean.E1=mean(Exam1), stddev.E1=sd(Exam1) )
```



## Split, apply, combine

Aside from unifying the syntax behind the common operations, the major strength 
of the `dplyr` package is the ability to split a data frame into a bunch of sub data 
frames, apply a sequence of one or more of the operations we just described, and 
then combine results back together. We'll consider data from an experiment from 
spinning wool into yarn. This experiment considered two different types of wool 
(A or B) and three different levels of tension on the thread. The response variable 
is the number of breaks in the resulting yarn. For each of the 6 `wool:tension` 
combinations, there are 9 replicated observations.
```{r}
data(warpbreaks)
str(warpbreaks)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=2.5}
library(ggplot2)
ggplot2::ggplot(warpbreaks, aes(x=tension, y=breaks, color=wool)) +
  ggplot2::geom_point() + 
  facet_grid(~wool)
```



The first we must do is to create a data frame with additional information about 
how to break the data into sub data frames. In this case, I want to break the data 
up into the 6 wool-by-tension combinations. Initially we will just figure out how 
many rows are in each wool-by-tension combination.
```{r}
# group_by:  what variable(s) shall we group on.
# n() is a function that returns how many rows are in the 
#   currently selected sub dataframe
# .groups tells R to drop the grouping structure after doing the summarize step
warpbreaks %>% 
  group_by( wool, tension) %>%                   # grouping
  summarise(n = n(), .groups='drop')             # how many in each group
```

The `group_by` function takes a data.frame and returns the same data.frame, but 
with some extra information so that any subsequent function acts on each unique 
combination defined in the `group_by`. If you wish to remove this behavior, use 
`group_by()` or `ungroup()` to reset the grouping to have no grouping variable. 

The `summarise()` function collapses many rows into fewer rows. a single row and therefore
it is natural to update the grouping structure during the call to` summarise`. 
The options are to `drop` grouping completely, `drop_last` to drop the last level
of grouping, `keep` the same grouping structure, or turn every row into its own 
group with `rowwise`. 

For several years `dplyr` did not require the `.groups` option because `summarise`
only allowed for single row results. In version `1.0.0`, a change was made to allow
`summarise` to only collapse the group to *fewer* rows and that means the choice
of resulting grouping should be thought about. While the default behavior to
`drop_last` if all the results have 1 row makes sense, the user really should
specify what the resulting grouping should be.

Using the same `summarise` function, we could calculate the group mean and 
standard deviation for each wool-by-tension group.
```{r}
summary_table <- 
  warpbreaks %>% 
  group_by(wool, tension) %>%
  summarise( n           = n() ,             # I added some formatting to show the
             mean.breaks = mean(breaks),     # reader I am calculating several
             sd.breaks   = sd(breaks),       # statistics.
             .groups = 'drop')               # drop all grouping structure
```

If instead of summarizing each split, we might want to just do some calculation 
and the output should have the same number of rows as the input data frame. In 
this case I'll tell `dplyr` that we are mutating the data frame instead of 
summarizing it. For example, suppose that I want to calculate the residual 
value $$e_{ijk}=y_{ijk}-\bar{y}_{ij\cdot}$$ where $\bar{y}_{ij\cdot}$ is the mean 
of each `wool:tension` combination.
```{r}
warpbreaks %>% 
   group_by(wool, tension) %>%                 # group by wool:tension
   mutate(resid = breaks - mean(breaks)) %>%   # mean(breaks) of the group!
   head(  )                                    # show the first couple of rows
```



## Exercises  {#Exercises_IntroDataWrangling}

1.  The dataset `ChickWeight` tracks the weights of 48 baby chickens (chicks) feed 
    four different diets. *Feel free to complete all parts of the exercise in a*
    *single R pipeline at the end of the problem.*
    a.  Load the dataset using  
        ```{r}
        data(ChickWeight)
        ```
    b.  Look at the help files for the description of the columns.
    c)  Remove all the observations except for observations from day 10 or day 20. 
        The tough part in this instruction is distinguishing between "and" and "or".
        Obviously there are no observations that occur from both day 10 AND day 20.
        Google 'R logical operators' to get an introduction to those, but the 
        short answer is that and is `&` and or is `|`.
    d)  Calculate the mean and standard deviation of the chick weights for each 
        diet group on days 10 and 20. 

2.  The OpenIntro textbook on statistics includes a data set on body dimensions. 
    *Instead of creating an R chunk for each step of this problem, create a*
    *single R pipeline that performs each of the following tasks.*
    a)  Load the file using 
        ```{r}
        Body <- read.csv('http://www.openintro.org/stat/data/bdims.csv')
        ```
    b)  The column `sex` is coded as a 1 if the individual is male and 0 if female. 
        This is a non-intuitive labeling system. Create a new column `sex.MF` 
        that uses labels Male and Female. Use this column for the rest of
        the problem. _Hint: The `ifelse()` command will be_
        _very convenient here. It functions similarly to the same command in Excel._
    c)  The columns `wgt` and `hgt` measure weight and height in kilograms and 
        centimeters (respectively). Use these to calculate the Body Mass Index 
        (BMI) for each individual where 
        $$BMI=\frac{Weight\,(kg)}{\left[Height\,(m)\right]^{2}}$$ 
    d)  Double check that your calculated BMI column is correct by examining the 
        summary statistics of the column (e.g. `summary(Body)`). BMI values should 
        be between 18 to 40 or so.  Did you make an error in your calculation?  
    e)  The function `cut` takes a vector of continuous numerical data and creates 
        a factor based on your given cut-points. 
        ```{r}
        # Define a continuous vector to convert to a factor
        x <- 1:10

        # divide range of x into three groups of equal length
        cut(x, breaks=3)

        # divide x into four groups, where I specify all 5 break points 
        cut(x, breaks = c(0, 2.5, 5.0, 7.5, 10))
        # (0,2.5] (2.5,5] means 2.5 is included in first group
        # right=FALSE changes this to make 2.5 included in the second  
        
        # divide x into 3 groups, but give them a nicer
        # set of group names
        cut(x, breaks=3, labels=c('Low','Medium','High'))
        ```
        Create a new column of in the data frame that divides the age into 
        decades (10-19, 20-29, 30-39, etc). Notice the oldest person in the study 
        is 67.
        ```{r}
        Body <- Body %>%
          mutate( Age.Grp = cut(age,
                                breaks=c(10,20,30,40,50,60,70),
                                right=FALSE))
        ```
    f)  Find the average BMI for each `Sex.MF` by `Age.Grp` combination.
    
3.  Suppose we have a data frame with the following two variables:
```{r}
df <- tribble(
  ~SubjectID, ~Outcome,
  1, 'good',
  1, 'good',
  2, 'good',
  2, 'bad', 
  2, 'good',
  3, 'bad',
  4, 'good',
  4, 'good')
```
    The `SubjectID` represents a particular individual that has had multiple 
    measurements. What we want to know is what proportion of individuals were
    consistently `good` for all outcomes they had observed. So in our toy example
    set, subjects `1` and `4` where consistently good, so our answer should be 
    $50\%$. *Hint: The steps below help understand the thinking, but this problem*
    *can be done in two lines of code.*
    a)  As a first step, we will summarize each subject with a column 
        denotes if all the subject's observations were `good`. This should result
        in a column of TRUE/FALSE values with one row for each subject. *The `all()`*
        *function should be quite useful here. The corresponding `any()` function*
        *is also useful to know about.*
    b)  Calculate the proportion of subjects that where consistently good by
        calculating the `mean()` of the TRUE/FALSE values. _This works because_
        _TRUE/FALSE values are converted to 1/0 values and then averaged._
