# Databases

```{r, echo=FALSE}
# Un-attach any packages that happen to already be loaded. In general this is unecessary
# but is important for the creation of the book to not have package namespaces
# fighting unexpectedly.
pkgs = names(sessionInfo()$otherPkgs)
if( length(pkgs > 0)){
  pkgs = paste('package:', pkgs, sep = "")
  for( i in 1:length(pkgs)){
    detach(pkgs[i], character.only = TRUE, force=TRUE)
  }
}
```

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(DBI)         # DataBase Interface Package
library(dbplyr)      # dplyr with databases!
```


As our data grows larger and is being updated more frequently, we need to stop using static input files and instead learn to interact with databases. There are a many reasons for using a database, but these are my favorite:

1. Data Freshness. Because the database holds the definitive copy of the data, there isn't a problem of using a .csv file that is months (or years) old. That means my results are constantly being updated with new data.
2. No Local Storage. Because the data lives on the database, I don't have to occupy gigabytes of space on my laptop to hold an out-of-date copy of the data.
3. Database actions are atomic. Whenever I update the database, the action either happens or it doesn't and the database should never be left in an inconsistent state. This extremely important when processing financial transactions, for example.

Fortunately, reading information from a database instead of an in-memory table won't change our current work flow and superficially the change is trivial. However, the impact can be quite profound in the timeliness and re-usability of our work. However, the package `dbplyr` is really only intended for *reading* from the data base and does not support *writing* to the data base. 

## Tutorial Set-Up
To demonstrate how a database works, we unfortunately need to have a live database to connect to. In real situations this would already be done (probably by somebody else) and you would just need to install some DataBase Interface (DBI) package that will negotiate creating a connection between your R-session and the database.

However for this example, we need to start up a data base before we can start working with.

```{r, eval=FALSE}
# Normally, a database connection looks something like this:
con <- DBI::dbConnect(RMariaDB::MariaDB(), 
  host = "database.rstudio.com",
  user = "hadley",
  password = rstudioapi::askForPassword("Database password")
)

# For a Postgres database, it might look like this:
con <- DBI::dbConnect(dbDriver("PostgresSQL"), dbname = "postgres",
  host = "database.nau.edu", port = 5432,
  user = "dls354", 
  password = rstudioapi::askForPassword("Database password"))
```

```{r}
# For our little toy example, we'll use a database I'll create right now.
# Establish a connection
con <- DBI::dbConnect(RSQLite::SQLite(), dbname = ":memory:")
```

Our final step involves populating our new database with some data so that we can play with it. For this example, we'll go back to the credit card data example from the data reshaping chapter. I have it available on the book's GitHub data-raw directory.
```{r, message=FALSE}
# Read in some data for the Credit Card example
stem <- 'https://raw.githubusercontent.com/dereksonderegger/444/master/data-raw/CreditCard'
stem <- '~/GitHub/444/data-raw/CreditCard'

Cards        <- read_csv(paste(stem, 'Cards.csv',        sep='_'), 
                         col_types = cols(CardID=col_character(),
                                          PersonID=col_character()))
Customers    <- read_csv(paste(stem, 'Customers.csv',    sep='_'),
                         col_types = cols(PersonID=col_character()))
Retailers    <- read_csv(paste(stem, 'Retailers.csv',    sep='_'),
                         col_types = cols(RetailID=col_character()))
Transactions <- read_csv(paste(stem, 'Transactions.csv', sep='_'),
                         col_types = cols(CardID=col_character(),
                                          RetailID=col_character()))

# Because the EXTREMELY simple RSQLite database doesn't support dates natively,
# we need to convert all the Date/Time values to be character strings. Notice
# because they are sorted as Year-Month-Day Hour:Minute:Second, sorting will
# still work.
# convert the Dates to character strings 
Cards <- Cards %>% 
  mutate(Issue_DateTime = as.character(Issue_DateTime),
         Exp_DateTime   = as.character(Exp_DateTime))
Transactions <- Transactions %>%
  mutate(DateTime = as.character(DateTime))
```

So that we remember what the data we are working with looks like:
```{r}
head(Customers)    # Key  is  PersonID
head(Cards)        # Keys are PersonID and CardID  
head(Transactions) # Keys are CardID and RetailID
head(Retailers)    # Key  is  RetailID
```

Critically, using the ID columns, we can take an individual transaction figure out what customer executed it. Finally I'll take these tables and load them into my RSQLite database.

```{r}
# Copy the tables to our newly set up database. The dbWriteTable() function is intended
# for database examples and is NOT how you would in practice create a database.
DBI::dbWriteTable(con, 'Cards', Cards,
                  field.types=c(CardID='character',PersonID='character',
                                Issue_DateTime='time',Exp_DateTime='time') )
DBI::dbWriteTable(con, 'Customers', Customers,
                  field.types=c(PersonID='character'))
DBI::dbWriteTable(con, 'Retailers', Retailers,
                  field.types=c(RetailID='character'))
DBI::dbWriteTable(con, 'Transactions', Transactions,
                  field.types=c(CardID='character', RetailID='character',
                                DateTime='time'))

rm(Cards, Customers, Retailers, Transactions)     # Remove all the setup except `con`
```


## SQL

The traditional way to interact with a database is by using SQL syntax. SQL stands for Structured Query Language and some understanding of SQL is mandatory for anyone that interacts with databases.  There are many good introduction to SQL but we'll present the basics here.

Within an Rmarkdown file, we can insert a SQL chunk just as we insert an R chunk

```
```{sql, connection=con}
/* SQL code Chunk, notice comments are marked in C++ style */
SELECT * FROM Transactions
```
```
```{r, echo=FALSE, eval=FALSE}
# If you are looking at the raw source code for this section, you might be shocked
# by the  character in the above. Because I want to show something completely 
# verbatim, and I don't want to remove the backticks, I needed to short-circuit
# the code chunk step.  So what I did was inserted a character with zero width.
# So there is a character there, but it doesn't get displayed.
```



```{sql, connection=con}
/* SQL code Chunk, notice comments are marked in C++ style */
SELECT * FROM Transactions
```

|  SQL Function    |  Description                                |
|:----------------:|:--------------------------------------------|
| `SELECT`         |  A keyword that denotes that the following is a *query*. |
|  `*`             | A placeholder meaning all columns. This could be any column name(s). |
| `FROM`           | A keyword indicating that whatever follows is the table (or tables) being selected from. Any table joins need to be constrained in the WHERE clause to tell us what columns need to match.  |
| `WHERE`          | A keyword indicating the following logical statements will be used to filter rows. Boolean operators `AND`, `OR`, and `NOT` can be used to create complex filter statements. |

Typical SQL statements can be quite long and sometimes difficult to read because the table join instructions are mixed in with the filtering instructions. For example, the following is the SQL command to generate my credit card statement
```{sql, connection=con}
/* SQL code Chunk, notice comments are marked in C++ style */
SELECT Customers.Name, Transactions.DateTime, Retailers.Name, Transactions.Amount
  FROM Customers, Cards, Transactions, Retailers
  WHERE Customers.PersonID = Cards.PersonID AND 
        Cards.CardID = Transactions.CardID AND
        Transactions.RetailID = Retailers.RetailID AND
        Customers.Name = 'Derek Sonderegger'
```



## `dbplyr`

There are a lot of good things about SQL, but for database queries, I would really like to pretend that the tables are in memory and use all of my favorite `dplyr` tools and pipelines. This would mean that I don't have to remember all the weird SQL syntax. However, the database interface `dbplyr` is ONLY intended for queries and NOT for updating or inserting rows into the tables.

The way this will work is that we will use the previously established database connection `con` to create a virtual link between the database table and some appropriately named R object.

```{r}
# connect the database tables to similarly named objects in R
Cards <-        tbl(con, 'Cards')
Customers <-    tbl(con, 'Customers')
Retailers <-    tbl(con, 'Retailers')
Transactions <- tbl(con, 'Transactions')
```

However, this does NOT download the whole table into R.  Instead it grabs only a couple of rows so that we can see the format. Notice that we don't know how many rows are in the Transactions table.
```{r}
Transactions %>% head(3)
# Transactions %>% tail(3)  # not supported because we haven't yet downloaded much information.
```

The guiding principle of `dbplyr` is to delay as much work for as long as possible actually pulling the data from the database. The rational is that we spend a great deal of time figuring out what the query should look like and too often we write a query that accidentally downloads millions of lines of data and slows down our network connection. Instead `dbplyr` returns just the first few rows of whatever query we are working on until we finish the pipeline with a `collect()` command that will cause us to download ALL of the query results and save them as a local `data.frame`.

```{r}
CC_statement <- Customers %>% 
  filter(Name == 'Derek Sonderegger') %>% select(PersonID) %>%
  left_join(Cards) %>% left_join(Transactions) %>% left_join(Retailers) %>%
  select(DateTime, Name, Amount) %>%
  rename(Retailer = Name) 

CC_statement
```

At this point, we *still* haven't downloaded all of the rows. Instead this is still a *lazy* query. It can be fun to see what the SQL code that is being generated is.

```{r}
CC_statement %>% show_query()
```

The algorithm used to convert my `dplyr` statement into a SQL statement doesn't mind nesting SQL statements and isn't the same as what I generated by hand, but it works.


```{r}
# Close our database connection when we are through...
dbDisconnect(con)
```


